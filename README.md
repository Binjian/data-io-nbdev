# tspace


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

# Overview

**tspace** is an data pipleline framework for deep reinforcement
learning with IO interface, processing and configuration. The current
code base depicts an automotive implementation. The main features are:

- working both training and inferrence mode, supporting
  - coordinated
    [ETL](https://en.wikipedia.org/wiki/Extract,_transform,_load) and ML
    pipelines,
  - online and offline training,
  - local and distributed training;
- multiple models of
  - reinforcement learning models with DDPG and
  - time sequence processing with recurrent models;
- data pipelines compatible to both ETL and ML dataflow with
  - support of multiple data sources (local CAN or remote cloud object
    storage),
  - stateful time sequence processing with sequential model and
  - support of both NoSQL database, local and cloud data storage

<img src="res/tspace_overview.svg" alt="Overview of tspace architecture" width="80%">

The diagram shows the basic architecture of **tspace**.

## [`Avatar`](https://Binjian.github.io/tspace/00.avatar.html#avatar)

It is the entry point of the `tspace`. It orchestrates the whole ETL and
ML workflow.

- It configures KvaserCAN, RemoteCAN, Cruncher, Agent, Model, Database,
  Pipeline.
- It manages the scheduling of two primary threads in the first tier of
  cascaded threading pools in
  [`tspace.avatar.main`](https://Binjian.github.io/tspace/00.avatar.html#main).
- It selects the either **KvaserCAN** or **RemoteCAN** as the vehicle
  interface for reading the observation and applying the action.

## KvaserCAN

It is implemented with
[`Kvaser`](https://Binjian.github.io/tspace/06.dataflow.kvaser.html#kvaser)
which provides

- a local interface for reading the observation (CAN messages of vehicle
  states) via Kvaser using
  [`udp_context`](https://Binjian.github.io/tspace/04.conn.udp.html#udp_context)
  to get CAN messages as json data from a local udp server. Then it
  encodes the raw json data into a
  [pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)
  for forwarding through the data pipeline to
  [`Cruncher`](https://Binjian.github.io/tspace/06.dataflow.cruncher.html#cruncher).

- It provides a local interface for applying the action (flashing
  parameters) onto the vehicle ECU (VCU). Before sending the action, it
  decodes the action from the
  [pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)
  into packed string buffer and then sends it to the ECU by calling
  [`send_float_array`](https://Binjian.github.io/tspace/04.conn.tbox.html#send_float_array)
  from
  [`VehicleInterface.consume`](https://Binjian.github.io/tspace/06.dataflow.vehicle_interface.html#vehicleinterface.consume).

- The control messages for training HMI go through the same UDP port.
  They are used to modify the threading events to control the episodic
  training process with
  [`VehicleInterface.hmi_control`](https://Binjian.github.io/tspace/06.dataflow.vehicle_interface.html#vehicleinterface.hmi_control).

## RemoteCAN

It provides a remote interface to the vehicle via the object storage
system on the cloud sent by the onboard TBox. It’s implemented with
[`Cloud`](https://Binjian.github.io/tspace/06.dataflow.cloud.html#cloud):

- It reads the observation (CAN messages of vehicle states) from the
  cloud object storage system through
  [`RemoteCanClient.get_signals`](https://Binjian.github.io/tspace/04.conn.remote_can_client.html#remotecanclient.get_signals).
  It then encodes the raw json data into a
  [pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)
  and forward it to
  [`Cruncher`](https://Binjian.github.io/tspace/06.dataflow.cruncher.html#cruncher)
  through the data pipeline.

- It sends the action (flashing parameters) to the vehicle ECU (VCU) in
  the shared
  [`VehicleInterface.consume`](https://Binjian.github.io/tspace/06.dataflow.vehicle_interface.html#vehicleinterface.consume)
  by calling
  [`RemoteCanClient.send_torque_map`](https://Binjian.github.io/tspace/04.conn.remote_can_client.html#remotecanclient.send_torque_map),
  which decodes the action from the
  [pandas.DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)
  into raw json string.

- It selects the training HMI to get the vehicle and driver information
  as configuration with
  [`Cloud.hmi_capture_from_udp`](https://Binjian.github.io/tspace/06.dataflow.cloud.html#cloud.hmi_capture_from_udp)
  for local udp server, with
  [`Cloud.hmi_capture_from_rmq`](https://Binjian.github.io/tspace/06.dataflow.cloud.html#cloud.hmi_capture_from_rmq)
  for remote RocketMQ server, with
  [`Cloud.hmi_capture_from_dummy`](https://Binjian.github.io/tspace/06.dataflow.cloud.html#cloud.hmi_capture_from_dummy)
  for pure inference mode without training or updating models. It shares
  the same control logic
  [`VehicleInterface.hmi_control`](https://Binjian.github.io/tspace/06.dataflow.vehicle_interface.html#vehicleinterface.hmi_control)
  with **KvaserCAN**.

## Cruncher

It is main pivot of the data pipeline for pre-processing the observation
and post-processing the action:

- The
  [`Cruncher.filter`](https://Binjian.github.io/tspace/06.dataflow.cruncher.html#cruncher.filter)
  reveives the observation through the data pipeline from **KvaserCAN**
  or **RemoteCAN**. It pre-processes the input data into the quadruple
  with a timestamp $(timestamp, state, action, reward, state')$ and give
  it to the reinforcement **Agent**
  [`DPG`](https://Binjian.github.io/tspace/07.agent.dpg.html#dpg),
  subsequently its child
  [`DDPG`](https://Binjian.github.io/tspace/07.agent.ddpg.html#ddpg) or
  [`RDPG`](https://Binjian.github.io/tspace/07.agent.rdpg.rdpg.html#rdpg),
  for inferring an optimal action determined by its current policy.
  After getting the prediction of the agent, it encodes the prediction
  result into an action object and forwards it to
  [`VehicleInterface.consume`](https://Binjian.github.io/tspace/06.dataflow.vehicle_interface.html#vehicleinterface.consume)
  to be flashed onto VCU.

- It collects the critic, actor loss, the total reward for each episode,
  the running reward and the action at the end of the episode. It also
  saves the model checkpoint and the training log locally.

## Agent

It provides a wrapper for the reinforcement learning model with
[`DPG`](https://Binjian.github.io/tspace/07.agent.dpg.html#dpg):

- It has an interface to data storage:

  - retrieves the observation meta information and database
    configuration from
    [`Avatar`](https://Binjian.github.io/tspace/00.avatar.html#avatar),

  - initializes repo interface
    [`Buffer`](https://Binjian.github.io/tspace/05.storage.buffer.buffer.html#buffer),
    subsequently
    [`MongoBuffer`](https://Binjian.github.io/tspace/05.storage.buffer.mongo.html#mongobuffer)
    or
    [`DaskBuffer`](https://Binjian.github.io/tspace/05.storage.buffer.dask.html#daskbuffer)
    which then initializes the database connection with
    [`MongoPool`](https://Binjian.github.io/tspace/05.storage.pool.mongo.html#mongopool)
    or
    [`DaskPool`](https://Binjian.github.io/tspace/05.storage.pool.dask.html#daskpool)
    respectively.

- It transfers observation data to the neural network:

  - initializes the episode states,

  - defines abstract methods
    [`DPG.actor_predict`](https://Binjian.github.io/tspace/07.agent.dpg.html#dpg.actor_predict),
    [`DPG.train`](https://Binjian.github.io/tspace/07.agent.dpg.html#dpg.train),
    [`DPG.get_losses`](https://Binjian.github.io/tspace/07.agent.dpg.html#dpg.get_losses),
    [`DPG.soft_update_target`](https://Binjian.github.io/tspace/07.agent.dpg.html#dpg.soft_update_target),
    [`DPG.init_checkpoint`](https://Binjian.github.io/tspace/07.agent.dpg.html#dpg.init_checkpoint),
    [`DPG.save_ckpt`](https://Binjian.github.io/tspace/07.agent.dpg.html#dpg.save_ckpt),
    [`DPG.touch_gpu`](https://Binjian.github.io/tspace/07.agent.dpg.html#dpg.touch_gpu)
    for concrete implementations in child classes
    [`DDPG`](https://Binjian.github.io/tspace/07.agent.ddpg.html#ddpg)
    and
    [`RDPG`](https://Binjian.github.io/tspace/07.agent.rdpg.rdpg.html#rdpg),

  - provides the concrete methods
    [`DPG.start_episode`](https://Binjian.github.io/tspace/07.agent.dpg.html#dpg.start_episode),
    [`DPG.end_episode`](https://Binjian.github.io/tspace/07.agent.dpg.html#dpg.end_episode),
    [`DPG.deposit`](https://Binjian.github.io/tspace/07.agent.dpg.html#dpg.deposit),
    [`DPG.deposit_episode`](https://Binjian.github.io/tspace/07.agent.dpg.html#dpg.deposit_episode).

  - [`DPG.touch_gpu`](https://Binjian.github.io/tspace/07.agent.dpg.html#dpg.touch_gpu)
    is used to warm up the GPU before starting inference.

### [`DDPG`](https://Binjian.github.io/tspace/07.agent.ddpg.html#ddpg)

- provides methods to create, load or initialize the [Recurrent
  Deterministic Policy Gradient](https://arxiv.org/abs/1512.04455)
  **Model**, or restore checkpoints to it. It also exports the tflite
  model.
- It provides the concrete methods for the abstract ones in the
  [`DPG`](https://Binjian.github.io/tspace/07.agent.dpg.html#dpg)
  interface.
- [`DDPG.infer_single_sample`](https://Binjian.github.io/tspace/07.agent.ddpg.html#ddpg.infer_single_sample)
  is the inference method with graph optimization via
  [tf.function](https://www.tensorflow.org/guide/function).
- [`DDPG.sample_minibatch`](https://Binjian.github.io/tspace/07.agent.ddpg.html#ddpg.sample_minibatch)
  provides a minibatch sampled from the buffer. It handles the bootstrap
  when the buffer is empty thus there is no samples in the
  [`Buffer`](https://Binjian.github.io/tspace/05.storage.buffer.buffer.html#buffer)
  when the first episode has not ended.
- [`DDPG.update_with_batch`](https://Binjian.github.io/tspace/07.agent.ddpg.html#ddpg.update_with_batch)
  enforces the back propagation and applies the weight update to the
  actor and critic network during
  [`DDPG.train`](https://Binjian.github.io/tspace/07.agent.ddpg.html#ddpg.train).

### [`RDPG`](https://Binjian.github.io/tspace/07.agent.rdpg.rdpg.html#rdpg)

- provides methods to create, load or initialize the [Deep Deterministic
  Policy Gradient](https://arxiv.org/abs/1509.02971) **Model**, or
  restore checkpoints to it.
- It provides the concrete methods for the abstract ones in the
  [`DPG`](https://Binjian.github.io/tspace/07.agent.dpg.html#dpg)
  interface.
- [`RDPG.actor_predict_step`](https://Binjian.github.io/tspace/07.agent.rdpg.rdpg.html#rdpg.actor_predict_step)
  is the inference method with graph optimization via
  [tf.function](https://www.tensorflow.org/guide/function).
- [`RDPG.train_step`](https://Binjian.github.io/tspace/07.agent.rdpg.rdpg.html#rdpg.train_step)
  is the training method with graph optimization via
  [tf.function](https://www.tensorflow.org/guide/function). It also
  applies the weight update to the actor and critic network
- [`RDPG.train`](https://Binjian.github.io/tspace/07.agent.rdpg.rdpg.html#rdpg.train)
  samples a ragged minibatch of episodes with different lengths from the
  buffer. It handles the truncated back propagation through time (TBPTT)
  by splitting the episodes and looping over the subsequences with
  Masking layers to update the weights by
  [`RDPG.train_step`](https://Binjian.github.io/tspace/07.agent.rdpg.rdpg.html#rdpg.train_step).

## Model

It’s the neural network model for the reinforcement learning agent. For
now it’s only implemented for
[`RDPG`](https://Binjian.github.io/tspace/07.agent.rdpg.rdpg.html#rdpg)
in
[`SeqActor`](https://Binjian.github.io/tspace/07.agent.rdpg.actor.html#seqactor)
and
[`SeqCritic`](https://Binjian.github.io/tspace/07.agent.rdpg.critic.html#seqcritic).

### [`SeqActor`](https://Binjian.github.io/tspace/07.agent.rdpg.actor.html#seqactor)

It is the actor network with two recurrent LSTM layers, two dense layers
and a Masking layer for handling ragged input sequence.

- [`SeqActor.predict`](https://Binjian.github.io/tspace/07.agent.rdpg.actor.html#seqactor.predict)
  gives the action given the state for inference, thus the batch
  dimension has to be one.
- [`SeqActor.evaluate_actions`](https://Binjian.github.io/tspace/07.agent.rdpg.actor.html#seqactor.evaluate_actions)
  gives the action given a batch of states for training. It’s used in
  the training loop to get the prediction of the target actor network to
  calculate the critic loss.
- It handles the ragged input sequences with Masking layer and the
  stateful recurrent layers for TBPTT
- For inference,
  [`SeqCritic`](https://Binjian.github.io/tspace/07.agent.rdpg.critic.html#seqcritic)
  is not used and only
  [`SeqActor`](https://Binjian.github.io/tspace/07.agent.rdpg.actor.html#seqactor)
  is required.

### [`SeqCritic`](https://Binjian.github.io/tspace/07.agent.rdpg.critic.html#seqcritic)

It is the critic network with two recurrent LSTM layers and two dense
layer and a Masking layer for handling ragged input sequence.

- [`SeqCritic.evaluate_q`](https://Binjian.github.io/tspace/07.agent.rdpg.critic.html#seqcritic.evaluate_q)
  gives the Q-value given a batch of the state and action. It’s used in
  the training loop
  [`RDPG.train_step`](https://Binjian.github.io/tspace/07.agent.rdpg.rdpg.html#rdpg.train_step)
  to calculate the critic and actor loss.

## Storage

represents the data storage in the repository pattern with two
polymorphic abstraction layers
[`Buffer`](https://Binjian.github.io/tspace/05.storage.buffer.buffer.html#buffer)
and
[`Pool`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool).

### [`Buffer`](https://Binjian.github.io/tspace/05.storage.buffer.buffer.html#buffer)

is an abstract class. It provides a view of data storage to the agent:

- **Agent** uses the abstract methods
  [`Buffer.load`](https://Binjian.github.io/tspace/05.storage.buffer.buffer.html#buffer.load),
  `Buffer.save` and
  [`Buffer.close`](https://Binjian.github.io/tspace/05.storage.buffer.buffer.html#buffer.close)
  loads or saves data from or to the
  [`Pool`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool),
  and closes the connection to the
  [`Pool`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool).
- The abstract
  [`Buffer.sample`](https://Binjian.github.io/tspace/05.storage.buffer.buffer.html#buffer.sample)
  samples a minibatch from the
  [`Pool`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool).
  It needs the child of
  [`Buffer`](https://Binjian.github.io/tspace/05.storage.buffer.buffer.html#buffer)
  to implement the concrete efficient sampling method, which depends on
  the underlying data storage system.
- The concrete methode
  [`Buffer.store`](https://Binjian.github.io/tspace/05.storage.buffer.buffer.html#buffer.store)
  store the whole episode data into the
  [`Pool`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool)
- The concrete methode
  [`Buffer.find`](https://Binjian.github.io/tspace/05.storage.buffer.buffer.html#buffer.find)
  simply calls
  [`Pool.find`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool.find)
  to find the data with the given query.

#### [`MongoBuffer`](https://Binjian.github.io/tspace/05.storage.buffer.mongo.html#mongobuffer)

It’s a concrete class for the underlying NoSQL database MongoDB.

- It implements the abstract methods required by the
  [`Buffer`](https://Binjian.github.io/tspace/05.storage.buffer.buffer.html#buffer)
  interface.
- [`MongoBuffer.decode_batch_records`](https://Binjian.github.io/tspace/05.storage.buffer.mongo.html#mongobuffer.decode_batch_records)
  prepare the sample batch data from
  [`MongoPool`](https://Binjian.github.io/tspace/05.storage.pool.mongo.html#mongopool)
  into a compliant format for agent training.
- It can handle both DDPG record data type and RDPG episode data type.

#### [`DaskBuffer`](https://Binjian.github.io/tspace/05.storage.buffer.dask.html#daskbuffer)

It’s a concrete class for the distributed data storage system Dask.

- It implements the abstract methods required by the
  [`Buffer`](https://Binjian.github.io/tspace/05.storage.buffer.buffer.html#buffer)
  interface.
- [`DaskBuffer.decode_batch_records`](https://Binjian.github.io/tspace/05.storage.buffer.dask.html#daskbuffer.decode_batch_records)
  prepare the sample batch data from
  [`DaskPool`](https://Binjian.github.io/tspace/05.storage.pool.dask.html#daskpool)
  into a compliant format for agent training.
- It can handle both DDPG record data type and RDPG episode data type.

### [`Pool`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool)

is an abstract class. It’s the interface for the underlying data
storage. For the moment, it’s implemented with
[`MongoPool`](https://Binjian.github.io/tspace/05.storage.pool.mongo.html#mongopool)
and
[`DaskPool`](https://Binjian.github.io/tspace/05.storage.pool.dask.html#daskpool).

- It defines the abstract methods
  [`Pool.load`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool.load),
  [`Pool.close`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool.close),
  [`Pool.store`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool.store),
  [`Pool.delete`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool.delete),
  [`Pool.find`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool.find),
  [`Pool.sample`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool.sample)
  and
  [`Pool._count`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool._count)
  for the concrete classes to implement.
- It defines
  [`PoolQuery`](https://Binjian.github.io/tspace/01.data.core.html#poolquery)
  as the query object for
  [`Pool.sample`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool.sample),
  [`Pool.find`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool.find)
  and
  [`Pool._count`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool._count)
  method.
- It implements the iterable protocol with
  [`Pool.__iter__`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool.__iter__)
  and
  [`Pool.__getitem__`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool.__getitem__)
  for the concrete classes to implement an efficient indexing method.

#### [`MongoPool`](https://Binjian.github.io/tspace/05.storage.pool.mongo.html#mongopool)

It’s a concrete class for the underlying NoSQL database MongoDB with
time series support. It handles both record data type and episode data
type with MongoDB collection features.

- It provides the interface to the MongoDB database with the
  [pymongo](https://pymongo.readthedocs.io/en/stable) library.
- It implements the abstract methods required by the
  [`Pool`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool)
  interface.
- [`MongoPool.store_record`](https://Binjian.github.io/tspace/05.storage.pool.mongo.html#mongopool.store_record)
  stores the record data into the MongoDB database for
  [`DDPG`](https://Binjian.github.io/tspace/07.agent.ddpg.html#ddpg)
  agent.
- [`MongoPool.store_episode`](https://Binjian.github.io/tspace/05.storage.pool.mongo.html#mongopool.store_episode)
  stores the episode data into the MongoDB database for
  [`RDPG`](https://Binjian.github.io/tspace/07.agent.rdpg.rdpg.html#rdpg)
  agent.

#### [`DaskPool`](https://Binjian.github.io/tspace/05.storage.pool.dask.html#daskpool)

It’s an abstract class for the distributed data storage system Dask,
since we have to use different backends: Parquet for record data type
and avro for episode data type.

- It supports both local file storage and remote object storage with the
  [dask](https://dask.org) library.
- It defines the generic data type for the abstract method required by
  the
  [`Pool`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool)
  interface. The generic data type can then be specialized by the
  concrete classes either as dask.DataFrame for record data type or
  dask.Bag for episode data type.

##### [`ParquetPool`](https://Binjian.github.io/tspace/05.storage.pool.parquet.html#parquetpool)

is a concrete class for the record data type with the Parquet file
format as backend storage.

- It implements the abstract methods required by the
  [`DaskPool`](https://Binjian.github.io/tspace/05.storage.pool.dask.html#daskpool)
  interface and
  [`Pool`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool)
  subsequently.
- [`ParquetPool.sample`](https://Binjian.github.io/tspace/05.storage.pool.parquet.html#parquetpool.sample)
  provides an efficient unified sampling interface via Dask.DataFrame to
  a Parquet storage either locally or remotely.
- [`ParquetPool.get_query`](https://Binjian.github.io/tspace/05.storage.pool.parquet.html#parquetpool.get_query)
  provides the query object through Dask indexing for the
  [`ParquetPool.sample`](https://Binjian.github.io/tspace/05.storage.pool.parquet.html#parquetpool.sample)
  method.

##### [`AvroPool`](https://Binjian.github.io/tspace/05.storage.pool.avro.avro.html#avropool)

is a concrete class for the episode data type with the avro file format
as backend storage.

- It implements the abstract methods required by the
  [`DaskPool`](https://Binjian.github.io/tspace/05.storage.pool.dask.html#daskpool)
  interface and
  [`Pool`](https://Binjian.github.io/tspace/05.storage.pool.pool.html#pool)
  subsequently.
- [`AvroPool.sample`](https://Binjian.github.io/tspace/05.storage.pool.avro.avro.html#avropool.sample)
  provides an efficient unified sampling interface via Dask.Bag to a
  avro storage either locally or remotely.
- [`AvroPool.get_query`](https://Binjian.github.io/tspace/05.storage.pool.avro.avro.html#avropool.get_query)
  provides the query object through Dask indexing for the
  [`AvroPool.sample`](https://Binjian.github.io/tspace/05.storage.pool.avro.avro.html#avropool.sample)
  method.

## Pipeline

## Configuration

## Scheduling

## TODO

1.  Add time sequence embedding database support with LanceDB for
    TimeGPT
2.  Batch mode for large scale inference and training with Unit of Work
    pattern

# How to use

## Install

``` sh
pip install tspace
```
