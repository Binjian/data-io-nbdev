{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e655fdc45ac18d",
   "metadata": {},
   "source": [
    "# SeqActor\n",
    "\n",
    "> Sequential Actor class with LSTM layers for RDPG agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaf8cf4e336a1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agent.rdpg.seq_actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ed83a395c2653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "from typing import ClassVar, Optional\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from data_io_nbdev.agent.utils.hyperparams import HyperParamRDPG\n",
    "from data_io_nbdev.agent.utils.ou_action_noise import OUActionNoise\n",
    "from data_io_nbdev.system.exception import ReadOnlyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139edb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SeqActor:\n",
    "    \"\"\"Sequential Actor network for the RDPG algorithm.\n",
    "    \n",
    "    Attributes:\n",
    "        \n",
    "        - state_dim (int): Dimension1 of the state space.\n",
    "        - action_dim (int): Dimension of the action space.\n",
    "        - hidden_dim (int): Dimension of the hidden layer.\n",
    "        - lr (float): Learning rate for the network.\n",
    "        - ckpt_dir (str): Directory to restore the checkpoint from.\n",
    "    \"\"\"\n",
    "\n",
    "    _hyperparams: ClassVar[\n",
    "        HyperParamRDPG\n",
    "    ] = HyperParamRDPG()  # for tf.function to get some of the default hyperparameters\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int = 0,  # dimension of the state space\n",
    "        action_dim: int = 0,  #  dimension of the action space\n",
    "        hidden_dim: int = 0,  # dimension of the hidden layer\n",
    "        n_layers: int = 0,  # number of lstm layers\n",
    "        batch_size: int = 0,  # batch size\n",
    "        padding_value: float = 0.0,  # padding value for masking\n",
    "        tau: float = 0.0,  # soft update parameter\n",
    "        lr: float = 0.0,  # learning rate\n",
    "        ckpt_dir: Path = Path(\".\"),  # checkpoint directory\n",
    "        ckpt_interval: int = 0,  # checkpoint interval\n",
    "        logger: Optional[logging.Logger] = None,  # logger\n",
    "        dict_logger: Optional[dict] = None,  # logger dict\n",
    "    ):\n",
    "        \"\"\"Initialize the actor network.\n",
    "\n",
    "        restore checkpoint from the provided directory if it exists,\n",
    "        initialize otherwise.\n",
    "        Args:\n",
    "            state_dim (int): Dimension of the state space.\n",
    "            action_dim (int): Dimension of the action space.\n",
    "            hidden_dim (int): Dimension of the hidden layer.\n",
    "            lr (float): Learning rate for the network.\n",
    "            ckpt_dir (str): Directory to restore the checkpoint from.\n",
    "        \"\"\"\n",
    "\n",
    "        self._state_dim = state_dim\n",
    "        self._action_dim = action_dim\n",
    "        self._hidden_dim = hidden_dim\n",
    "        self._lr = lr\n",
    "        self._padding_value = padding_value\n",
    "        self._n_layers = n_layers\n",
    "        self._tau = tau\n",
    "        self.logger = logger\n",
    "        self.dict_logger = dict_logger\n",
    "\n",
    "        states = keras.layers.Input(\n",
    "            batch_shape=(batch_size, None, state_dim), name=\"states\"\n",
    "        )\n",
    "        last_actions = keras.layers.Input(\n",
    "            batch_shape=(batch_size, None, action_dim), name=\"last_actions\"\n",
    "        )\n",
    "\n",
    "        inputs = [\n",
    "            states,\n",
    "            last_actions,\n",
    "        ]  # history update consists of current states s_t and last actions a_{t-1}\n",
    "        x = keras.layers.Concatenate(axis=-1)(\n",
    "            inputs\n",
    "        )  # feature dimension would be [states + actions]\n",
    "\n",
    "        # attach mask to the inputs, & apply recursive lstm layer to the output\n",
    "        x = keras.layers.Masking(\n",
    "            mask_value=padding_value,\n",
    "            input_shape=(batch_size, None, state_dim + action_dim),\n",
    "        )(\n",
    "            x\n",
    "        )  # input (observation) padded with -10000.0, on the time dimension\n",
    "\n",
    "        x = keras.layers.Dense(hidden_dim, activation=\"relu\")(\n",
    "            x\n",
    "        )  # linear layer to map [states + actions] to [hidden_dim]\n",
    "\n",
    "        # if n_layers <= 1, the loop will be skipped in default\n",
    "        for i in range(n_layers - 1):\n",
    "            x = keras.layers.LSTM(\n",
    "                hidden_dim,\n",
    "                batch_input_shape=(batch_size, None, hidden_dim),\n",
    "                return_sequences=True,\n",
    "                return_state=False,\n",
    "                stateful=True,  # stateful for batches of long sequences split into batches of shorter sequences\n",
    "                name=f\"lstm_{i}\",\n",
    "            )(\n",
    "                x\n",
    "            )  # only return full sequences of hidden states, necessary for stacking LSTM layers,\n",
    "            # last hidden state is not needed\n",
    "\n",
    "        lstm_output = keras.layers.LSTM(\n",
    "            hidden_dim,\n",
    "            batch_input_shape=(batch_size, None, hidden_dim),\n",
    "            return_sequences=True,\n",
    "            return_state=False,  # return hidden and cell states for inference of each time step,\n",
    "            stateful=True,  # stateful for batches of long sequences split into batches of shorter sequences\n",
    "            name=f\"lstm_{n_layers - 1}\",\n",
    "            # need to reset_states when the episode ends\n",
    "        )(x)\n",
    "\n",
    "        # rescale the output of the lstm layer to (-1, 1)\n",
    "        action_output = keras.layers.Dense(action_dim, activation=\"tanh\")(lstm_output)\n",
    "\n",
    "        self.eager_model = tf.keras.Model([states, last_actions], action_output)\n",
    "        # no need to evaluate the last action separately\n",
    "        # just run the model inference and get the last action\n",
    "        # self.action_last = action_outputs[:, -1, :]\n",
    "\n",
    "        self.eager_model.summary()\n",
    "        # self.graph_model = tf.function(self.eager_model)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "        self._std_dev = 0.2\n",
    "        self.ou_noise = OUActionNoise(\n",
    "            mean=np.zeros(action_dim),\n",
    "            std_deviation=float(self._std_dev) * np.ones(action_dim),\n",
    "        )\n",
    "\n",
    "        # restore the checkpoint if it exists\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "        self._ckpt_interval = ckpt_interval\n",
    "        self.ckpt = tf.train.Checkpoint(\n",
    "            step=tf.Variable(tf.constant(1), name=\"step\"),\n",
    "            optimizer=self.optimizer,\n",
    "            net=self.eager_model,\n",
    "        )\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            self.ckpt, self.ckpt_dir, max_to_keep=10\n",
    "        )\n",
    "        self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.logger.info(\n",
    "                f\"Restored actor from {self.ckpt_manager.latest_checkpoint}\",\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "        else:\n",
    "            self.logger.info(f\"Actor Initializing from scratch\", extra=self.dict_logger)\n",
    "\n",
    "    def clone_weights(self, moving_net):\n",
    "        \"\"\"Clone weights from a model to another model. only for target critic\"\"\"\n",
    "        self.eager_model.set_weights(moving_net.eager_model.get_weights())\n",
    "\n",
    "    def soft_update(self, moving_net):\n",
    "        \"\"\"Update the target weights.\"\"\"\n",
    "        self.eager_model.set_weights(\n",
    "            [\n",
    "                self._tau * w + (1 - self._tau) * w_t\n",
    "                for w, w_t in zip(\n",
    "                    moving_net.eager_model.get_weights(),\n",
    "                    self.eager_model.get_weights(),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def save_ckpt(self):\n",
    "        \"\"\"Save the checkpoint.\"\"\"\n",
    "        self.ckpt.step.assign_add(1)\n",
    "        if int(self.ckpt.step) % self.ckpt_interval == 0:\n",
    "            save_path = self.ckpt_manager.save()\n",
    "            self.logger.info(\n",
    "                f\"Saved ckpt for step {int(self.ckpt.step)}: {save_path}\",\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"Reset the ou_noise.\"\"\"\n",
    "        self.ou_noise.reset()\n",
    "\n",
    "    # @tf.function(input_signature=[tf.TensorSpec(shape=[None, None, self._state_dim], dtype=tf.float32)])\n",
    "    def predict(self, states: tf.Tensor, last_actions: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Predict the action given the state. Batch dimension needs to be one.\n",
    "        \n",
    "        Args:\n",
    "            \n",
    "            states: State, Batch dimension needs to be one.\n",
    "            last_actions: Last action, Batch dimension needs to be one.\n",
    "\n",
    "        Return:\n",
    "            Action\n",
    "        \"\"\"\n",
    "\n",
    "        # get the last step action and squeeze the batch dimension\n",
    "        action = self.predict_step(states, last_actions)\n",
    "        sampled_action = (\n",
    "            action + self.ou_noise()\n",
    "        )  # noise object is a row vector, without batch and time dimension\n",
    "        assert isinstance(sampled_action, tf.Tensor), \"sampled_action is not a tensor\"\n",
    "        return sampled_action\n",
    "\n",
    "    @tf.function(\n",
    "        input_signature=[\n",
    "            tf.TensorSpec(\n",
    "                shape=[None, None, _hyperparams.NStates], dtype=tf.float32\n",
    "            ),  # [None, None, 600] for cloud / [None, None, 90] for kvaser\n",
    "            tf.TensorSpec(\n",
    "                shape=[None, None, _hyperparams.NActions], dtype=tf.float32\n",
    "            ),  # [None, None, 68] for both cloud and kvaser\n",
    "        ]\n",
    "    )\n",
    "    def predict_step(self, states, last_actions):\n",
    "        \"\"\"Predict the action given the state.\n",
    "        \n",
    "        For Inferring\n",
    "        \n",
    "        Args:\n",
    "            \n",
    "            states (tf.Tensor): State, Batch dimension needs to be one.\n",
    "            last_actions (tf.Tensor): State, Batch dimension needs to be one.\n",
    "\n",
    "        Returns:\n",
    "            \n",
    "            np.array: Action, ditch the batch dimension\n",
    "        \"\"\"\n",
    "        action_seq = self.eager_model([states, last_actions])  # actor output sequences\n",
    "\n",
    "        # get the last step action and squeeze the time dimension,\n",
    "        # since Batch is one when inferring, squeeze also the batch dimension by tf.squeeze default\n",
    "        # action = tf.squeeze(action_seq[:, -1, :])\n",
    "        # action = tf.squeeze(action_seq)\n",
    "        action = action_seq[:, -1, :]  # get the last step action\n",
    "        return action\n",
    "\n",
    "    @tf.function(\n",
    "        input_signature=[\n",
    "            tf.TensorSpec(\n",
    "                shape=[None, None, _hyperparams.NStates], dtype=tf.float32\n",
    "            ),  # [None, None, 600] for cloud / [None, None, 90] for kvaser\n",
    "            tf.TensorSpec(\n",
    "                shape=[None, None, _hyperparams.NActions], dtype=tf.float32\n",
    "            ),  # [None, None, 68] for both cloud and kvaser\n",
    "        ]\n",
    "    )\n",
    "    def evaluate_actions(self, states, last_actions):\n",
    "        \"\"\"Evaluate the action given the state.\n",
    "        \n",
    "        For training\n",
    "        \n",
    "        Args:\n",
    "            \n",
    "            states (tf.Tensor): State, Batch dimension needs to be one.\n",
    "            last_actions (tf.Tensor): State, Batch dimension needs to be one.\n",
    "\n",
    "        Return:\n",
    "            np.array: Action, keep the batch dimension\n",
    "        \"\"\"\n",
    "        return self.eager_model([states, last_actions])\n",
    "\n",
    "    @property\n",
    "    def state_dim(self):\n",
    "        return self._state_dim\n",
    "\n",
    "    @state_dim.setter\n",
    "    def state_dim(self, value):\n",
    "        raise ReadOnlyError(\"state_dim is read-only\")\n",
    "\n",
    "    @property\n",
    "    def action_dim(self):\n",
    "        return self._action_dim\n",
    "\n",
    "    @action_dim.setter\n",
    "    def action_dim(self, value):\n",
    "        raise ReadOnlyError(\"action_dim is read-only\")\n",
    "\n",
    "    @property\n",
    "    def hidden_dim(self):\n",
    "        return self._hidden_dim\n",
    "\n",
    "    @hidden_dim.setter\n",
    "    def hidden_dim(self, value):\n",
    "        raise ReadOnlyError(\"hidden_dim is read-only\")\n",
    "\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    @lr.setter\n",
    "    def lr(self, value):\n",
    "        raise ReadOnlyError(\"lr is read-only\")\n",
    "\n",
    "    @property\n",
    "    def padding_value(self):\n",
    "        return self._padding_value\n",
    "\n",
    "    @padding_value.setter\n",
    "    def padding_value(self, value):\n",
    "        raise ReadOnlyError(\"padding_value is read-only\")\n",
    "\n",
    "    @property\n",
    "    def n_layers(self):\n",
    "        return self._n_layers\n",
    "\n",
    "    @n_layers.setter\n",
    "    def n_layers(self, value):\n",
    "        raise ReadOnlyError(\"n_layers is read-only\")\n",
    "\n",
    "    @property\n",
    "    def tau(self):\n",
    "        return self._tau\n",
    "\n",
    "    @tau.setter\n",
    "    def tau(self, value):\n",
    "        raise ReadOnlyError(\"tau is read-only\")\n",
    "\n",
    "    @property\n",
    "    def ckpt_interval(self):\n",
    "        return self._ckpt_interval\n",
    "\n",
    "    @ckpt_interval.setter\n",
    "    def ckpt_interval(self, value):\n",
    "        raise ReadOnlyError(\"ckpt_interval is read-only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d9d9f6f94dcae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c324adb0de1e1d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SeqActor.clone_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b867d326abd8e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SeqActor.soft_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7ddbc2f4a089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SeqActor.save_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b8c9532ef963ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SeqActor.reset_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673c92dcc73f2670",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SeqActor.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1af88b6777b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SeqActor.predict_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32d82007a82665",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(SeqActor.evaluate_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d874c7a8b64109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
