{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150df761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06804fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Union, final\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef04acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # type: ignore\n",
    "from bson.codec_options import CodecOptions\n",
    "from pymongo import MongoClient\n",
    "from pymongo.collection import Collection\n",
    "from pymongo.errors import CollectionInvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eos.data_io.config import DBConfig\n",
    "from eos.data_io.eos_struct import (\n",
    "    DataFrameDoc,\n",
    "    ObservationMeta,\n",
    "    PoolQuery,\n",
    "    veos_lifetime_start_date,\n",
    ")\n",
    "from eos.data_io.utils import eos_df_to_nested_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3eaa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from .pool import Pool  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e33f2c1",
   "metadata": {},
   "source": [
    "client_default: MongoClient = MongoClient('mongodb://localhost:27017/')\n",
    "db_default = client_default['eos']\n",
    "db_config_default = db_config_servers_by_name[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a91ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only=True)\n",
    "class MongoPool(Pool[pd.DataFrame]):\n",
    "    \"\"\"\n",
    "    A MongoDB store for storing data in a time series collection.\n",
    "\n",
    "    features:\n",
    "        timeseries interfacing pandas.DataFrame\n",
    "        multikey index for fast query\n",
    "        typed hints\n",
    "\n",
    "    Args:\n",
    "        the key leads to a config with db_name and\n",
    "        collection name with a switch for record or episode:\n",
    "            - string for db server name\n",
    "            - or string of the format \"usr:password@host:port\"\n",
    "                for mongo_cluster:\n",
    "                    Host=\"10.10.0.4\",  # url for the database server\n",
    "                    port=\"23000\",  # port for the database server\n",
    "                    user_name=\"admin\",  # username for the database server\n",
    "                    password=\"ty02ydhVqDj3QFjT\",  # password for the database server\n",
    "                    ==> mongo_key = \"admin:ty02ydhVqDj3QFjT@10.10.0.4:23000\"\n",
    "                    : regex -> RE_MONGO_KEY\n",
    "    \"\"\"\n",
    "\n",
    "    meta: ObservationMeta  # = field(default_factory=ObservationMeta)\n",
    "    db_config: DBConfig  # db_config_default\n",
    "    doc_query: dict = field(default_factory=dict)  # query dict for mongodb\n",
    "    codec_option: CodecOptions\n",
    "    query: Optional[PoolQuery] = None  # field(default_factory=PoolQuery)\n",
    "    coll_name: Optional[str] = None  # 'default'\n",
    "    collection: Optional[Collection] = None  # db_default['default']\n",
    "    client: Optional[MongoClient] = None  # client_default\n",
    "    logger: Optional[logging.Logger] = None\n",
    "    dict_logger: Optional[dict] = None\n",
    "\n",
    "    def __post_init__(\n",
    "        self,\n",
    "    ):\n",
    "        self.logger = self.logger.getChild(\"mongo_pool\")\n",
    "        super().__post_init__()\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        mongo_url = \"mongodb://\" + self.db_config.host + \":\" + self.db_config.port\n",
    "        self.client = MongoClient(\n",
    "            mongo_url,\n",
    "            username=self.db_config.user_name,\n",
    "            password=self.db_config.password,\n",
    "        )  # mongo_url = 'mongodb://host.docker.internal:27017/'\n",
    "        version = self.client.server_info()[\"version\"]\n",
    "        self.logger.info(f\"MongoDB version: {version}\", extra=self.dict_logger)\n",
    "\n",
    "        databases = self.client.list_database_names()\n",
    "        if self.db_config.database_name not in databases:\n",
    "            self.logger.info(\n",
    "                f\"{self.db_config.database_name} not in {databases}, \"\n",
    "                f\"create database {self.db_config.database_name}!\",\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "        else:\n",
    "            self.logger.info(f\"{self.db_config.database_name} exists.\")\n",
    "        # assert (\n",
    "        #     self.db_config.database_name in databases\n",
    "        # ), f\"{self.db_config.database_name} not in {databases}!\"\n",
    "        cls = self.__class__\n",
    "\n",
    "        self.logger.info(f\"Concrete type: {cls._type_T}\", extra=self.dict_logger)\n",
    "        # if self.db_config.type == \"RECORD\":\n",
    "        #     self.coll_name = self.db_config.collection_name  + self.db_config.type.lower()\n",
    "        # else:  # type is EPISODE\n",
    "        #     self.coll_name = self.db_config.collection_name  + self.db_config.type.lower()\n",
    "        self.coll_name = self.db_config.collection_name\n",
    "\n",
    "        db = None\n",
    "        try:\n",
    "            db = self.client[self.db_config.database_name]\n",
    "            db.create_collection(\n",
    "                self.coll_name,\n",
    "                timeseries={\n",
    "                    \"timeField\": \"timestamp\",  # timestamp as timeField\n",
    "                    \"metaField\": \"meta\",  # plot as meta field\n",
    "                    \"granularity\": \"seconds\",\n",
    "                },\n",
    "                codec_options=self.codec_option,  # enable tz aware\n",
    "                expireAfterSeconds=60 * 60 * 24 * 7 * 365 * 3,  # 3 years\n",
    "            )\n",
    "        except CollectionInvalid:\n",
    "            self.logger.info(f\"{self.coll_name} exists.\", extra=self.dict_logger)\n",
    "        except Exception as e:\n",
    "            self.logger.info(\n",
    "                f\"Cannot get collection {self.coll_name} \"\n",
    "                f\"out of database {self.db_config.database_name} \"\n",
    "                f\"from {self.db_config.server_name}, \"\n",
    "                f\"exception: {e}!\",\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "        # specify new TypedDict for DataFrame\n",
    "\n",
    "        # if not db:\n",
    "        #     self.collection: Collection[DataFrameDoc] = db.get_collection(  # type: ignore\n",
    "        #         self.coll_name\n",
    "        #     )\n",
    "        # else:\n",
    "        self.collection: Collection[DataFrameDoc] = db.get_collection(  # type: ignore\n",
    "            self.coll_name\n",
    "        )\n",
    "        self.doc_query = self.parse_query(self.query)\n",
    "        self.cnt = self._count(\n",
    "            self.query\n",
    "        )  # as a by-product, get the default self.doc_query\n",
    "\n",
    "    def find_item(self, doc_id: int):\n",
    "        \"\"\"\n",
    "        Find a record by id.\n",
    "        \"\"\"\n",
    "        return self.collection.find_one({\"_id\": doc_id})\n",
    "\n",
    "    def close(self):\n",
    "        self.client.close()\n",
    "        self.logger.info(f\"close mongo client\", extra=self.dict_logger)\n",
    "\n",
    "    @final\n",
    "    def drop_collection(self):\n",
    "        \"\"\"\n",
    "        Drop all the records in the db.\n",
    "        \"\"\"\n",
    "\n",
    "        self.collection.drop()\n",
    "        self.logger.info(f\"drop collection\", extra=self.dict_logger)\n",
    "\n",
    "    def parse_query(self, query: PoolQuery) -> dict:\n",
    "        \"\"\"\n",
    "        One-Trick Pony: check query is valid or not\n",
    "        return dict as mongo document query filter\n",
    "        if query is None, return empty dict\n",
    "        \"\"\"\n",
    "\n",
    "        if query is None:\n",
    "            return {}\n",
    "\n",
    "        try:\n",
    "            epi_start = query.episodestart_start  # .replace(tzinfo=None)\n",
    "            epi_end = query.episodestart_end  # .replace(tzinfo=None)\n",
    "        except KeyError as e:\n",
    "            self.logger.error(f\"KeyError: {e}\", extra=self.dict_logger)\n",
    "            raise e\n",
    "\n",
    "        if self.db_config.type == \"RECORD\":\n",
    "            timestamp_start = query.timestamp_start  # .replace(tzinfo=None)\n",
    "            timestamp_end = query.timestamp_end  # .replace(tzinfo=None)\n",
    "\n",
    "            if epi_start is None:\n",
    "                epi_start = (\n",
    "                    veos_lifetime_start_date.to_pydatetime()\n",
    "                )  # .replace(tzinfo=None)\n",
    "            if epi_end is None:\n",
    "                epi_end = pd.Timestamp.now(\n",
    "                    tz=self.meta.site.tz,  # \"Asia/Shanghai\"\n",
    "                ).to_pydatetime()  # .replace(tzinfo=None)  # until now\n",
    "            if timestamp_start is None:\n",
    "                timestamp_start = (\n",
    "                    veos_lifetime_start_date  # .to_pydatetime().replace(tzinfo=None)\n",
    "                )\n",
    "            if timestamp_end is None:\n",
    "                timestamp_end = pd.Timestamp.now(\n",
    "                    tz=self.meta.site.tz,  # \"Asia/Shanghai\"\n",
    "                ).to_pydatetime()  # .replace(tzinfo=None)  # until now\n",
    "\n",
    "            doc_query = {\n",
    "                \"$and\": [\n",
    "                    {\"meta.vehicle\": query.vehicle},\n",
    "                    {\"meta.driver\": query.driver},\n",
    "                    {\"meta.episodestart\": {\"$gt\": epi_start}},\n",
    "                    {\"meta.episodestart\": {\"$lt\": epi_end}},\n",
    "                    {\"meta.timestamp\": {\"$gt\": timestamp_start}},\n",
    "                    {\"meta.timestamp\": {\"$lt\": timestamp_end}},\n",
    "                ]\n",
    "            }\n",
    "        else:  # type is EPISODE\n",
    "            assert (\n",
    "                type(query) is PoolQuery\n",
    "            ), \"Query type doesn't match db collection type (EPISODE)!\"\n",
    "\n",
    "            if epi_start is None:\n",
    "                epi_start = (\n",
    "                    veos_lifetime_start_date.to_pydatetime()\n",
    "                )  # .replace(tzinfo=None)\n",
    "            if epi_end is None:\n",
    "                epi_end = pd.Timestamp.now(\n",
    "                    tz=\"Asia/Shanghai\"\n",
    "                ).to_pydatetime()  # .replace(tzinfo=None)  # until now\n",
    "\n",
    "            if not query.seq_len_from:\n",
    "                query.seq_len_from = 0\n",
    "\n",
    "            if not query.seq_len_to:\n",
    "                query.seq_len_to = int(\n",
    "                    1e09\n",
    "                )  # 1 bio steps is enough as upper bound >74k Years\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"sequence length from {query.seq_len_from} to {query.seq_len_to}\",  # type: ignore\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "            doc_query = {\n",
    "                \"$and\": [\n",
    "                    {\"meta.vehicle\": query.vehicle},\n",
    "                    {\"meta.driver\": query.driver},\n",
    "                    {\"meta.episodestart\": {\"$gte\": epi_start}},\n",
    "                    {\"meta.episodestart\": {\"$lte\": epi_end}},\n",
    "                    {\"meta.seq_len\": {\"$gte\": query.seq_len_from}},\n",
    "                    {\"meta.seq_len\": {\"$lte\": query.seq_len_to}},\n",
    "                ]\n",
    "            }\n",
    "\n",
    "        return doc_query\n",
    "\n",
    "    def store_record(self, episode: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Deposit the records of an episode into the db.\n",
    "        \"\"\"\n",
    "\n",
    "        # encoding DataFrame to nested dict (json format), add meta info then insert_many\n",
    "        dict_nested = eos_df_to_nested_dict(\n",
    "            episode\n",
    "        )  # or 'index'/'tight' nested dict with tuples as keys\n",
    "\n",
    "        # generate indices info (vehicle, driver, episodestart, timestamp') from DataFrame MultiIndex for meta info\n",
    "        indices_dict = [\n",
    "            {episode.index.names[i]: level for i, level in enumerate(levels)}\n",
    "            for levels in episode.index\n",
    "        ]\n",
    "        docs = [\n",
    "            DataFrameDoc(\n",
    "                timestamp=idx[\n",
    "                    \"timestamp\"\n",
    "                ]  # redundant, same as in meta['timestamp'] and 'observation'\n",
    "                .to_pydatetime()\n",
    "                .replace(\n",
    "                    microsecond=0  # mongodb timestamp is in BSON Date format, doesn't support microsecond,\n",
    "                ),  # but only for timestamp, not necessary for timestamps as timestep data\n",
    "                meta={\n",
    "                    **idx,\n",
    "                    **(self.meta.model_dump()),  # site will dump tz as IANA string as defined in Eoslocation class\n",
    "                },  # merge two dicts into meta: df.index + ObservationMeta\n",
    "                observation=dict_nested[key],\n",
    "            )\n",
    "            for (idx, key) in zip(indices_dict, dict_nested)\n",
    "        ]  # list of records, each record is a dict of timestamp, meta, observation (quadruple with timestamp)\n",
    "        # each row in rows will be a document in MongoDB\n",
    "        # docs = [{'timestamp': idx[\"timestamp\"].to_pydatetime(),  # redundant, same as in meta['timestamp']\n",
    "        #         'meta': {**idx, **(self.meta.model_dump())},  # merge two dicts into meta: df.index + ObservationMeta\n",
    "        #         'observation': dict_nested[key]}\n",
    "        #         for (idx, key) in zip(indices_dict, dict_nested)]\n",
    "        # list of records, each record is a dict of timestamp, meta, observation (quadruple with timestamp)\n",
    "\n",
    "        # use typed collection for type checking\n",
    "        try:\n",
    "            result = self.collection.insert_many(docs)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        assert result.acknowledged is True, \"Record not stored in MongoDB!\"\n",
    "        rec_inserted = self.collection.find({\"_id\": {\"$in\": result.inserted_ids}})\n",
    "        # assert (\n",
    "        #         set(rec_inserted).symmetric_difference(set(docs))\n",
    "        #         == set()  # result has to be an empty set if all records are inserted\n",
    "        # ), \"Record stored is not the same as the one inputted!\"\n",
    "        inserted_cnt = len(list(rec_inserted))\n",
    "        self.cnt = self.cnt + inserted_cnt\n",
    "        self.logger.info(\n",
    "            f\"'header': 'deposit item number', \" f\"'inserted item': '{inserted_cnt}'\",\n",
    "            extra=self.dict_logger,\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    def store_episode(self, episode: pd.DataFrame):\n",
    "        #  TODO implement episode store\n",
    "\n",
    "        # convert dataframe episode to dict\n",
    "        # encoding DataFrame to nested dict (json format), add meta info then insert_many\n",
    "        dict_nested = eos_df_to_nested_dict(\n",
    "            episode\n",
    "        )  # single key of observation timestamp, or 'index'/'tight' nested dict with tuples as keys\n",
    "        #  convert timestamp key to string for mongodb (only strings are allowed as key for mongodb item key)\n",
    "        dict_nested = {key.isoformat(): dict_nested[key] for key in dict_nested}\n",
    "\n",
    "        # generate indices info (vehicle, driver, episodestart, timestamp') from DataFrame MultiIndex for meta info\n",
    "        indices_dict = [\n",
    "            {episode.index.names[i]: level for i, level in enumerate(levels)}\n",
    "            for levels in episode.index\n",
    "        ]  # all elements in the array should have the same vehicle, driver, episodestart\n",
    "\n",
    "        meta_episode = indices_dict[0].copy()  # shallow copy is sufficient here\n",
    "        try:\n",
    "            meta_episode.pop(\n",
    "                \"timestamp\"\n",
    "            )  # delete the timestamp for the step, the rest are meta info for the episode\n",
    "        except KeyError:\n",
    "            self.logger.info(\n",
    "                f\"{{'header': 'timestamp not in the index of the episode!'}}\",\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "        meta_episode[\"seq_len\"] = len(\n",
    "            dict_nested\n",
    "        )  # add sequence length to meta for efficient querying and sampling\n",
    "\n",
    "        meta = {\n",
    "            **(self.meta.model_dump()),  # units of measurements\n",
    "            **meta_episode,  # meta information of the episode, e.g. vehicle, driver, episodestart\n",
    "        }  # merge two dicts into meta: df.index + ObservationMeta\n",
    "        doc = DataFrameDoc(\n",
    "            timestamp=indices_dict[0][\n",
    "                \"episodestart\"  # the timestamp of a mongo episode document is the episodestart\n",
    "            ]  # redundant, same as in meta['timestamp'] and 'observation'\n",
    "            .to_pydatetime()\n",
    "            .replace(\n",
    "                microsecond=0  # mongodb timestamp is in BSON Date format, doesn't support microsecond,\n",
    "            ),  # but only for timestamp, not necessary for timestamps as timestep data\n",
    "            meta=meta,  # Done add episodestart to meta\n",
    "            observation=dict_nested,\n",
    "        )\n",
    "        # list of records, each record is a dict of timestamp, meta, observation (quadruple with timestamp)\n",
    "        # each row in rows will be a document in MongoDB\n",
    "\n",
    "        # use typed collection for type checking\n",
    "        result = self.collection.insert_one(doc)\n",
    "        assert result.acknowledged is True, \"Episode not stored in MongoDB!\"\n",
    "        # _ = self.collection.find({\"_id\": {\"$in\": result.inserted_id}})\n",
    "        # assert (\n",
    "        #         doc_inserted\n",
    "        #         == doc  # result has to be an empty set if all records are inserted\n",
    "        # ), \"Record stored is not the same as the one inputted!\"\n",
    "        self.cnt = self.cnt + 1\n",
    "        self.logger.info(f\"deposit one item\", extra=self.dict_logger)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def store(self, episode: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Deposit the records of an episode into the db.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.db_config.type == \"RECORD\":\n",
    "            res = self.store_record(episode)\n",
    "        else:  # type is EPISODE\n",
    "            res = self.store_episode(episode)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def find(self, query: Union[PoolQuery | PoolQuery]) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Find records by a query dict.\n",
    "        \"\"\"\n",
    "\n",
    "        doc_query = self.parse_query(query)\n",
    "\n",
    "        cursor = self.collection.find(doc_query)\n",
    "        return pd.DataFrame(list(cursor)).drop(\"_id\", axis=1)\n",
    "\n",
    "    def delete(self, item_id):\n",
    "        \"\"\"\n",
    "        Delete a record by item id.\n",
    "        \"\"\"\n",
    "        self.cnt = self.cnt - 1\n",
    "        return self.collection.delete_one({\"_id\": item_id})\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterate over all the records in the db collection\n",
    "        using generator expression created by Cursor object returned by find().\n",
    "        \"\"\"\n",
    "        return (item for item in self.collection.find())\n",
    "\n",
    "    def _count(self, query: Optional[PoolQuery] = None):\n",
    "        \"\"\"\n",
    "        Count the number of records in the db.\n",
    "        rule is an optional dictionary specifying a rule or\n",
    "        a pipeline in mongodb\n",
    "        for episode/record document\n",
    "        \"\"\"\n",
    "\n",
    "        # if query is not None, doc_query is {}\n",
    "        doc_query = self.parse_query(query)\n",
    "        doc_count = self.collection.count_documents(doc_query)\n",
    "\n",
    "        return doc_count\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        size: int = 4,\n",
    "        *,\n",
    "        query: Optional[PoolQuery] = None,\n",
    "    ) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "            db.hyperparameters_collection.aggregate([\n",
    "                {\"$match\": {\"start_time\": {\"$exists\": False}}},\n",
    "                {\"$sample\": {\"size\": 1}}\n",
    "            ])\n",
    "\n",
    "        Sample a batch of records from the db.\n",
    "        if PoolQuery doesn't contain 'timestamp_start' and 'timestamp_end', the full episode is retrieved.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        assert size > 0\n",
    "\n",
    "        if query == self.query:  # only if self.query is None can we use self.query\n",
    "            doc_query = self.doc_query\n",
    "\n",
    "            if self.cnt <= 0:\n",
    "                self.logger.info(\n",
    "                    f\"No document for query {query}\", extra=self.dict_logger\n",
    "                )\n",
    "                return None\n",
    "            else:\n",
    "                sample_size = self.cnt\n",
    "        else:\n",
    "            self.logger.info(\n",
    "                f\"Non-Default query {query}\",\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "            doc_query = self.parse_query(query)\n",
    "            sample_size = self.collection.count_documents(doc_query)\n",
    "\n",
    "        rest_size = size\n",
    "        batch: List = []\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"sample batch from vehicle {query.vehicle} with driver {query.driver}, \"\n",
    "            f\"batch_size {size} from document number {sample_size}, \"\n",
    "            f\"episodestart from {query.episodestart_start} to {query.episodestart_end}\",\n",
    "            extra=self.dict_logger,\n",
    "        )\n",
    "        if self.db_config.type == \"RECORD\":\n",
    "            self.logger.info(\n",
    "                f\"timestamp from {query.timestamp_start} to {query.timestamp_end}\",  # type: ignore\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "\n",
    "        while rest_size > sample_size:\n",
    "            batch_cursor = self.collection.aggregate(\n",
    "                [\n",
    "                    {\n",
    "                        \"$match\": doc_query,\n",
    "                    },\n",
    "                    {\"$sample\": {\"size\": sample_size}},\n",
    "                ],\n",
    "                allowDiskUse=True,\n",
    "            )\n",
    "            # batch_sampled = list(batch_cursor)\n",
    "            # sample_size = len(batch_sampled)\n",
    "            batch = batch + list(batch_cursor)\n",
    "            rest_size = rest_size - sample_size\n",
    "        else:  # if\n",
    "            batch_cursor = self.collection.aggregate(\n",
    "                [\n",
    "                    {\n",
    "                        \"$match\": doc_query,\n",
    "                    },\n",
    "                    {\"$sample\": {\"size\": rest_size}},\n",
    "                ]\n",
    "            )\n",
    "            batch = batch + list(batch_cursor)\n",
    "\n",
    "        return pd.DataFrame(list(batch)).drop(\"_id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33052e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
