{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2524500094559986",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d05b09acb17bbe7",
   "metadata": {},
   "source": [
    "# Mongo\n",
    "\n",
    "> MongoPool class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c427d1af2c7c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp storage.pool.mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd8a250accb2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Union, final\n",
    "import logging\n",
    "import pandas as pd  # type: ignore\n",
    "from bson.codec_options import CodecOptions\n",
    "from pymongo import MongoClient\n",
    "from pymongo.collection import Collection\n",
    "from pymongo.errors import CollectionInvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aba47a806fb1dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 18:52:23.169192: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-18 18:52:23.209462: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-18 18:52:23.209515: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-18 18:52:23.209554: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-18 18:52:23.218923: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-18 18:52:23.219896: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from data_io_nbdev.config.db import DBConfig\n",
    "from data_io_nbdev.data.core import (\n",
    "    DataFrameDoc,\n",
    "    ObservationMeta,\n",
    "    PoolQuery,\n",
    "    veos_lifetime_start_date,\n",
    ")\n",
    "from data_io_nbdev.data.external.pandas_utils import eos_df_to_nested_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8facece8e15e7161",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from data_io_nbdev.storage.pool.pool import Pool  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc5c5827aa1232",
   "metadata": {},
   "source": [
    "client_default: MongoClient = MongoClient('mongodb://localhost:27017/')\n",
    "db_default = client_default['eos']\n",
    "db_config_default = db_config_servers_by_name[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbabc5bea5bff60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass(kw_only=True)\n",
    "class MongoPool(Pool[pd.DataFrame]):\n",
    "    \"\"\"\n",
    "    A MongoDB store for storing data in a time series collection.\n",
    "\n",
    "    features:\n",
    "\n",
    "        timeseries interfacing pandas.DataFrame\n",
    "        multikey index for fast query\n",
    "        typed hints\n",
    "\n",
    "    The key leads to a config with db_name and collection name with a switch for record or episode.\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "        - meta: ObservationMeta, meta information of the observation\n",
    "        - db_config: DBConfig, database configuration\n",
    "        - doc_query: dict, query dict for mongodb\n",
    "        - codec_option: CodecOptions, codec options for mongodb\n",
    "        - query: PoolQuery, query for mongodb\n",
    "        - coll_name: str, collection name\n",
    "        - collection: Collection, collection for mongodb\n",
    "        - client: MongoClient, client for mongodb\n",
    "        - logger: logging.Logger, logger for mongodb\n",
    "        - dict_logger: dict, dict for logging\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    meta: ObservationMeta  # = field(default_factory=ObservationMeta)\n",
    "    db_config: DBConfig  # db_config_default\n",
    "    doc_query: dict = field(default_factory=dict)  # query dict for mongodb\n",
    "    codec_option: CodecOptions\n",
    "    query: Optional[PoolQuery] = None  # field(default_factory=PoolQuery)\n",
    "    coll_name: Optional[str] = None  # 'default'\n",
    "    collection: Optional[Collection] = None  # db_default['default']\n",
    "    client: Optional[MongoClient] = None  # client_default\n",
    "    logger: Optional[logging.Logger] = None\n",
    "    dict_logger: Optional[dict] = None\n",
    "\n",
    "    def __post_init__(\n",
    "        self,\n",
    "    ):\n",
    "        self.logger = self.logger.getChild(\"mongo_pool\")\n",
    "        super().__post_init__()\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        mongo_url = \"mongodb://\" + self.db_config.host + \":\" + self.db_config.port\n",
    "        self.client = MongoClient(\n",
    "            mongo_url,\n",
    "            username=self.db_config.user_name,\n",
    "            password=self.db_config.password,\n",
    "        )  # mongo_url = 'mongodb://host.docker.internal:27017/'\n",
    "        version = self.client.server_info()[\"version\"]\n",
    "        self.logger.info(f\"MongoDB version: {version}\", extra=self.dict_logger)\n",
    "\n",
    "        databases = self.client.list_database_names()\n",
    "        if self.db_config.database_name not in databases:\n",
    "            self.logger.info(\n",
    "                f\"{self.db_config.database_name} not in {databases}, \"\n",
    "                f\"create database {self.db_config.database_name}!\",\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "        else:\n",
    "            self.logger.info(f\"{self.db_config.database_name} exists.\")\n",
    "        # assert (\n",
    "        #     self.db_config.database_name in databases\n",
    "        # ), f\"{self.db_config.database_name} not in {databases}!\"\n",
    "        cls = self.__class__\n",
    "\n",
    "        self.logger.info(f\"Concrete type: {cls._type_T}\", extra=self.dict_logger)\n",
    "        # if self.db_config.type == \"RECORD\":\n",
    "        #     self.coll_name = self.db_config.collection_name  + self.db_config.type.lower()\n",
    "        # else:  # type is EPISODE\n",
    "        #     self.coll_name = self.db_config.collection_name  + self.db_config.type.lower()\n",
    "        self.coll_name = self.db_config.collection_name\n",
    "\n",
    "        db = None\n",
    "        try:\n",
    "            db = self.client[self.db_config.database_name]\n",
    "            db.create_collection(\n",
    "                self.coll_name,\n",
    "                timeseries={\n",
    "                    \"timeField\": \"timestamp\",  # timestamp as timeField\n",
    "                    \"metaField\": \"meta\",  # plot as meta field\n",
    "                    \"granularity\": \"seconds\",\n",
    "                },\n",
    "                codec_options=self.codec_option,  # enable tz aware\n",
    "                expireAfterSeconds=60 * 60 * 24 * 7 * 365 * 3,  # 3 years\n",
    "            )\n",
    "        except CollectionInvalid:\n",
    "            self.logger.info(f\"{self.coll_name} exists.\", extra=self.dict_logger)\n",
    "        except Exception as e:\n",
    "            self.logger.info(\n",
    "                f\"Cannot get collection {self.coll_name} \"\n",
    "                f\"out of database {self.db_config.database_name} \"\n",
    "                f\"from {self.db_config.server_name}, \"\n",
    "                f\"exception: {e}!\",\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "        # specify new TypedDict for DataFrame\n",
    "\n",
    "        # if not db:\n",
    "        #     self.collection: Collection[DataFrameDoc] = db.get_collection(  # type: ignore\n",
    "        #         self.coll_name\n",
    "        #     )\n",
    "        # else:\n",
    "        self.collection: Collection[DataFrameDoc] = db.get_collection(  # type: ignore\n",
    "            self.coll_name\n",
    "        )\n",
    "        self.doc_query = self.parse_query(self.query)\n",
    "        self.cnt = self._count(\n",
    "            self.query\n",
    "        )  # as a by-product, get the default self.doc_query\n",
    "\n",
    "    def find_item(self, doc_id: int):\n",
    "        \"\"\"\n",
    "        Find a record by id.\n",
    "        \"\"\"\n",
    "        return self.collection.find_one({\"_id\": doc_id})\n",
    "\n",
    "    def close(self):\n",
    "        self.client.close()\n",
    "        self.logger.info(f\"close mongo client\", extra=self.dict_logger)\n",
    "\n",
    "    @final\n",
    "    def drop_collection(self):\n",
    "        \"\"\"\n",
    "        Drop all the records in the db.\n",
    "        \"\"\"\n",
    "\n",
    "        self.collection.drop()\n",
    "        self.logger.info(f\"drop collection\", extra=self.dict_logger)\n",
    "\n",
    "    def parse_query(self, query: PoolQuery) -> dict:\n",
    "        \"\"\"\n",
    "        One-Trick Pony: check query is valid or not\n",
    "        return dict as mongo document query filter\n",
    "        if query is None, return empty dict\n",
    "        \"\"\"\n",
    "\n",
    "        if query is None:\n",
    "            return {}\n",
    "\n",
    "        try:\n",
    "            epi_start = query.episodestart_start  # .replace(tzinfo=None)\n",
    "            epi_end = query.episodestart_end  # .replace(tzinfo=None)\n",
    "        except KeyError as e:\n",
    "            self.logger.error(f\"KeyError: {e}\", extra=self.dict_logger)\n",
    "            raise e\n",
    "\n",
    "        if self.db_config.type == \"RECORD\":\n",
    "            timestamp_start = query.timestamp_start  # .replace(tzinfo=None)\n",
    "            timestamp_end = query.timestamp_end  # .replace(tzinfo=None)\n",
    "\n",
    "            if epi_start is None:\n",
    "                epi_start = (\n",
    "                    veos_lifetime_start_date.to_pydatetime()\n",
    "                )  # .replace(tzinfo=None)\n",
    "            if epi_end is None:\n",
    "                epi_end = pd.Timestamp.now(\n",
    "                    tz=self.meta.site.tz,  # \"Asia/Shanghai\"\n",
    "                ).to_pydatetime()  # .replace(tzinfo=None)  # until now\n",
    "            if timestamp_start is None:\n",
    "                timestamp_start = (\n",
    "                    veos_lifetime_start_date  # .to_pydatetime().replace(tzinfo=None)\n",
    "                )\n",
    "            if timestamp_end is None:\n",
    "                timestamp_end = pd.Timestamp.now(\n",
    "                    tz=self.meta.site.tz,  # \"Asia/Shanghai\"\n",
    "                ).to_pydatetime()  # .replace(tzinfo=None)  # until now\n",
    "\n",
    "            doc_query = {\n",
    "                \"$and\": [\n",
    "                    {\"meta.vehicle\": query.vehicle},\n",
    "                    {\"meta.driver\": query.driver},\n",
    "                    {\"meta.episodestart\": {\"$gt\": epi_start}},\n",
    "                    {\"meta.episodestart\": {\"$lt\": epi_end}},\n",
    "                    {\"meta.timestamp\": {\"$gt\": timestamp_start}},\n",
    "                    {\"meta.timestamp\": {\"$lt\": timestamp_end}},\n",
    "                ]\n",
    "            }\n",
    "        else:  # type is EPISODE\n",
    "            assert (\n",
    "                type(query) is PoolQuery\n",
    "            ), \"Query type doesn't match db collection type (EPISODE)!\"\n",
    "\n",
    "            if epi_start is None:\n",
    "                epi_start = (\n",
    "                    veos_lifetime_start_date.to_pydatetime()\n",
    "                )  # .replace(tzinfo=None)\n",
    "            if epi_end is None:\n",
    "                epi_end = pd.Timestamp.now(\n",
    "                    tz=\"Asia/Shanghai\"\n",
    "                ).to_pydatetime()  # .replace(tzinfo=None)  # until now\n",
    "\n",
    "            if not query.seq_len_from:\n",
    "                query.seq_len_from = 0\n",
    "\n",
    "            if not query.seq_len_to:\n",
    "                query.seq_len_to = int(\n",
    "                    1e09\n",
    "                )  # 1 bio steps is enough as upper bound >74k Years\n",
    "\n",
    "            self.logger.info(\n",
    "                f\"sequence length from {query.seq_len_from} to {query.seq_len_to}\",  # type: ignore\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "            doc_query = {\n",
    "                \"$and\": [\n",
    "                    {\"meta.vehicle\": query.vehicle},\n",
    "                    {\"meta.driver\": query.driver},\n",
    "                    {\"meta.episodestart\": {\"$gte\": epi_start}},\n",
    "                    {\"meta.episodestart\": {\"$lte\": epi_end}},\n",
    "                    {\"meta.seq_len\": {\"$gte\": query.seq_len_from}},\n",
    "                    {\"meta.seq_len\": {\"$lte\": query.seq_len_to}},\n",
    "                ]\n",
    "            }\n",
    "\n",
    "        return doc_query\n",
    "\n",
    "    def store_record(self, episode: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Deposit the records of an episode into the db.\n",
    "        \"\"\"\n",
    "\n",
    "        # encoding DataFrame to nested dict (json format), add meta info then insert_many\n",
    "        dict_nested = eos_df_to_nested_dict(\n",
    "            episode\n",
    "        )  # or 'index'/'tight' nested dict with tuples as keys\n",
    "\n",
    "        # generate indices info (vehicle, driver, episodestart, timestamp') from DataFrame MultiIndex for meta info\n",
    "        indices_dict = [\n",
    "            {episode.index.names[i]: level for i, level in enumerate(levels)}\n",
    "            for levels in episode.index\n",
    "        ]\n",
    "        docs = [\n",
    "            DataFrameDoc(\n",
    "                timestamp=idx[\n",
    "                    \"timestamp\"\n",
    "                ]  # redundant, same as in meta['timestamp'] and 'observation'\n",
    "                .to_pydatetime()\n",
    "                .replace(\n",
    "                    microsecond=0  # mongodb timestamp is in BSON Date format, doesn't support microsecond,\n",
    "                ),  # but only for timestamp, not necessary for timestamps as timestep data\n",
    "                meta={\n",
    "                    **idx,\n",
    "                    **(\n",
    "                        self.meta.model_dump()\n",
    "                    ),  # site will dump tz as IANA string as defined in Eoslocation class\n",
    "                },  # merge two dicts into meta: df.index + ObservationMeta\n",
    "                observation=dict_nested[key],\n",
    "            )\n",
    "            for (idx, key) in zip(indices_dict, dict_nested)\n",
    "        ]  # list of records, each record is a dict of timestamp, meta, observation (quadruple with timestamp)\n",
    "        # each row in rows will be a document in MongoDB\n",
    "        # docs = [{'timestamp': idx[\"timestamp\"].to_pydatetime(),  # redundant, same as in meta['timestamp']\n",
    "        #         'meta': {**idx, **(self.meta.model_dump())},  # merge two dicts into meta: df.index + ObservationMeta\n",
    "        #         'observation': dict_nested[key]}\n",
    "        #         for (idx, key) in zip(indices_dict, dict_nested)]\n",
    "        # list of records, each record is a dict of timestamp, meta, observation (quadruple with timestamp)\n",
    "\n",
    "        # use typed collection for type checking\n",
    "        try:\n",
    "            result = self.collection.insert_many(docs)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        assert result.acknowledged is True, \"Record not stored in MongoDB!\"\n",
    "        rec_inserted = self.collection.find({\"_id\": {\"$in\": result.inserted_ids}})\n",
    "        # assert (\n",
    "        #         set(rec_inserted).symmetric_difference(set(docs))\n",
    "        #         == set()  # result has to be an empty set if all records are inserted\n",
    "        # ), \"Record stored is not the same as the one inputted!\"\n",
    "        inserted_cnt = len(list(rec_inserted))\n",
    "        self.cnt = self.cnt + inserted_cnt\n",
    "        self.logger.info(\n",
    "            f\"'header': 'deposit item number', \" f\"'inserted item': '{inserted_cnt}'\",\n",
    "            extra=self.dict_logger,\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    def store_episode(self, episode: pd.DataFrame):\n",
    "        \"\"\"Deposit a DataFrame of an episode into the db.\"\"\"\n",
    "\n",
    "        # convert dataframe episode to dict\n",
    "        # encoding DataFrame to nested dict (json format), add meta info then insert_many\n",
    "        dict_nested = eos_df_to_nested_dict(\n",
    "            episode\n",
    "        )  # single key of observation timestamp, or 'index'/'tight' nested dict with tuples as keys\n",
    "        #  convert timestamp key to string for mongodb (only strings are allowed as key for mongodb item key)\n",
    "        dict_nested = {key.isoformat(): dict_nested[key] for key in dict_nested}\n",
    "\n",
    "        # generate indices info (vehicle, driver, episodestart, timestamp') from DataFrame MultiIndex for meta info\n",
    "        indices_dict = [\n",
    "            {episode.index.names[i]: level for i, level in enumerate(levels)}\n",
    "            for levels in episode.index\n",
    "        ]  # all elements in the array should have the same vehicle, driver, episodestart\n",
    "\n",
    "        meta_episode = indices_dict[0].copy()  # shallow copy is sufficient here\n",
    "        try:\n",
    "            meta_episode.pop(\n",
    "                \"timestamp\"\n",
    "            )  # delete the timestamp for the step, the rest are meta info for the episode\n",
    "        except KeyError:\n",
    "            self.logger.info(\n",
    "                f\"{{'header': 'timestamp not in the index of the episode!'}}\",\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "        meta_episode[\"seq_len\"] = len(\n",
    "            dict_nested\n",
    "        )  # add sequence length to meta for efficient querying and sampling\n",
    "\n",
    "        meta = {\n",
    "            **(self.meta.model_dump()),  # units of measurements\n",
    "            **meta_episode,  # meta information of the episode, e.g. vehicle, driver, episodestart\n",
    "        }  # merge two dicts into meta: df.index + ObservationMeta\n",
    "        doc = DataFrameDoc(\n",
    "            timestamp=indices_dict[0][\n",
    "                \"episodestart\"  # the timestamp of a mongo episode document is the episodestart\n",
    "            ]  # redundant, same as in meta['timestamp'] and 'observation'\n",
    "            .to_pydatetime()\n",
    "            .replace(\n",
    "                microsecond=0  # mongodb timestamp is in BSON Date format, doesn't support microsecond,\n",
    "            ),  # but only for timestamp, not necessary for timestamps as timestep data\n",
    "            meta=meta,  # Done add episodestart to meta\n",
    "            observation=dict_nested,\n",
    "        )\n",
    "        # list of records, each record is a dict of timestamp, meta, observation (quadruple with timestamp)\n",
    "        # each row in rows will be a document in MongoDB\n",
    "\n",
    "        # use typed collection for type checking\n",
    "        result = self.collection.insert_one(doc)\n",
    "        assert result.acknowledged is True, \"Episode not stored in MongoDB!\"\n",
    "        # _ = self.collection.find({\"_id\": {\"$in\": result.inserted_id}})\n",
    "        # assert (\n",
    "        #         doc_inserted\n",
    "        #         == doc  # result has to be an empty set if all records are inserted\n",
    "        # ), \"Record stored is not the same as the one inputted!\"\n",
    "        self.cnt = self.cnt + 1\n",
    "        self.logger.info(f\"deposit one item\", extra=self.dict_logger)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def store(self, episode: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Deposit the records of an episode into the db.\n",
    "\n",
    "        Based on the type of the db collection, store the episode as a set of records or as a complete episode.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.db_config.type == \"RECORD\":\n",
    "            res = self.store_record(episode)\n",
    "        else:  # type is EPISODE\n",
    "            res = self.store_episode(episode)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def find(self, query: Union[PoolQuery | PoolQuery]) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Find records by a query object.\n",
    "        \"\"\"\n",
    "        doc_query = self.parse_query(query)\n",
    "\n",
    "        cursor = self.collection.find(doc_query)\n",
    "        return pd.DataFrame(list(cursor)).drop(\"_id\", axis=1)\n",
    "\n",
    "    def delete(self, item_id):\n",
    "        \"\"\"\n",
    "        Delete a record by item id.\n",
    "        \"\"\"\n",
    "        self.cnt = self.cnt - 1\n",
    "        return self.collection.delete_one({\"_id\": item_id})\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterate over all the records in the db collection\n",
    "        using generator expression created by Cursor object returned by find().\n",
    "        \"\"\"\n",
    "        return (item for item in self.collection.find())\n",
    "\n",
    "    def _count(self, query: Optional[PoolQuery] = None):\n",
    "        \"\"\"\n",
    "        Count the number of records in the db.\n",
    "\n",
    "        for episode/record document\n",
    "        \"\"\"\n",
    "\n",
    "        # if query is not None, doc_query is {}\n",
    "        doc_query = self.parse_query(query)\n",
    "        doc_count = self.collection.count_documents(doc_query)\n",
    "\n",
    "        return doc_count\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        size: int = 4,  # batch size, default 4\n",
    "        *,\n",
    "        query: Optional[PoolQuery] = None,  # query for mongodb\n",
    "    ) -> Optional[pd.DataFrame]:  # samples as one multi-indexed Pandas DataFrame\n",
    "        \"\"\"\n",
    "        Sample a batch of records from the db.\n",
    "\n",
    "        db.hyperparameters_collection.aggregate([\n",
    "            {\"$match\": {\"start_time\": {\"$exists\": False}}},\n",
    "            {\"$sample\": {\"size\": 1}}\n",
    "        ])\n",
    "\n",
    "        if PoolQuery doesn't contain 'timestamp_start' and 'timestamp_end', the full episode is retrieved.\n",
    "        \"\"\"\n",
    "\n",
    "        assert size > 0\n",
    "\n",
    "        if query == self.query:  # only if self.query is None can we use self.query\n",
    "            doc_query = self.doc_query\n",
    "\n",
    "            if self.cnt <= 0:\n",
    "                self.logger.info(\n",
    "                    f\"No document for query {query}\", extra=self.dict_logger\n",
    "                )\n",
    "                return None\n",
    "            else:\n",
    "                sample_size = self.cnt\n",
    "        else:\n",
    "            self.logger.info(\n",
    "                f\"Non-Default query {query}\",\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "            doc_query = self.parse_query(query)\n",
    "            sample_size = self.collection.count_documents(doc_query)\n",
    "\n",
    "        rest_size = size\n",
    "        batch: List = []\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"sample batch from vehicle {query.vehicle} with driver {query.driver}, \"\n",
    "            f\"batch_size {size} from document number {sample_size}, \"\n",
    "            f\"episodestart from {query.episodestart_start} to {query.episodestart_end}\",\n",
    "            extra=self.dict_logger,\n",
    "        )\n",
    "        if self.db_config.type == \"RECORD\":\n",
    "            self.logger.info(\n",
    "                f\"timestamp from {query.timestamp_start} to {query.timestamp_end}\",  # type: ignore\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "\n",
    "        while rest_size > sample_size:\n",
    "            batch_cursor = self.collection.aggregate(\n",
    "                [\n",
    "                    {\n",
    "                        \"$match\": doc_query,\n",
    "                    },\n",
    "                    {\"$sample\": {\"size\": sample_size}},\n",
    "                ],\n",
    "                allowDiskUse=True,\n",
    "            )\n",
    "            # batch_sampled = list(batch_cursor)\n",
    "            # sample_size = len(batch_sampled)\n",
    "            batch = batch + list(batch_cursor)\n",
    "            rest_size = rest_size - sample_size\n",
    "        else:  # if\n",
    "            batch_cursor = self.collection.aggregate(\n",
    "                [\n",
    "                    {\n",
    "                        \"$match\": doc_query,\n",
    "                    },\n",
    "                    {\"$sample\": {\"size\": rest_size}},\n",
    "                ]\n",
    "            )\n",
    "            batch = batch + list(batch_cursor)\n",
    "\n",
    "        return pd.DataFrame(list(batch)).drop(\"_id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6364020b511ab804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e36c17558809e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L70){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.__post_init__\n\n>      MongoPool.__post_init__ ()\n\nUser weakref finalizer to make sure close is called when the object is destroyed",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L70){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.__post_init__\n\n>      MongoPool.__post_init__ ()\n\nUser weakref finalizer to make sure close is called when the object is destroyed"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MongoPool.__post_init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6accdd67fb55e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L77){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.load\n\n>      MongoPool.load ()\n\nInitialize the pool interface\n\nThis function should:\n    - connect to db\n    - init",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L77){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.load\n\n>      MongoPool.load ()\n\nInitialize the pool interface\n\nThis function should:\n    - connect to db\n    - init"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MongoPool.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa5b0c16e5b607b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L148){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.find_item\n\n>      MongoPool.find_item (doc_id:int)\n\nFind a record by id.",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L148){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.find_item\n\n>      MongoPool.find_item (doc_id:int)\n\nFind a record by id."
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MongoPool.find_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5210bfadeb9b16c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L154){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.close\n\n>      MongoPool.close ()\n\nclose the pool, for destructor",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L154){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.close\n\n>      MongoPool.close ()\n\nclose the pool, for destructor"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MongoPool.close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57ae982d78a1df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L167){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.parse_query\n\n>      MongoPool.parse_query (query:data_io_nbdev.data.core.PoolQuery)\n\nOne-Trick Pony: check query is valid or not\nreturn dict as mongo document query filter\nif query is None, return empty dict",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L167){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.parse_query\n\n>      MongoPool.parse_query (query:data_io_nbdev.data.core.PoolQuery)\n\nOne-Trick Pony: check query is valid or not\nreturn dict as mongo document query filter\nif query is None, return empty dict"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MongoPool.parse_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e57231f07c10235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L254){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.store_record\n\n>      MongoPool.store_record (episode:pandas.core.frame.DataFrame)\n\nDeposit the records of an episode into the db.",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L254){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.store_record\n\n>      MongoPool.store_record (episode:pandas.core.frame.DataFrame)\n\nDeposit the records of an episode into the db."
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MongoPool.store_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec073cb050ee353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L314){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.store_episode\n\n>      MongoPool.store_episode (episode:pandas.core.frame.DataFrame)\n\nDeposit a DataFrame of an episode into the db.",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L314){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.store_episode\n\n>      MongoPool.store_episode (episode:pandas.core.frame.DataFrame)\n\nDeposit a DataFrame of an episode into the db."
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MongoPool.store_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdf034567f8bfd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L376){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.store\n\n>      MongoPool.store (episode:pandas.core.frame.DataFrame)\n\nDeposit the records of an episode into the db.\n\nBased on the type of the db collection, store the episode as a set of records or as a complete episode.",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L376){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.store\n\n>      MongoPool.store (episode:pandas.core.frame.DataFrame)\n\nDeposit the records of an episode into the db.\n\nBased on the type of the db collection, store the episode as a set of records or as a complete episode."
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MongoPool.store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a584153b8f0cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L388){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.find\n\n>      MongoPool.find (query:data_io_nbdev.data.core.PoolQuery)\n\nFind records by a query object.",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L388){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.find\n\n>      MongoPool.find (query:data_io_nbdev.data.core.PoolQuery)\n\nFind records by a query object."
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MongoPool.find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723eaa12f509bbd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L398){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.delete\n\n>      MongoPool.delete (item_id)\n\nDelete a record by item id.",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L398){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.delete\n\n>      MongoPool.delete (item_id)\n\nDelete a record by item id."
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MongoPool.delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed2d4238ff785d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L412){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool._count\n\n>      MongoPool._count (query:Optional[data_io_nbdev.data.core.PoolQuery]=None)\n\nCount the number of records in the db.\n\nfor episode/record document",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L412){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool._count\n\n>      MongoPool._count (query:Optional[data_io_nbdev.data.core.PoolQuery]=None)\n\nCount the number of records in the db.\n\nfor episode/record document"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MongoPool._count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c5d0089ae1c081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L426){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.sample\n\n>      MongoPool.sample (size:int=4,\n>                        query:Optional[data_io_nbdev.data.core.PoolQuery]=None)\n\nSample a batch of records from the db.\n\ndb.hyperparameters_collection.aggregate([\n    {\"$match\": {\"start_time\": {\"$exists\": False}}},\n    {\"$sample\": {\"size\": 1}}\n])\n\nif PoolQuery doesn't contain 'timestamp_start' and 'timestamp_end', the full episode is retrieved.",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/mongo.py#L426){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### MongoPool.sample\n\n>      MongoPool.sample (size:int=4,\n>                        query:Optional[data_io_nbdev.data.core.PoolQuery]=None)\n\nSample a batch of records from the db.\n\ndb.hyperparameters_collection.aggregate([\n    {\"$match\": {\"start_time\": {\"$exists\": False}}},\n    {\"$sample\": {\"size\": 1}}\n])\n\nif PoolQuery doesn't contain 'timestamp_start' and 'timestamp_end', the full episode is retrieved."
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(MongoPool.sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aa626756f0fe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
