{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f260ba9e48f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bee67e9d8440801",
   "metadata": {},
   "source": [
    "# DPG\n",
    "\n",
    "> ABC DPG class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c8b6faf8c3a326",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agent.dpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a326f164de71367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import abc\n",
    "import re\n",
    "from collections.abc import Hashable\n",
    "from dataclasses import dataclass\n",
    "from typing import ClassVar, Optional, Union\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d142f86cf8c6a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from tspace.storage.buffer.dask import DaskBuffer\n",
    "from tspace.storage.buffer.mongo import MongoBuffer\n",
    "from tspace.config.db import RE_DB_KEY, get_db_config\n",
    "from tspace.config.drivers import Driver\n",
    "from tspace.config.vehicles import Truck, TruckInCloud, trucks_by_id\n",
    "from tspace.data.core import (\n",
    "    RE_RECIPEKEY,\n",
    "    ActionSpecs,\n",
    "    ObservationMetaCloud,\n",
    "    ObservationMetaECU,\n",
    "    RewardSpecs,\n",
    "    StateSpecsCloud,\n",
    "    StateSpecsECU,\n",
    "    get_filemeta_config,\n",
    ")\n",
    "from tspace.data.external.pandas_utils import (\n",
    "    encode_episode_dataframe_from_series,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a328664f6bd6a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from tspace.agent.utils.hyperparams import HyperParamDDPG, HyperParamRDPG  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49390bedb38c1bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass(kw_only=True)\n",
    "class DPG(Hashable):\n",
    "    \"\"\"\n",
    "    Base class for differentiable policy gradient methods\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "        truck_type: class variable `Truck` Type,\n",
    "        rdpg_hyper_type: class variable `HyperParamRDPG`\n",
    "        _truck: Truck object\n",
    "        _driver: Driver object\n",
    "        _resume: bool type, whether to resume training or start from scratch\n",
    "        _coll_type: str, either \"RECORD\" or \"EPISODE\"\n",
    "        _hyper_param: either HyperParamDDPG or HyperParamRDPG object\n",
    "        _pool_key: str, database account, password, host and port specs\n",
    "        _data_folder: str, root for data folder\n",
    "        _infer_mode: bool, either pure inferring and no training or both inferring and training\n",
    "        _buffer: Buffer object, either `MongoBuffer` or `DaskBuffer`\n",
    "        _episdoe_start_dt: Timestamp, starting time of the current episode\n",
    "        -observation_meta: metadata of the observation, either from Cloud or from Kvaser\n",
    "        _torque_table_row_name: list of str, ['r0', 'r1', 'r2', ...]\n",
    "        _observations: list of pd.Series, the observation quadruple (s, a, r, s')\n",
    "        _epi_no: int, sequence number of the episode\n",
    "        logger: logging.Logger, logging object\n",
    "        dict_logger: dict, logging format specs\n",
    "    \"\"\"\n",
    "\n",
    "    truck_type: ClassVar[Truck] = trucks_by_id[\n",
    "        \"VB7_FIELD\"\n",
    "    ]  # class attribute for default truck properties, used for rdpg input_signature spec of tf.function\n",
    "    rdpg_hyper_type: ClassVar[HyperParamRDPG] = HyperParamRDPG()\n",
    "\n",
    "    _truck: Truck\n",
    "    _driver: Driver\n",
    "    _resume: bool  # True\n",
    "    # as last of non-default parameters, so that derived class can override with default\n",
    "    _coll_type: str  # \"RECORD\"  # or 'EPISODE', used for create different buffer and pool\n",
    "    _hyper_param: Union[\n",
    "        HyperParamDDPG, HyperParamRDPG\n",
    "    ]  # field(default_factory=HyperParamDDPG)\n",
    "    _pool_key: str  # \"mongo_local\"  # 'mongo_***'\n",
    "    # or 'veos:asdf@localhost:27017' for database access\n",
    "    # or 'recipe.ini': when combined with _data_folder, indicate the configparse ini file for local file access\n",
    "    _data_folder: str  # \"./\"\n",
    "    _infer_mode: bool  # False\n",
    "    # Following are derived from above\n",
    "    _buffer: Optional[\n",
    "        Union[MongoBuffer, DaskBuffer]\n",
    "    ] = None  # field(default_factory=MongoBuffer)\n",
    "    _episode_start_dt: Optional[pd.Timestamp] = None  # datetime.now()\n",
    "    _observation_meta: Optional[\n",
    "        Union[ObservationMetaCloud, ObservationMetaECU]\n",
    "    ] = None  # field(default_factory=ObservationMetaCloud)\n",
    "    _torque_table_row_names: Optional[\n",
    "        list[str]\n",
    "    ] = None  # field(default_factory=list[str])\n",
    "    _observations: Optional[\n",
    "        list[pd.Series]\n",
    "    ] = None  # field(default_factory=list[pd.Series])\n",
    "    _epi_no: Optional[int] = None\n",
    "    logger: Optional[logging.Logger] = None  # logging.Logger(\"eos.agent.ddpg.ddpg\")\n",
    "    dict_logger: Optional[dict] = None  # dict_logger\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the DPG object.\n",
    "\n",
    "        Heavy lifting of data interface (buffer, pool) for both DDPG and RDPG\n",
    "        \"\"\"\n",
    "        # pass\n",
    "        # Number of \"experiences\" to store     at max\n",
    "        # Num of tuples to train on.\n",
    "\n",
    "        self.epi_no = 0\n",
    "        # dt = datetime.now()\n",
    "        # dt_milliseconds = int(dt.microsecond / 1000) * 1000\n",
    "        # self.episode_start_dt = dt.replace(microsecond=dt_milliseconds)\n",
    "        self.episode_start_dt = pd.Timestamp.now(self.truck.site.tz)\n",
    "\n",
    "        #  init observation meta info object,\n",
    "        #  episode start time will be updated for each episode, for now it is the time when the algo is created\n",
    "\n",
    "        # connect truck interface and Cloud or Ecu interface\n",
    "        if isinstance(self.truck, TruckInCloud):\n",
    "            self.observation_meta = ObservationMetaCloud(\n",
    "                state_specs=StateSpecsCloud(),\n",
    "                action_specs=ActionSpecs(\n",
    "                    action_unit_code=\"nm\",\n",
    "                    action_row_number=self.truck.torque_table_row_num_flash,\n",
    "                    action_column_number=self.truck.torque_table_col_num,\n",
    "                ),\n",
    "                reward_specs=RewardSpecs(reward_unit_code=\"wh\", reward_number=1),\n",
    "                site=self.truck.site,\n",
    "            )\n",
    "        else:  # Kvaser\n",
    "            self.observation_meta = ObservationMetaECU(\n",
    "                state_specs=StateSpecsECU(),\n",
    "                action_specs=ActionSpecs(\n",
    "                    action_unit_code=\"nm\",\n",
    "                    action_row_number=self.truck.torque_table_row_num_flash,\n",
    "                    action_column_number=self.truck.torque_table_col_num,\n",
    "                ),\n",
    "                reward_specs=RewardSpecs(reward_unit_code=\"wh\", reward_number=1),\n",
    "                site=self.truck.site,\n",
    "            )  # use default\n",
    "\n",
    "        (\n",
    "            self.truck.observation_numel,\n",
    "            self.truck.torque_flash_numel,\n",
    "        ) = self.observation_meta.get_number_of_states_actions()\n",
    "\n",
    "        self.torque_table_row_names = (\n",
    "            self.observation_meta.get_torque_table_row_names()\n",
    "        )  # part of the action MultiIndex\n",
    "        login_pattern = re.compile(RE_DB_KEY)\n",
    "        recipe_pattern = re.compile(RE_RECIPEKEY)\n",
    "        # if pool_key is an url or a mongodb name\n",
    "        if \"mongo\" in self.pool_key.lower() or login_pattern.match(self.pool_key):\n",
    "            # TODO coll_type needs to be passed in for differentiation between RECORD and EPISODE\n",
    "            db_config = get_db_config(self.pool_key)\n",
    "            self.buffer = MongoBuffer(  # choose item type: Record/Episode\n",
    "                db_config=db_config,\n",
    "                batch_size=self.hyper_param.BatchSize,\n",
    "                driver=self.driver,\n",
    "                truck=self.truck,\n",
    "                meta=self.observation_meta,\n",
    "                torque_table_row_names=self.torque_table_row_names,\n",
    "                logger=self.logger,\n",
    "                dict_logger=self.dict_logger,\n",
    "            )\n",
    "        elif self.pool_key is None or recipe_pattern.match(\n",
    "            self.pool_key\n",
    "        ):  # if pool_key is an ini filename, use parquet as pool\n",
    "            recipe = get_filemeta_config(\n",
    "                data_folder=self.data_folder,\n",
    "                config_file=self.pool_key,\n",
    "                meta=self.observation_meta,\n",
    "                coll_type=self.coll_type,\n",
    "            )\n",
    "            self.buffer = DaskBuffer(\n",
    "                recipe=recipe,\n",
    "                batch_size=self.hyper_param.BatchSize,\n",
    "                driver=self.driver,\n",
    "                truck=self.truck,\n",
    "                meta=self.observation_meta,\n",
    "                torque_table_row_names=self.torque_table_row_names,\n",
    "                logger=self.logger,\n",
    "                dict_logger=self.dict_logger,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"pool_key {self.pool_key} is not a valid mongodb login string nor an ini filename.\"\n",
    "            )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"DPG({self.truck.vid}, {self.driver.pid})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"DPG\"\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.__repr__())\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def touch_gpu(self):\n",
    "        \"\"\"warm up gpu for computing\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def init_checkpoint(self):\n",
    "        \"\"\"\n",
    "        Actor create or restore from checkpoint\n",
    "\n",
    "        add checkpoints manager\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def actor_predict(self, state: pd.Series):\n",
    "        \"\"\"\n",
    "        Evaluate the actors given a single observations.\n",
    "\n",
    "        batch_size is 1.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def start_episode(self, ts: pd.Timestamp):\n",
    "        \"\"\"initialize observation list\"\"\"\n",
    "        # self.logger.info(f'Episode start at {dt}', extra=self.dict_logger)\n",
    "        # somehow mongodb does not like microseconds in rec['plot']\n",
    "        # ts_milliseconds = int(ts.microsecond / 1000) * 1000\n",
    "        # self.episode_start_dt = ts.replace(microsecond=ts_milliseconds)\n",
    "        self.episode_start_dt = ts\n",
    "\n",
    "        self.observations: list[\n",
    "            pd.Series\n",
    "        ] = []  # create a new empty list for each episode\n",
    "\n",
    "    # @abc.abstractmethod\n",
    "    def deposit(\n",
    "        self,\n",
    "        timestamp: pd.Timestamp,  #   timestamp of the quadruple\n",
    "        state: pd.Series,  # state, pd.Series [brake row -> thrust row  -> timestep row -> velocity row ]\n",
    "        action: pd.Series,  # action, pd.Series [r0, r1, r2, ... rows -> speed row -> throttle row-> (flash) timestep row ]\n",
    "        reward: pd.Series,  # reward, pd.Series [timestep row -> work row]\n",
    "        nstate: pd.Series,  # next state, like state\n",
    "    ):\n",
    "        \"\"\"Deposit the experience quadruple into the replay buffer.\"\"\"\n",
    "        # Create MultiIndex\n",
    "        timestamp_ser = pd.Series(\n",
    "            [timestamp], name=\"timestamp\"\n",
    "        )  # timestamp_ser has only one element\n",
    "        timestamp_ser.index = pd.MultiIndex.from_product(\n",
    "            [timestamp_ser.index, [0]], names=[\"rows\", \"idx\"]\n",
    "        )\n",
    "        timestamp_index = (timestamp_ser.name, \"\", 0)  # triple index (name, row, idx)\n",
    "        state_index = [(state.name, *i) for i in state.index]\n",
    "        reward_index = [(reward.name, *i) for i in reward.index]\n",
    "        action_index = [(action.name, *i) for i in action.index]\n",
    "        # nstate.name = 'nstate'  # fix: to distinguish from state\n",
    "        nstate_index = [(\"nstate\", *i) for i in nstate.index]\n",
    "\n",
    "        multiindex = pd.MultiIndex.from_tuples(\n",
    "            [timestamp_index, *state_index, *action_index, *reward_index, *nstate_index]\n",
    "        )\n",
    "        observation_list = [timestamp_ser, state, action, reward, nstate]\n",
    "        observation = pd.concat(\n",
    "            observation_list\n",
    "        )  # concat Series along MultiIndex, still a Series\n",
    "        observation.index = multiindex  # each observation is a series for the quadruple (s,a,r,s') with a MultiIndex\n",
    "\n",
    "        assert (\n",
    "            type(observation) is pd.Series\n",
    "        ), f\"observation is not a Series, but {type(observation)}\"\n",
    "        assert self.observations is not None, \"self.observations is None\"\n",
    "        self.observations.append(\n",
    "            observation  # type: ignore\n",
    "        )  # each observation is a series for the quadruple (s,a,r,s')\n",
    "\n",
    "    # @abc.abstractmethod\n",
    "    def end_episode(self):\n",
    "        \"\"\"\n",
    "        Deposit the whole episode of experience into the replay buffer for DPG.\n",
    "        \"\"\"\n",
    "        self.deposit_episode()\n",
    "        self.epi_no += 1\n",
    "\n",
    "    def deposit_episode(self):\n",
    "        \"\"\"Deposit the whole episode of experience into the replay buffer for DPG.\"\"\"\n",
    "\n",
    "        episode = encode_episode_dataframe_from_series(\n",
    "            observations=self.observations,\n",
    "            torque_table_row_names=self.torque_table_row_names,\n",
    "            episode_start_dt=self.episode_start_dt,\n",
    "            driver_str=self.driver.pid,\n",
    "            truck_str=self.truck.vid,\n",
    "        )\n",
    "        self.buffer.store(episode)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the actor and critic moving network.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            tuple: (actor_loss, critic_loss, value_loss)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_losses(self):\n",
    "        \"\"\"\n",
    "        Get the actor and critic losses without calculating the gradients.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def soft_update_target(self):\n",
    "        \"\"\"\n",
    "        update target networks with tiny tau value, typical value 0.001.\n",
    "\n",
    "        It'll be done once after each batch, slowly update target by Polyak averaging.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def save_ckpt(self):\n",
    "        \"\"\"\n",
    "        save checkpoints of actor and critic\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def pool_key(self) -> str:\n",
    "        return self._pool_key\n",
    "\n",
    "    @pool_key.setter\n",
    "    def pool_key(self, value: str):\n",
    "        raise AttributeError(\"pool_key is read-only\")\n",
    "\n",
    "    @property\n",
    "    def truck(self):\n",
    "        return self._truck\n",
    "\n",
    "    @truck.setter\n",
    "    def truck(self, value):\n",
    "        raise AttributeError(\"truck is read-only\")\n",
    "\n",
    "    @property\n",
    "    def driver(self):\n",
    "        return self._driver\n",
    "\n",
    "    @driver.setter\n",
    "    def driver(self, value):\n",
    "        raise AttributeError(\"driver is read-only\")\n",
    "\n",
    "    @property\n",
    "    def data_folder(self) -> str:\n",
    "        return self._data_folder\n",
    "\n",
    "    @data_folder.setter\n",
    "    def data_folder(self, value: str):\n",
    "        raise AttributeError(\"datafolder is read-only\")\n",
    "\n",
    "    @property\n",
    "    def resume(self):\n",
    "        return self._resume\n",
    "\n",
    "    @resume.setter\n",
    "    def resume(self, value):\n",
    "        raise AttributeError(\"resume is read-only\")\n",
    "\n",
    "    @property\n",
    "    def infer_mode(self):\n",
    "        return self._infer_mode\n",
    "\n",
    "    @infer_mode.setter\n",
    "    def infer_mode(self, value):\n",
    "        raise AttributeError(\"infer_mode is read-only\")\n",
    "\n",
    "    @property\n",
    "    def episode_start_dt(self) -> Optional[pd.Timestamp]:\n",
    "        return self._episode_start_dt\n",
    "\n",
    "    @episode_start_dt.setter\n",
    "    def episode_start_dt(self, value: pd.Timestamp):\n",
    "        self._episode_start_dt = value\n",
    "\n",
    "    @property\n",
    "    def observation_meta(\n",
    "        self,\n",
    "    ) -> Optional[Union[ObservationMetaCloud, ObservationMetaECU]]:\n",
    "        return self._observation_meta\n",
    "\n",
    "    @observation_meta.setter\n",
    "    def observation_meta(self, value: Union[ObservationMetaCloud, ObservationMetaECU]):\n",
    "        self._observation_meta = value\n",
    "\n",
    "    @property\n",
    "    def buffer(self) -> Optional[Union[MongoBuffer, DaskBuffer]]:\n",
    "        return self._buffer\n",
    "\n",
    "    @buffer.setter\n",
    "    def buffer(self, value: Union[MongoBuffer, DaskBuffer]):\n",
    "        self._buffer = value\n",
    "\n",
    "    @property\n",
    "    def coll_type(self) -> str:\n",
    "        return self._coll_type\n",
    "\n",
    "    @coll_type.setter\n",
    "    def coll_type(self, value: str):\n",
    "        self._coll_type = value\n",
    "\n",
    "    @property\n",
    "    def observations(self) -> Optional[list[pd.Series]]:\n",
    "        return self._observations\n",
    "\n",
    "    @observations.setter\n",
    "    def observations(self, value: list[pd.Series]):\n",
    "        self._observations = value\n",
    "\n",
    "    @property\n",
    "    def epi_no(self) -> Optional[int]:\n",
    "        return self._epi_no\n",
    "\n",
    "    @epi_no.setter\n",
    "    def epi_no(self, value: int):\n",
    "        self._epi_no = value\n",
    "\n",
    "    @property\n",
    "    def torque_table_row_names(self) -> Optional[list[str]]:\n",
    "        return self._torque_table_row_names\n",
    "\n",
    "    @torque_table_row_names.setter\n",
    "    def torque_table_row_names(self, value: list[str]):\n",
    "        self._torque_table_row_names = value\n",
    "\n",
    "    @property\n",
    "    def hyper_param(self) -> Union[HyperParamDDPG, HyperParamRDPG]:\n",
    "        return self._hyper_param\n",
    "\n",
    "    @hyper_param.setter\n",
    "    def hyper_param(self, value: Union[HyperParamDDPG, HyperParamRDPG]):\n",
    "        self._hyper_param = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331f45cccaa3598d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d826414bc679b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DPG.__post_init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf24eb6877ed1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DPG.touch_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c96b29173abfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DPG.init_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e992e0b25405ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DPG.actor_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674cea73a75a8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DPG.start_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea556bf6c4d133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DPG.deposit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2573228ac8b2e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DPG.end_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32b22b480f90050",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DPG.deposit_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ebd9b5d9daa873",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DPG.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f0b9c684d168fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DPG.get_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e5226816993551",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DPG.soft_update_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597af4d6908b9b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DPG.save_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99889a0f2d7894fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
