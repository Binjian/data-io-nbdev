{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDQL \n",
    "\n",
    "> IDQL class\n",
    "> \n",
    ">Title: Implicit Q-Learning with Diffusion Policy\n",
    ">Author: Binjian Xin\n",
    ">Date created: 2024/06/20\n",
    ">Last modified: 2024/06/20\n",
    ">Description: Agent from IDQL repo\n",
    ">\n",
    ">\n",
    ">Title: IDQL: Implicit Q-learning as an Actor-Critic Meithod with Diffusion Policies\n",
    ">Author: Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien Kuba, Sergey levine \n",
    ">Description: Implementing IDQL algorithm on VEOS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agent.idql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from typeguard import check_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from tspace.agent.utils.hyperparams import HyperParamRDPG\n",
    "from tspace.storage.buffer.dask import DaskBuffer\n",
    "from tspace.storage.buffer.mongo import MongoBuffer  # type: ignore\n",
    "from tspace.data.core import PoolQuery  # type: ignore\n",
    "from tspace.data.time import veos_lifetime_end_date, veos_lifetime_start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from tspace.agent.dpg import DPG  # type: ignore\n",
    "from tspace.agent.rdpg.seq_actor import SeqActor  # type: ignore\n",
    "from tspace.agent.rdpg.seq_critic import SeqCritic  # type: ignore\n",
    "from tspace.agent.utils.hyperparams import HyperParamRDPG  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| hide\n",
    "from tspace.config.vehicles import Truck, TruckInCloud, trucks_by_id\n",
    "from tspace.config.drivers import Driver\n",
    "import logging\n",
    "from typing import Union\n",
    "from tspace.data.core import (\n",
    "    RE_RECIPEKEY,\n",
    "    ActionSpecs,\n",
    "    ObservationMetaCloud,\n",
    "    ObservationMetaECU,\n",
    "    RewardSpecs,\n",
    "    StateSpecsCloud,\n",
    "    StateSpecsECU,\n",
    "    get_filemeta_config,\n",
    ")\n",
    "\n",
    "from tspace.agent.utils.hyperparams import HyperParamDDPG  # type: ignore\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class IDQL(DPG):\n",
    "    \"\"\"IDQL agent for VEOS.\n",
    "\n",
    "    Abstracts:\n",
    "\n",
    "        data interface:\n",
    "            - pool in mongodb\n",
    "            - buffer in memory (numpy array)\n",
    "        model interface:\n",
    "            - actor network\n",
    "            - critic network\n",
    "\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "        actor_net: actor network\n",
    "        critic_net: critic network\n",
    "        target_actor_net: target actor network\n",
    "        target_critic_net: target critic network\n",
    "        _ckpt_actor_dir: checkpoint directory for actor\n",
    "        _ckpt_critic_dir: checkpoint directory for critic\n",
    "    \"\"\"\n",
    "\n",
    "    # Following are derived\n",
    "    actor_net: Optional[SeqActor] = None  # actor_net_default\n",
    "    critic_net: Optional[SeqCritic] = None  # critic_net_default\n",
    "    target_actor_net: Optional[SeqActor] = None  # actor_net_default\n",
    "    target_critic_net: Optional[SeqCritic] = None  # critic_net_default\n",
    "    _ckpt_actor_dir: Optional[Path] = None  # Path(\"\")\n",
    "    _ckpt_critic_dir: Optional[Path] = None  # Path(\"\")\n",
    "\n",
    "    def __post_init__(\n",
    "        self,\n",
    "    ):\n",
    "        \"\"\"initialize the rdpg agent.\n",
    "\n",
    "        args:\n",
    "            truck.ObservationNumber (int): dimension of the state space.\n",
    "            padding_value (float): value to pad the state with, impossible value for observation, action or re\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger = self.logger.getChild(\"eos\").getChild(self.__str__())\n",
    "        self.logger.propagate = True\n",
    "\n",
    "        super().__post_init__()  # call DPG post_init for pool init and plot init\n",
    "        self.coll_type = \"EPISODE\"\n",
    "        self.hyper_param = HyperParamRDPG(\n",
    "            HiddenDimension=256,\n",
    "            PaddingValue=-10000,\n",
    "            tbptt_k1=200,\n",
    "            tbptt_k2=200,\n",
    "            BatchSize=4,\n",
    "            NStates=self.truck.observation_numel,\n",
    "            NActions=self.truck.torque_flash_numel,\n",
    "            ActionBias=self.truck.torque_bias,\n",
    "            NLayerActor=2,\n",
    "            NLayerCritic=2,\n",
    "            Gamma=0.99,\n",
    "            TauActor=0.005,\n",
    "            TauCritic=0.005,\n",
    "            ActorLR=0.001,\n",
    "            CriticLR=0.001,\n",
    "            CkptInterval=5,\n",
    "        )\n",
    "\n",
    "        self.buffer.query = PoolQuery(\n",
    "            vehicle=self.truck.vid,\n",
    "            driver=self.driver.pid,\n",
    "            episodestart_start=veos_lifetime_start_date,\n",
    "            episodestart_end=veos_lifetime_end_date,\n",
    "            seq_len_from=1,  # from 10  # sample sequence with a length from 1 to 200\n",
    "            seq_len_to=self.hyper_param.tbptt_k1 + 100,  # to 300\n",
    "        )\n",
    "        self.buffer.pool.query = self.buffer.query\n",
    "\n",
    "        # actor network (w/ target network)\n",
    "        self.init_checkpoint()\n",
    "\n",
    "        self.actor_net = SeqActor(\n",
    "            self.truck.observation_numel,\n",
    "            self.truck.torque_flash_numel,\n",
    "            self.hyper_param.HiddenDimension,  # 256\n",
    "            self.hyper_param.NLayerActor,  # 2\n",
    "            self.hyper_param.BatchSize,  # 4\n",
    "            self.hyper_param.PaddingValue,  # -10000\n",
    "            self.hyper_param.TauActor,  # 0.005\n",
    "            self.hyper_param.ActorLR,  # 0.001\n",
    "            self._ckpt_actor_dir,\n",
    "            self.hyper_param.CkptInterval,  # 5\n",
    "            self.logger,\n",
    "            self.dict_logger,\n",
    "        )\n",
    "\n",
    "        self.target_actor_net = SeqActor(\n",
    "            self.truck.observation_numel,\n",
    "            self.truck.torque_flash_numel,\n",
    "            self.hyper_param.HiddenDimension,  # 256\n",
    "            self.hyper_param.NLayerActor,  # 2\n",
    "            self.hyper_param.BatchSize,  # 4\n",
    "            self.hyper_param.PaddingValue,  # -10000\n",
    "            self.hyper_param.TauActor,  # 0.005\n",
    "            self.hyper_param.ActorLR,  # 0.001\n",
    "            self._ckpt_actor_dir,\n",
    "            self.hyper_param.CkptInterval,  # 5\n",
    "            self.logger,\n",
    "            self.dict_logger,\n",
    "        )\n",
    "        # clone necessary for the first time training\n",
    "        self.target_actor_net.clone_weights(self.actor_net)\n",
    "\n",
    "        # critic network (w/ target network)\n",
    "\n",
    "        self.critic_net = SeqCritic(\n",
    "            self.truck.observation_numel,\n",
    "            self.truck.torque_flash_numel,\n",
    "            self.hyper_param.HiddenDimension,  # 256\n",
    "            self.hyper_param.NLayerCritic,  # 2\n",
    "            self.hyper_param.BatchSize,  # 4\n",
    "            self.hyper_param.PaddingValue,  # -10000\n",
    "            self.hyper_param.TauCritic,  # 0.005\n",
    "            self.hyper_param.CriticLR,  # 0.002\n",
    "            self._ckpt_critic_dir,\n",
    "            self.hyper_param.CkptInterval,  # 5\n",
    "            self.logger,\n",
    "            self.dict_logger,\n",
    "        )\n",
    "\n",
    "        self.target_critic_net = SeqCritic(\n",
    "            self.truck.observation_numel,\n",
    "            self.truck.torque_flash_numel,\n",
    "            self.hyper_param.HiddenDimension,  # 256\n",
    "            self.hyper_param.NLayerCritic,  # 2\n",
    "            self.hyper_param.BatchSize,  # 4\n",
    "            self.hyper_param.PaddingValue,  # -10000\n",
    "            self.hyper_param.TauCritic,  # 0.005\n",
    "            self.hyper_param.CriticLR,  # 0.002\n",
    "            self._ckpt_critic_dir,\n",
    "            self.hyper_param.CkptInterval,  # 5\n",
    "            self.logger,\n",
    "            self.dict_logger,\n",
    "        )\n",
    "        # clone necessary for the first time training\n",
    "        self.target_critic_net.clone_weights(self.critic_net)\n",
    "        self.touch_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
