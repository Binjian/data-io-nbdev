{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDQL \n",
    "\n",
    "> IDQL class\n",
    "> \n",
    ">Title: Implicit Q-Learning with Diffusion Policy\n",
    ">Author: Binjian Xin\n",
    ">Date created: 2024/06/20\n",
    ">Last modified: 2024/06/20\n",
    ">Description: Agent from IDQL repo\n",
    ">\n",
    ">\n",
    ">Title: IDQL: Implicit Q-learning as an Actor-Critic Meithod with Diffusion Policies\n",
    ">Author: Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien Kuba, Sergey levine \n",
    ">Description: Implementing IDQL algorithm on VEOS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agent.idql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from typeguard import check_type\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from tspace.agent.utils.hyperparams import HyperParamDDPG, HyperParamRDPG, HyperParamIDQL\n",
    "from tspace.storage.buffer.dask import DaskBuffer\n",
    "from tspace.storage.buffer.mongo import MongoBuffer  # type: ignore\n",
    "from tspace.data.core import PoolQuery  # type: ignore\n",
    "from tspace.data.time import veos_lifetime_end_date, veos_lifetime_start_date\n",
    "from tspace.data.core import PoolQuery  # type: ignore\n",
    "from tspace.data.time import veos_lifetime_end_date, veos_lifetime_start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from tspace.agent.dpg import DPG  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from tspace.config.vehicles import Truck, TruckInCloud, trucks_by_id\n",
    "from tspace.config.drivers import Driver\n",
    "import logging\n",
    "from typing import Union\n",
    "from tspace.data.core import (\n",
    "    RE_RECIPEKEY,\n",
    "    ActionSpecs,\n",
    "    ObservationMetaCloud,\n",
    "    ObservationMetaECU,\n",
    "    RewardSpecs,\n",
    "    StateSpecsCloud,\n",
    "    StateSpecsECU,\n",
    "    get_filemeta_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/x/.pyenv/versions/miniconda3-3.11-24.1.2-0/envs/tspace/lib/python3.11/site-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if (distutils.version.LooseVersion(tf.__version__) <\n",
      "/Users/x/.pyenv/versions/miniconda3-3.11-24.1.2-0/envs/tspace/lib/python3.11/site-packages/tensorflow_probability/python/__init__.py:58: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  distutils.version.LooseVersion(required_tensorflow_version)):\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from jaxrl5.agents import DDPMIQLLearner\n",
    "from jaxrl5.types import DataType\n",
    "from flax.core import FrozenDict\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "DatasetDict = dict[str, DataType]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class IDQL(DPG):\n",
    "    \"\"\"IDQL agent for VEOS.\n",
    "\n",
    "    Abstracts:\n",
    "\n",
    "        data interface:\n",
    "            - pool in mongodb\n",
    "            - buffer in memory (numpy array)\n",
    "        model interface:\n",
    "            - actor networkVj\n",
    "            - critic network\n",
    "\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "        idql_net: actor network\n",
    "        _ckpt_idql_dir: checkpoint directory for critic\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Following are derived\n",
    "    idql_net: Optional[DDPMIQLLearner] = None  # actor_net_default\n",
    "    action_space: Optional[gym.spaces.Space] = None  # action_space_default\n",
    "    observation_space: Optional[gym.spaces.Space] = None  # action_space_default\n",
    "    _ckpt_idql_dir: Optional[Path] = None  # Path(\"\")\n",
    "\n",
    "    def __post_init__(\n",
    "        self,\n",
    "    ):\n",
    "        \"\"\"initialize the rdpg agent.\n",
    "\n",
    "        args:\n",
    "            truck.ObservationNumber (int): dimension of the state space.\n",
    "            padding_value (float): value to pad the state with, impossible value for observation, action or re\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger = self.logger.getChild(\"eos\").getChild(self.__str__())\n",
    "        self.logger.propagate = True\n",
    "\n",
    "        super().__post_init__()  # call DPG post_init for pool init and plot init\n",
    "        self.coll_type = \"EPISODE\"\n",
    "        self.hyper_param = HyperParamIDQL(\n",
    "            HiddenDimension=256,\n",
    "            PaddingValue=-10000,\n",
    "            tbptt_k1=200,\n",
    "            tbptt_k2=200,\n",
    "            BatchSize=4,\n",
    "            NStates=self.truck.observation_numel,\n",
    "            NActions=self.truck.torque_flash_numel,\n",
    "            ActionBias=self.truck.torque_bias,\n",
    "            NLayerActor=2,\n",
    "            NLayerCritic=2,\n",
    "            Gamma=0.99,\n",
    "            TauActor=0.005,\n",
    "            TauCritic=0.005,\n",
    "            ActorLR=0.001,\n",
    "            CriticLR=0.001,\n",
    "            CkptInterval=5,\n",
    "        )\n",
    "\n",
    "        self.buffer.query = PoolQuery(\n",
    "            vehicle=self.truck.vid,\n",
    "            driver=self.driver.pid,\n",
    "            episodestart_start=veos_lifetime_start_date,\n",
    "            episodestart_end=veos_lifetime_end_date,\n",
    "            seq_len_from=1,  # from 10  # sample sequence with a length from 1 to 200\n",
    "            seq_len_to=self.hyper_param.tbptt_k1 + 100,  # to 300\n",
    "        )\n",
    "        self.buffer.pool.query = self.buffer.query\n",
    "\n",
    "        # actor network (w/ target network)\n",
    "        self.init_checkpoint()\n",
    "        \n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=0.0, high=1.0, shape=(self.truck.torque_flash_numel,), dtype=np.float32\n",
    "        ) # action space is 4*17 matrix\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0.0, high=50.0, shape=(self.truck.observation_numel,), dtype=np.float32\n",
    "        )  # observation space is 30*3 matrix\n",
    "        \n",
    "\n",
    "        self.idql_net = DDPMIQLLearner.create(\n",
    "            seed = 42,\n",
    "            observation_space = self.observation_space,\n",
    "            action_space = self.action_space,\n",
    "        )\n",
    "\n",
    "        self.touch_gpu()\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"IDQL({self.truck.vid}, {self.driver.pid})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"IDQL\"\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.__repr__())\n",
    "\n",
    "    def touch_gpu(self):\n",
    "        \"\"\"touch the gpu to avoid the first time delay\"\"\"\n",
    "\n",
    "        # tf.summary.trace_on(graph=true, profiler=true)\n",
    "        # ignites manual loading of tensorflow library, \\\n",
    "        # to guarantee the real-time processing of first data in main thread\n",
    "        init_states = pd.Series(\n",
    "            np.random.rand(self.truck.observation_numel)\n",
    "        )  # state must have 30 (speed, throttle, current, voltage) 5 tuple\n",
    "\n",
    "        # init_states = tf.expand_dims(input_array, 0)  # motion states is 30*2 matrix\n",
    "\n",
    "        _ = self.actor_predict(init_states)\n",
    "        self.logger.info(\n",
    "            f\"manual load tf library by calling convert_to_tensor\",\n",
    "            extra=self.dict_logger,\n",
    "        )\n",
    "\n",
    "        self.actor_net.ou_noise.reset()\n",
    "\n",
    "        # warm up the gpu training graph execution pipeline\n",
    "        if self.buffer.pool.cnt != 0:\n",
    "            if not self.infer_mode:\n",
    "                self.logger.info(\n",
    "                    f\"rdpg warm up training!\",\n",
    "                    extra=self.dict_logger,\n",
    "                )\n",
    "                (_, _) = self.train()\n",
    "\n",
    "                self.logger.info(\n",
    "                    f\"rdpg warm up training done!\",\n",
    "                    extra=self.dict_logger,\n",
    "                )\n",
    "\n",
    "    def init_checkpoint(self):\n",
    "        \"\"\"create or restore from checkpoint\"\"\"\n",
    "\n",
    "        # actor create or restore from checkpoint\n",
    "        # add checkpoints manager\n",
    "        self._ckpt_idql_dir = Path(self.data_folder).joinpath(\n",
    "            \"tf_ckpts-\"\n",
    "            + self.__str__()\n",
    "            + \"-\"\n",
    "            + self.truck.vid\n",
    "            + \"-\"\n",
    "            + self.driver.pid\n",
    "            + \"_\"\n",
    "            + \"/actor\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            os.makedirs(self._ckpt_actor_dir)\n",
    "            self.logger.info(\n",
    "                \"created checkpoint directory for actor: %s\",\n",
    "                self._ckpt_actor_dir,\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "        except FileExistsError:\n",
    "            self.logger.info(\n",
    "                \"actor checkpoint directory already exists: %s\",\n",
    "                self._ckpt_actor_dir,\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "\n",
    "    # TODO for infer only mode, implement a method without noisy exploration.\n",
    "    def actor_predict(\n",
    "        self, state: pd.Series  # state sequence of the current episode\n",
    "    ) -> np.ndarray:  # action sequence of the current episode\n",
    "        \"\"\"\n",
    "        sample actions with additive ou noise\n",
    "        \n",
    "        input: state is a pd.Series of length 3*10*3/4*50*3 (r*c), output numpy array\n",
    "\n",
    "        Action outputs and noise object are all row vectors of length 21*17 (r*c), output numpy array\n",
    "        \"\"\"\n",
    "\n",
    "        # We make sure action is within bounds\n",
    "        # legal_action = np.clip(sampled_actions, action_lower, action_upper)\n",
    "        # get flat interleaved (not column-wise stacked) tensor from dataframe\n",
    "\n",
    "        state_flat = None\n",
    "        try:\n",
    "            state_flat = jnp.array(state.to_numpy())\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "\n",
    "        states = jnp.expand_dims(\n",
    "            check_type(state_flat, jnp.ndarray), 0\n",
    "        )  # motion states is 30*3 matrix\n",
    "\n",
    "        # sample implicit policy (not the behavior poicly), batch dimension in result is squeezed\n",
    "        sampled_actions = self.idql_net.sample_implicit_policy(states)\n",
    "        \n",
    "        self.logger.info(f\"Inference IDQL done!\", extra=self.dict_logger)\n",
    "        # return np.squeeze(sampled_actions)  # ? might be unnecessary\n",
    "        return sampled_actions\n",
    "\n",
    "\n",
    "    def sample_minibatch(self):\n",
    "        \"\"\"Convert batch type from DataFrames to flattened tensors.\"\"\"\n",
    "        if (\n",
    "            self.buffer.pool.cnt == 0\n",
    "        ):  # bootstrap for Episode 0 from the current self.observations list\n",
    "            self.logger.info(\n",
    "                f\"no data in pool, bootstrap from observation_list, \"\n",
    "                f\"truck: {self.truck.vid}, driver: {self.driver.pid}.\",\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "            assert (\n",
    "                len(self.observations) > 0\n",
    "            ), \"no data in temporary buffer self.observations!\"\n",
    "\n",
    "            # sample from self.observations\n",
    "\n",
    "            batch_idx = np.random.choice(\n",
    "                len(self.observations), self.hyper_param.BatchSize  # 4\n",
    "            )\n",
    "            observation_samples = [\n",
    "                self.observations[i] for i in batch_idx\n",
    "            ]  # a sampled list of Series\n",
    "\n",
    "            idx = pd.IndexSlice\n",
    "            state = []\n",
    "            action = []\n",
    "            reward = []\n",
    "            nstate = []\n",
    "            for (\n",
    "                observation\n",
    "            ) in (\n",
    "                observation_samples\n",
    "            ):  # each observation is a Series, contiguous storage in rows (already flattened!)\n",
    "                state.append(\n",
    "                    observation.loc[\n",
    "                        idx[\"state\", [\"velocity\", \"thrust\", \"brake\"]]\n",
    "                    ].values\n",
    "                )\n",
    "                action.append(\n",
    "                    observation.loc[idx[\"action\", self.torque_table_row_names]].values\n",
    "                )\n",
    "                reward.append(observation.loc[idx[\"reward\", [\"work\"]]].values)\n",
    "                nstate.append(\n",
    "                    observation.loc[\n",
    "                        idx[\"nstate\", [\"velocity\", \"thrust\", \"brake\"]]\n",
    "                    ].values\n",
    "                )\n",
    "\n",
    "            # convert to tensors by stacking so that the first dimension is batch_size\n",
    "            states = tf.convert_to_tensor(np.stack(state), dtype=tf.float32)\n",
    "            actions = tf.convert_to_tensor(np.stack(action), dtype=tf.float32)\n",
    "            rewards = tf.convert_to_tensor(np.stack(reward), dtype=tf.float32)\n",
    "            next_states = tf.convert_to_tensor(np.stack(nstate), dtype=tf.float32)\n",
    "\n",
    "        else:  # otherwise sample from pool, ignoring the current ongoing episode to reduce complexity\n",
    "            # TODO combine current episode with pool, need evenly sampling pool and list of observations, then combine\n",
    "            # get sampling range, if not enough data, batch is small\n",
    "            self.logger.info(\n",
    "                f\"start sample from pool with size: {self.hyper_param.BatchSize}, \"\n",
    "                f\"truck: {self.truck.vid}, driver: {self.driver.pid}.\",\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "\n",
    "            (\n",
    "                states,\n",
    "                actions,\n",
    "                rewards,\n",
    "                nstates,\n",
    "            ) = self.buffer.sample()  # for both mongo and arrow pool\n",
    "\n",
    "            states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "            actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "            rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "            next_states = tf.convert_to_tensor(nstates, dtype=tf.float32)\n",
    "\n",
    "        return states, actions, rewards, next_states\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the networks on the batch sampled from the pool.\"\"\"\n",
    "        (\n",
    "            states,\n",
    "            actions,\n",
    "            rewards,\n",
    "            next_states,\n",
    "        ) = self.sample_minibatch()\n",
    "\n",
    "        batch = FrozenDict({\"actions\": actions, \n",
    "                            \"rewards\": rewards, \n",
    "                            \"observations\": states, \n",
    "                            \"next_observations\": next_states})\n",
    "        \n",
    "        agent, info = self.agent.update(batch)\n",
    "        self.agent = agent\n",
    "        critic_loss = info[\"critic_loss\"]\n",
    "        actor_loss = info[\"actor_loss\"]\n",
    "        _ = info[\"value_loss\"]\n",
    "        return critic_loss, actor_loss\n",
    "\n",
    "\n",
    "    def soft_update_target(self):\n",
    "        \"\"\"update the target networks with Polyak averaging\"\"\"\n",
    "        # it's already done in jaxrl5 agent update\n",
    "        pass\n",
    "\n",
    "    # we only calculate the loss\n",
    "\n",
    "    def save_ckpt(self):\n",
    "        \"\"\"TODO Save the checkpoint of the actor, critic and value network in Flax.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    # We only compute the loss and don't update parameters\n",
    "    def get_losses(self):\n",
    "        \"\"\"Get the losses of the networks on the batch sampled from the pool.\"\"\"\n",
    "        (\n",
    "            states,\n",
    "            actions,\n",
    "            rewards,\n",
    "            next_states,\n",
    "        ) = self.sample_minibatch()\n",
    "\n",
    "        batch = FrozenDict({\"actions\": actions, \n",
    "                            \"rewards\": rewards, \n",
    "                            \"observations\": states, \n",
    "                            \"next_observations\": next_states})\n",
    "        \n",
    "        agent, info = self.agent.eval_loss(batch)\n",
    "\n",
    "        critic_loss = 0.0\n",
    "        actor_loss = info[\"actor_loss\"]\n",
    "        return critic_loss, actor_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/x/.pyenv/versions/miniconda3-3.11-24.1.2-0/envs/tspace/lib/python3.11/site-packages/nbdev/doclinks.py:17: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources,importlib\n",
      "/Users/x/.pyenv/versions/miniconda3-3.11-24.1.2-0/envs/tspace/lib/python3.11/site-packages/pkg_resources/__init__.py:2825: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
      "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
      "  declare_namespace(pkg)\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L57){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.__post_init__\n",
       "\n",
       ">      IDQL.__post_init__ ()\n",
       "\n",
       "*initialize the rdpg agent.\n",
       "\n",
       "args:\n",
       "    truck.ObservationNumber (int): dimension of the state space.\n",
       "    padding_value (float): value to pad the state with, impossible value for observation, action or re*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L57){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.__post_init__\n",
       "\n",
       ">      IDQL.__post_init__ ()\n",
       "\n",
       "*initialize the rdpg agent.\n",
       "\n",
       "args:\n",
       "    truck.ObservationNumber (int): dimension of the state space.\n",
       "    padding_value (float): value to pad the state with, impossible value for observation, action or re*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(IDQL.__post_init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L119){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.__repr__\n",
       "\n",
       ">      IDQL.__repr__ ()\n",
       "\n",
       "*Return repr(self).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L119){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.__repr__\n",
       "\n",
       ">      IDQL.__repr__ ()\n",
       "\n",
       "*Return repr(self).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(IDQL.__repr__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L122){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.__str__\n",
       "\n",
       ">      IDQL.__str__ ()\n",
       "\n",
       "*Return str(self).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L122){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.__str__\n",
       "\n",
       ">      IDQL.__str__ ()\n",
       "\n",
       "*Return str(self).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(IDQL.__str__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L125){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.__hash__\n",
       "\n",
       ">      IDQL.__hash__ ()\n",
       "\n",
       "*Return hash(self).*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L125){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.__hash__\n",
       "\n",
       ">      IDQL.__hash__ ()\n",
       "\n",
       "*Return hash(self).*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(IDQL.__hash__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L128){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.touch_gpu\n",
       "\n",
       ">      IDQL.touch_gpu ()\n",
       "\n",
       "*touch the gpu to avoid the first time delay*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L128){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.touch_gpu\n",
       "\n",
       ">      IDQL.touch_gpu ()\n",
       "\n",
       "*touch the gpu to avoid the first time delay*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(IDQL.touch_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L162){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.init_checkpoint\n",
       "\n",
       ">      IDQL.init_checkpoint ()\n",
       "\n",
       "*create or restore from checkpoint*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L162){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.init_checkpoint\n",
       "\n",
       ">      IDQL.init_checkpoint ()\n",
       "\n",
       "*create or restore from checkpoint*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(IDQL.init_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L193){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.actor_predict\n",
       "\n",
       ">      IDQL.actor_predict (state:pandas.core.series.Series)\n",
       "\n",
       "*sample actions with additive ou noise\n",
       "\n",
       "input: state is a pd.Series of length 3*10*3/4*50*3 (r*c), output numpy array\n",
       "\n",
       "Action outputs and noise object are all row vectors of length 21*17 (r*c), output numpy array*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| state | Series | state sequence of the current episode |\n",
       "| **Returns** | **ndarray** | **action sequence of the current episode** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L193){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.actor_predict\n",
       "\n",
       ">      IDQL.actor_predict (state:pandas.core.series.Series)\n",
       "\n",
       "*sample actions with additive ou noise\n",
       "\n",
       "input: state is a pd.Series of length 3*10*3/4*50*3 (r*c), output numpy array\n",
       "\n",
       "Action outputs and noise object are all row vectors of length 21*17 (r*c), output numpy array*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| state | Series | state sequence of the current episode |\n",
       "| **Returns** | **ndarray** | **action sequence of the current episode** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(IDQL.actor_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L225){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.sample_minibatch\n",
       "\n",
       ">      IDQL.sample_minibatch ()\n",
       "\n",
       "*Convert batch type from DataFrames to flattened tensors.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L225){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.sample_minibatch\n",
       "\n",
       ">      IDQL.sample_minibatch ()\n",
       "\n",
       "*Convert batch type from DataFrames to flattened tensors.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(IDQL.sample_minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L302){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.train\n",
       "\n",
       ">      IDQL.train ()\n",
       "\n",
       "*Train the networks on the batch sampled from the pool.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L302){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.train\n",
       "\n",
       ">      IDQL.train ()\n",
       "\n",
       "*Train the networks on the batch sampled from the pool.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(IDQL.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L327){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.soft_update_target\n",
       "\n",
       ">      IDQL.soft_update_target ()\n",
       "\n",
       "*update the target networks with Polyak averaging*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L327){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.soft_update_target\n",
       "\n",
       ">      IDQL.soft_update_target ()\n",
       "\n",
       "*update the target networks with Polyak averaging*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(IDQL.soft_update_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L334){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.save_ckpt\n",
       "\n",
       ">      IDQL.save_ckpt ()\n",
       "\n",
       "*TODO Save the checkpoint of the actor, critic and value network in Flax.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L334){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.save_ckpt\n",
       "\n",
       ">      IDQL.save_ckpt ()\n",
       "\n",
       "*TODO Save the checkpoint of the actor, critic and value network in Flax.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(IDQL.save_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L339){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.get_losses\n",
       "\n",
       ">      IDQL.get_losses ()\n",
       "\n",
       "*Get the losses of the networks on the batch sampled from the pool.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/Binjian/tspace/blob/main/tspace/agent/idql.py#L339){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### IDQL.get_losses\n",
       "\n",
       ">      IDQL.get_losses ()\n",
       "\n",
       "*Get the losses of the networks on the batch sampled from the pool.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(IDQL.get_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
