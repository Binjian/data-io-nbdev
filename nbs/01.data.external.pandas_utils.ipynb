{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925ba9082d74d377",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b38130d4ed75344",
   "metadata": {},
   "source": [
    "# Pandas utilities\n",
    "\n",
    "> utilites for auxiliary pandas DataFrame processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b79acf6cb8376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.external.pandas_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55f58841f0510c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd51d27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import math\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "from typing import Dict, List, Optional, Tuple, Union, cast\n",
    "from zoneinfo import ZoneInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eed8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b5a14a8cba841d",
   "metadata": {},
   "source": [
    "## Dataframe for state, action, reward, next_state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ccea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def assemble_state_ser(\n",
    "    state_columns: pd.DataFrame,  # state_columns: Dataframe with columns ['timestep', 'velocity', 'thrust', 'brake']\n",
    "    tz: ZoneInfo,  # timezone for the timestamp\n",
    ") -> Tuple[\n",
    "    pd.Series, int\n",
    "]:  # state: Series with index ['rows', 'idx'], table_row_start: int\n",
    "    \"\"\"\n",
    "    assemble state df from state_columns dataframe order is vital for the model\n",
    "\n",
    "    inputs:\n",
    "\n",
    "        state_columns: pd.DataFrame\n",
    "\n",
    "    \"timestep, velocity, thrust, brake\"\n",
    "    contiguous storage in each measurement\n",
    "    due to sort_index, output:\n",
    "    [col0: brake, col1: thrust, col2: timestep, col3: velocity]\n",
    "\n",
    "    return:\n",
    "\n",
    "        state: pd.Series\n",
    "        table_row_start: int\n",
    "    \"\"\"\n",
    "\n",
    "    # state_columns['timestep'] = pd.to_datetime(datetime.now().timestamp(), unit='us').tz_localize(tz)\n",
    "    state: pd.Series = cast(\n",
    "        pd.Series,\n",
    "        (state_columns.stack().swaplevel(0, 1)),\n",
    "    )\n",
    "    state.name = \"state\"\n",
    "    state.index.names = [\"rows\", \"idx\"]\n",
    "    state.sort_index(\n",
    "        inplace=True\n",
    "    )  # sort by rows and idx (brake, thrust, timestep, velocity)\n",
    "    # str_as_type = f\"datetime64[us,{tz.key}]\"  # type: ignore\n",
    "    # state['timestep'].astype(str_as_type, copy=False)\n",
    "\n",
    "    vel_stats = state[\"velocity\"].astype(\"float\").describe()\n",
    "\n",
    "    # 0~20km/h; 7~30km/h; 10~40km/h; 20~50km/h; ...\n",
    "    # average concept\n",
    "    # 10; 18; 25; 35; 45; 55; 65; 75; 85; 95; 105\n",
    "    #   13; 18; 22; 27; 32; 37; 42; 47; 52; 57; 62;\n",
    "    # here upper bound rule adopted\n",
    "    if vel_stats[\"max\"] < 20:\n",
    "        table_row_start = 0\n",
    "    elif vel_stats[\"max\"] < 30:\n",
    "        table_row_start = 1\n",
    "    elif vel_stats[\"max\"] < 120:\n",
    "        table_row_start = math.floor((vel_stats[\"max\"] - 30) / 10) + 2\n",
    "    else:\n",
    "        table_row_start = 16  # cycle higher than 120km/h!\n",
    "    # get the row of the table\n",
    "\n",
    "    return state, table_row_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38859bc049df0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(assemble_state_ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a5e526835d7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from data_io_nbdev.utils import generate_eos_df, generate_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c04c99938da6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "tz = ZoneInfo(\"Asia/Shanghai\")\n",
    "df = generate_eos_df(tz)\n",
    "df[\"state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2473448cf0698d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "state = df['state'][[\"timestep\", \"velocity\", \"thrust\", \"brake\"]].iloc[0]\n",
    "# state = df['state'][\"timestep\"]\n",
    "# state[\"timestep\"].values\n",
    "state = pd.DataFrame(\n",
    "    [state[\"timestep\"].values, state[\"velocity\"].values, state[\"thrust\"].values, state[\"brake\"].values]).T\n",
    "state.columns = [\"timestep\", \"velocity\", \"thrust\", \"brake\"]\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea670600d116dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "state_ser = state.stack().swaplevel(0, 1)\n",
    "state_ser.name = \"state\"\n",
    "state_ser.index.names = [\"rows\", \"idx\"]\n",
    "state_ser.sort_index(inplace=True)\n",
    "state_ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe1adc5d02bf592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "tz = ZoneInfo(\"Asia/Shanghai\")\n",
    "# state = df['state'].stack\n",
    "ser_state, row_start = assemble_state_ser(state, tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e266489f5beb75eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| output: true\n",
    "assemble_state_ser(state, tz)[0]  # just showd the Dataframe, ignore row_start (its's 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac750cb897f35b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert assemble_state_ser(state, tz)[1] == 0  # row_start should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eac8455ba36abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(assemble_state_ser(state, tz)[0], pd.Series) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ed3e33de79fb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf20a314185ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(isinstance(assemble_state_ser(state, tz)[0], pd.Series), True)  # use fastcore testing utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cbdedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def assemble_reward_ser(\n",
    "    power_columns: pd.DataFrame, obs_sampling_rate: int, ts\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    assemble reward df from motion_power df\n",
    "    order is vital for the model:\n",
    "    contiguous storage in each row, due to sort_index, output:\n",
    "    power_columns: ['current', 'voltage']\n",
    "    [timestep, work]\n",
    "    \"\"\"\n",
    "\n",
    "    ui_sum = power_columns.prod(axis=1).sum()\n",
    "    wh = (\n",
    "        ui_sum / 3600.0 / obs_sampling_rate\n",
    "    )  # rate 0.05 for kvaser, 0.02 remote # negative wh\n",
    "    work = wh * (-1.0)\n",
    "    reward: pd.Series = cast(\n",
    "        pd.Series,\n",
    "        (\n",
    "            pd.DataFrame({\"work\": work, \"timestep\": ts}, index=[0])\n",
    "            .stack()\n",
    "            .swaplevel(0, 1)\n",
    "            .sort_index()  # columns oder (timestep, work)\n",
    "        ),\n",
    "    )\n",
    "    reward.name = \"reward\"\n",
    "    reward.index.names = [\"rows\", \"idx\"]\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c594c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def assemble_flash_table(\n",
    "    torque_map_line: np.ndarray,\n",
    "    table_start: int,\n",
    "    torque_table_row_num_flash: int,\n",
    "    torque_table_col_num: int,\n",
    "    speed_scale: tuple,\n",
    "    pedal_scale: tuple,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    generate flash table df from torque_map_line\n",
    "    order is vital for the model:\n",
    "    contiguous storage in each row, due to sort_index, output:\n",
    "    \"r0, r1, r2, r3, ..., speed, throttle(map),timestep\"\n",
    "    \"\"\"\n",
    "    # assemble_action_df\n",
    "\n",
    "    speed_ser = pd.Series(\n",
    "        speed_scale[table_start : table_start + torque_table_row_num_flash],\n",
    "        name=\"speed\",\n",
    "    )\n",
    "    throttle_ser = pd.Series(pedal_scale, name=\"throttle\")\n",
    "    torque_table = np.reshape(\n",
    "        torque_map_line,\n",
    "        [torque_table_row_num_flash, torque_table_col_num],\n",
    "    )\n",
    "    df_torque_table = pd.DataFrame(torque_table)  # not transpose!\n",
    "    df_torque_table.index = speed_ser\n",
    "    df_torque_table.columns = throttle_ser\n",
    "\n",
    "    return df_torque_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc157348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def assemble_action_ser(\n",
    "    torque_map_line: np.ndarray,\n",
    "    torque_table_row_names: list[str],\n",
    "    table_start: int,\n",
    "    flash_start_ts: pd.Timestamp,\n",
    "    flash_end_ts: pd.Timestamp,\n",
    "    torque_table_row_num_flash: int,\n",
    "    torque_table_col_num: int,\n",
    "    speed_scale: tuple,\n",
    "    pedal_scale: tuple,\n",
    "    tz: ZoneInfo,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    generate action df from torque_map_line\n",
    "    order is vital for the model:\n",
    "    contiguous storage in each row, due to sort_index, output:\n",
    "    \"r0, r1, r2, r3, ..., speed, throttle(map),timestep\"\n",
    "    \"\"\"\n",
    "    # assemble_action_df\n",
    "    row_num = torque_table_row_num_flash\n",
    "\n",
    "    speed_ser = pd.Series(\n",
    "        speed_scale[table_start : table_start + torque_table_row_num_flash],\n",
    "        name=\"speed\",\n",
    "    )\n",
    "    throttle_ser = pd.Series(pedal_scale, name=\"throttle\")\n",
    "    torque_map = np.reshape(\n",
    "        torque_map_line,\n",
    "        [torque_table_row_num_flash, torque_table_col_num],\n",
    "    )\n",
    "    df_torque_map = pd.DataFrame(torque_map).transpose()  # row to columns\n",
    "    df_torque_map.columns = pd.Index(torque_table_row_names)  # index: [r0, r1, ...]\n",
    "    # df_torque_map.index = throttle_ser  # torque map index: if using [throttle], the index dtypes will become float!\n",
    "\n",
    "    span_each_row = (flash_end_ts - flash_start_ts) / row_num\n",
    "    flash_timestamps_ser = pd.Series(\n",
    "        [\n",
    "            pd.to_datetime(\n",
    "                flash_start_ts + step * span_each_row, utc=True, unit=\"us\"\n",
    "            ).tz_convert(tz)\n",
    "            for step in np.linspace(0, row_num, row_num)\n",
    "        ],\n",
    "        name=\"timestep\",\n",
    "    )\n",
    "\n",
    "    dfs: list[Union[pd.DataFrame, pd.Series]] = [\n",
    "        df_torque_map,\n",
    "        flash_timestamps_ser,\n",
    "        speed_ser,\n",
    "        throttle_ser,\n",
    "    ]\n",
    "    action_df: pd.DataFrame = cast(\n",
    "        pd.DataFrame,\n",
    "        reduce(\n",
    "            lambda left, right: pd.merge(\n",
    "                left,\n",
    "                right,\n",
    "                how=\"outer\",\n",
    "                left_index=True,\n",
    "                right_index=True,\n",
    "            ),\n",
    "            dfs,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    action = cast(\n",
    "        pd.Series, (action_df.stack().swaplevel(0, 1).sort_index())\n",
    "    )  # columns order (r0, r1, ..., speed, throttle, timestep)\n",
    "    action.name = \"action\"\n",
    "    action.index.names = [\"rows\", \"idx\"]\n",
    "    # action.column.names = []\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec40c0ac1fc410",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"action\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f99d264303af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = df[\"action\", \"timestep\", 0].values\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a29cc5a9d31e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"action\", \"timestep\"].iloc[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8209da1a40ddef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "idx = pd.IndexSlice\n",
    "action = df.loc[:, idx['action', [\"r0\", \"r1\", \"r2\", \"speed\", \"throttle\", \"timestep\"]]]\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe50548ccbd7491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_ser = action['action'].iloc[0]\n",
    "action_ser.name = \"action\"\n",
    "action_ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a4ab807852a085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = df['state'][\"timestep\"]\n",
    "# state[\"timestep\"].values\n",
    "actn = df[\"action\"].iloc[0]\n",
    "actn[\"r0\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1778a25cde78f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The construction of DF by raw values will lose timezone information\n",
    "# So you have to alway not directly build Dataframe from numpy array values\n",
    "action1 = pd.DataFrame(\n",
    "    [actn[\"r0\"].values, \n",
    "     actn[\"r1\"].values, \n",
    "     actn[\"r2\"].values, \n",
    "     actn[\"speed\"].values,\n",
    "     actn[\"throttle\"].values, \n",
    "     actn[\"timestep\"].values]\n",
    ").T\n",
    "action1.columns = [\"r0\", \"r1\", \"r2\", \"speed\", \"throttle\", \"timestep\"]\n",
    "action1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1b8a3a577ce946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "action1_ser = action1.stack().swaplevel(0, 1)\n",
    "action1_ser.name = \"action\"\n",
    "action1_ser.index.names = [\"rows\", \"idx\"]\n",
    "action1_ser.sort_index(inplace=True)\n",
    "action1_ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a28da6a3b95f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "torque_table_line = action_ser[[\"r0\", \"r1\", \"r2\"]].values\n",
    "flash_start_ts = action_ser[\"timestep\", 0]\n",
    "flash_end_ts = action_ser[\"timestep\", 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddb3dfaa35fde67",
   "metadata": {},
   "outputs": [],
   "source": [
    "torque_table_row_names = [\"r0\", \"r1\", \"r2\"]\n",
    "table_start = 4\n",
    "torque_table_row_num_flash = 3\n",
    "torque_table_col_num = 5\n",
    "speed_scale = (0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120)\n",
    "pedal_scale = (0, 0.25, 0.5, 0.75, 1.0)\n",
    "tz = ZoneInfo(\"Asia/Shanghai\")\n",
    "\n",
    "# state = df['state'].stack\n",
    "ser_action = assemble_action_ser(\n",
    "    torque_table_line,\n",
    "    torque_table_row_names,\n",
    "    table_start,\n",
    "    flash_start_ts,\n",
    "    flash_end_ts,\n",
    "    torque_table_row_num_flash,\n",
    "    torque_table_col_num,\n",
    "    speed_scale,\n",
    "    pedal_scale,\n",
    "    tz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56231dfeaf3092e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# test_close(ser_action, action_ser, eps=1e-6)\n",
    "# due to linspace interpolatin of timestamps , comparison will fail\n",
    "# need to implement specific validation in Pydantic way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b00b3950aa900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert assemble_state_ser(state, tz)[1] == 0  # row_start should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cfb102a1dce40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(assemble_state_ser(state, tz)[0], pd.Series) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f9a4902f3aa060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fee42abf15f1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(isinstance(assemble_state_ser(state, tz)[0], pd.Series), True)  # use fastcore testing utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77fe250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def nest(d: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a flat dictionary with tuple key to a nested dictionary through to the leaves\n",
    "    arrays will be converted to dictionaries with the index as the key\n",
    "    no conversion of pd.Timestamp\n",
    "    only for use in mongo records\n",
    "    \"\"\"\n",
    "    result: Dict = {}\n",
    "    for key, value in d.items():\n",
    "        target = result\n",
    "        for k in key[:-1]:\n",
    "            target = target.setdefault(k, {})\n",
    "        target[str(key[-1])] = value  # for mongo only string keys are allowed.\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ffebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def df_to_nested_dict(df_multi_indexed_col: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a dataframe with multi-indexed columns to a nested dictionary\n",
    "    \"\"\"\n",
    "    d = df_multi_indexed_col.to_dict(\n",
    "        \"index\"\n",
    "    )  # for multi-indexed dataframe, the index in the first level of the dictionary is still a tuple!\n",
    "    return {k: nest(v) for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8ee7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def eos_df_to_nested_dict(episode: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Convert an eos dataframe with multi-indexed columns to a nested dictionary\n",
    "    Remove all the levels of the multi-indexed columns except for 'timestamp'\n",
    "    Keep only the timestamp as the single key for the nested dictionary\n",
    "    \"\"\"\n",
    "    dict_nested = df_to_nested_dict(\n",
    "        episode\n",
    "    )  # for multi-indexed dataframe, the index in the first level of the dictionary is still a tuple!\n",
    "    indices_dict = [\n",
    "        {episode.index.names[i]: level for i, level in enumerate(levels)}\n",
    "        for levels in episode.index\n",
    "    ]  # all elements in the array should have the same vehicle, driver, episodestart\n",
    "    single_key_dict = {\n",
    "        idx[\"timestamp\"]: dict_nested[key]\n",
    "        for idx, key in zip(indices_dict, dict_nested)\n",
    "    }\n",
    "\n",
    "    return single_key_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f359cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ep_nest(d: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Convert a flat dictionary with tuple key to a nested dictionary with arrays at the leaves\n",
    "    convert pd.Timestamp to millisecond long integer\n",
    "    Timestamp with zoneinfo will be converted to UTC and then to millisecond long integer\n",
    "    \"\"\"\n",
    "    result: Dict = {}\n",
    "    for key, value in d.items():\n",
    "        target = result\n",
    "        for k in key[:-2]:\n",
    "            target = target.setdefault(k, {})\n",
    "        if key[-2] not in target:\n",
    "            target[key[-2]] = []\n",
    "\n",
    "        if isinstance(value, pd.Timestamp):\n",
    "            value = value.timestamp() * 1e6  # convert to microsecond long integer,\n",
    "        target[key[-2]].append(value)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f17eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def df_to_ep_nested_dict(df_multi_indexed_col: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a dataframe with multi-indexed columns to a nested dictionary\n",
    "    \"\"\"\n",
    "    d = df_multi_indexed_col.to_dict(\n",
    "        \"index\"\n",
    "    )  # for multi-indexed dataframe, the index in the first level of the dictionary is still a tuple!\n",
    "    return {k: ep_nest(v) for k, v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def avro_ep_encoding(episode: pd.DataFrame) -> list[Dict]:\n",
    "    \"\"\"\n",
    "    avro encoding,\n",
    "    parsing requires a schema defined in \"data_io/pool/episode_avro_schema.py\"\n",
    "\n",
    "    Convert an eos dataframe with multi-indexed columns to a nested dictionary\n",
    "    Remove all the levels of the multi-indexed columns except for 'timestamp'\n",
    "    Keep only the timestamp as the single key for the nested dictionary\n",
    "    ! Convert Timestamp to millisecond long integer!! for compliance to the  avro storage format\n",
    "    ! Timestamp with ZoneInfo will be converted to UTC and then to millisecond long integer\n",
    "    as flat as possible\n",
    "    PEP20: flat is better than nested!\n",
    "    \"\"\"\n",
    "    dict_nested = df_to_ep_nested_dict(\n",
    "        episode\n",
    "    )  # for multi-indexed dataframe, the index in the first level of the dictionary is still a tuple!\n",
    "    indices_dict = [\n",
    "        {episode.index.names[i]: level for i, level in enumerate(levels)}\n",
    "        for levels in episode.index\n",
    "    ]  # all elements in the array should have the same vehicle, driver, episodestart\n",
    "    array_of_dict = [\n",
    "        {\n",
    "            \"timestamp\": idx[\n",
    "                \"timestamp\"\n",
    "            ].timestamp()  # Timestamp with ZoneInfo will be converted to UTC\n",
    "            * 1e6,  # convert to microsecond long integer\n",
    "            **dict_nested[\n",
    "                key\n",
    "            ],  # merge the nested dict with the timestamp, as flat as possible\n",
    "        }\n",
    "        for (idx, key) in zip(indices_dict, dict_nested)\n",
    "    ]\n",
    "\n",
    "    return array_of_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93025395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def avro_ep_decoding(episodes: list[Dict], tz_info: Optional[ZoneInfo]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    avro decoding,\n",
    "\n",
    "    Convert a list of nested dictionaries to DataFrame with multi-indexed columns and index\n",
    "    ! Convert microsecond long integer to Timestamp!\n",
    "    (avro storage format stores timestamp as long integer in keys but\n",
    "    seem to have DateTime with timezone in the values.)\n",
    "\n",
    "    Apache Avro store datetime/timestamp as timezone unaware (default as UTC)\n",
    "    Therefore, we need tz info either in the metadata or elsewhere to designate the timezone\n",
    "\n",
    "    sort the column order\n",
    "    \"\"\"\n",
    "\n",
    "    batch = []\n",
    "    for ep in episodes:\n",
    "        dict_observations = [\n",
    "            {\n",
    "                (\n",
    "                    ep[\"meta\"][\"episode_meta\"][\"vehicle\"],\n",
    "                    ep[\"meta\"][\"episode_meta\"][\"driver\"],\n",
    "                    pd.to_datetime(\n",
    "                        ep[\"meta\"][\"episode_meta\"][\"episodestart\"], unit=\"us\", utc=True\n",
    "                    ).tz_convert(tz_info),\n",
    "                    pd.to_datetime(step[\"timestamp\"], unit=\"us\", utc=True).tz_convert(\n",
    "                        tz_info\n",
    "                    ),\n",
    "                    qtuple,\n",
    "                    rows,\n",
    "                    idx,\n",
    "                ): item\n",
    "                if rows != \"timestep\"\n",
    "                else pd.to_datetime(item, utc=True).tz_convert(tz_info)\n",
    "                for qtuple, obs in step.items()\n",
    "                if qtuple\n",
    "                != \"timestamp\"  # \"timestamp\" is not a real valid qtuple, although it is in this level\n",
    "                for rows, value in obs.items()  # but mixed in during avro encoding for storing\n",
    "                for idx, item in enumerate(value)\n",
    "            }\n",
    "            for step in ep[\"sequence\"]\n",
    "        ]\n",
    "\n",
    "        dict_ep = {k: v for d in dict_observations for k, v in d.items()}\n",
    "\n",
    "        ser_decoded = pd.Series(dict_ep)\n",
    "        ser_decoded.index.names = [\n",
    "            \"vehicle\",\n",
    "            \"driver\",\n",
    "            \"episodestart\",\n",
    "            \"timestamp\",\n",
    "            \"qtuple\",\n",
    "            \"rows\",\n",
    "            \"idx\",\n",
    "        ]\n",
    "        df_decoded = ser_decoded.unstack(level=[\"qtuple\", \"rows\", \"idx\"])  # type: ignore\n",
    "        df_decoded.sort_index(inplace=True, axis=1)  # sort the column order\n",
    "        batch.append(df_decoded)\n",
    "\n",
    "    index_names = batch[0].index.names\n",
    "    df_episodes = pd.concat(\n",
    "        batch, keys=range(len(batch)), names=[\"batch\"] + index_names\n",
    "    )\n",
    "\n",
    "    return df_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b574ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def decode_mongo_records(\n",
    "    df: pd.DataFrame,\n",
    "    torque_table_row_names: list[str],\n",
    ") -> tuple[\n",
    "    list[pd.DataFrame], list[pd.DataFrame], list[pd.DataFrame], list[pd.DataFrame]\n",
    "]:\n",
    "    \"\"\"\n",
    "    decoding the batch RECORD observations from mongodb nested dicts to pandas dataframe\n",
    "    (EPISODE doesn't need decoding, it is already a dataframe)\n",
    "    TODO need to check whether sort_index is necessary\n",
    "    \"\"\"\n",
    "\n",
    "    dict_observations_list = (\n",
    "        [  # list of observations as dict with tuple key suitable as MultiIndex\n",
    "            {\n",
    "                (\n",
    "                    meta[\"episodestart\"],\n",
    "                    meta[\"vehicle\"],\n",
    "                    meta[\"driver\"],\n",
    "                    meta[\"timestamp\"],\n",
    "                    qtuple,\n",
    "                    rows,\n",
    "                    idx,\n",
    "                ): value\n",
    "                for qtuple, obs1 in obs.items()\n",
    "                for rows, obs2 in obs1.items()\n",
    "                for idx, value in obs2.items()\n",
    "            }\n",
    "            for meta, obs in zip(df[\"meta\"], df[\"observation\"])\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df_actions = []\n",
    "    df_states = []\n",
    "    df_nstates = []\n",
    "    ser_rewards = []\n",
    "    idx = pd.IndexSlice\n",
    "    for dict_observations in dict_observations_list:  # decode each measurement from\n",
    "        ser_decoded = pd.Series(dict_observations)\n",
    "        ser_decoded.index.names = [\n",
    "            \"episodestart\",\n",
    "            \"vehicle\",\n",
    "            \"driver\",\n",
    "            \"timestamp\",\n",
    "            \"qtuple\",\n",
    "            \"rows\",\n",
    "            \"idx\",\n",
    "        ]\n",
    "\n",
    "        # decode state\n",
    "        ser_state = ser_decoded.loc[\n",
    "            idx[:, :, :, :, \"state\", [\"brake\", \"thrust\", \"velocity\", \"timestep\"]]\n",
    "        ]\n",
    "        df_state = ser_state.unstack(level=[0, 1, 2, 3, 4, 5])  # type: ignore\n",
    "        multiindex = df_state.columns\n",
    "        df_state.set_index(multiindex[-1], inplace=True)  # last index has timestep\n",
    "        df_states.append(df_state)\n",
    "\n",
    "        # decode action\n",
    "        ser_action = ser_decoded.loc[\n",
    "            idx[:, :, :, :, \"action\", [*torque_table_row_names, \"throttle\"]]\n",
    "        ]\n",
    "        df_action = ser_action.unstack(level=[0, 1, 2, 3, 4, 5])  # type: ignore\n",
    "        multiindex = df_action.columns\n",
    "        df_action.set_index(multiindex[-1], inplace=True)  # last index has throttle\n",
    "\n",
    "        action_timestep = ser_decoded.loc[idx[:, :, :, :, \"action\", \"timestep\"]]\n",
    "        action_speed = ser_decoded.loc[idx[:, :, :, :, \"action\", \"speed\"]]\n",
    "        action_multi_col = [\n",
    "            (*column, speed, timestep)  # swap speed and timestep\n",
    "            for column, timestep, speed in zip(df_action.columns, action_timestep, action_speed)  # type: ignore\n",
    "        ]\n",
    "        df_action.columns = pd.MultiIndex.from_tuples(\n",
    "            action_multi_col,\n",
    "            names=[\n",
    "                \"episodestart\",\n",
    "                \"vehicle\",\n",
    "                \"driver\",\n",
    "                \"timestamp\",\n",
    "                \"qtuple\",\n",
    "                \"rows\",\n",
    "                \"speed\",\n",
    "                \"timestep\",\n",
    "            ],\n",
    "        )\n",
    "        df_actions.append(df_action)\n",
    "\n",
    "        # decode reward\n",
    "        ser_reward = ser_decoded.loc[idx[:, :, :, :, \"reward\", [\"work\", \"timestep\"]]]\n",
    "        df_reward = ser_reward.unstack([0, 1, 2, 3, 4, 5])  # type: ignore\n",
    "        multiindex = df_reward.columns\n",
    "        df_reward.set_index(multiindex[-1], inplace=True)  # last index has timestep\n",
    "        # df_reward\n",
    "        ser_rewards.append(df_reward)\n",
    "\n",
    "        # decode nstate\n",
    "        ser_nstate = ser_decoded.loc[\n",
    "            idx[:, :, :, :, \"nstate\", [\"brake\", \"thrust\", \"velocity\", \"timestep\"]]\n",
    "        ]\n",
    "        df_nstate = ser_nstate.unstack([0, 1, 2, 3, 4, 5])  # type: ignore\n",
    "        multiindex = df_nstate.columns\n",
    "        df_nstate.set_index(multiindex[-1], inplace=True)\n",
    "        df_nstates.append(df_nstate)\n",
    "\n",
    "    return df_states, df_actions, ser_rewards, df_nstates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cb87eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def decode_mongo_episodes(\n",
    "    df: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    decoding the batch RECORD observations from mongodb nested dicts to pandas dataframe\n",
    "    (EPISODE doesn't need decoding, it is already a dataframe)\n",
    "    TODO need to check whether sort_index is necessary\"\"\"\n",
    "    dict_observations = [\n",
    "        {\n",
    "            (\n",
    "                meta[\"vehicle\"],\n",
    "                meta[\"driver\"],\n",
    "                meta[\"episodestart\"],\n",
    "                timestamp,\n",
    "                qtuple,\n",
    "                rows,\n",
    "                idx,\n",
    "            ): value\n",
    "            for timestamp, obs1 in obs.items()\n",
    "            for qtuple, obs2 in obs1.items()  # (state, action, reward, next_state)\n",
    "            for rows, obs3 in obs2.items()  # (velocity, thrust, brake), (r0, r1, r2, ...),\n",
    "            for idx, value in obs3.items()  # (0, 1, 2, ...)\n",
    "        }\n",
    "        for meta, obs in zip(df[\"meta\"], df[\"observation\"])\n",
    "    ]\n",
    "\n",
    "    batch = []\n",
    "    for dict_obs in dict_observations:\n",
    "        ser_decoded = pd.Series(dict_obs)\n",
    "        ser_decoded.index.names = [\n",
    "            \"vehicle\",\n",
    "            \"driver\",\n",
    "            \"episodestart\",\n",
    "            \"timestamp\",\n",
    "            \"qtuple\",\n",
    "            \"rows\",\n",
    "            \"idx\",\n",
    "        ]\n",
    "        df_decoded = ser_decoded.unstack(level=[\"qtuple\", \"rows\", \"idx\"])  # type: ignore\n",
    "        df_decoded.sort_index(inplace=True, axis=1)\n",
    "        batch.append(df_decoded)  # qtuple, rows, index\n",
    "\n",
    "    # batch.sort_index(inplace=True, axis=0)\n",
    "    # must not sort_index, otherwise the order of the columns will be changed, if there were duplicated episodes\n",
    "    index_names = batch[0].index.names\n",
    "    df_episodes = pd.concat(\n",
    "        batch, keys=range(len(batch)), names=[\"batch\"] + index_names\n",
    "    )\n",
    "    return df_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace4c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def encode_dataframe_from_parquet(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    decode the dataframe from parquet with flat column indices to MultiIndexed DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    multi_tpl = [tuple(col.split(\"_\")) for col in df.columns]\n",
    "    multi_col = pd.MultiIndex.from_tuples(multi_tpl)\n",
    "    i1 = multi_col.get_level_values(0)\n",
    "    i1 = pd.Index(\n",
    "        [\"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else idx for idx in i1]\n",
    "    )  # convert index of level 2 type to int and '' if NA\n",
    "    i2 = multi_col.get_level_values(\n",
    "        1\n",
    "    )  # must be null string instead of the default pd.NA or np.nan\n",
    "    i2 = pd.Index(\n",
    "        [\"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else idx for idx in i2]\n",
    "    )  # convert index of level 2 type to int and '' if NA\n",
    "    i3 = multi_col.get_level_values(\n",
    "        2\n",
    "    )  # must be null string instead of the default pd.NA or np.nan\n",
    "    i3 = pd.Index(\n",
    "        [\"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else int(idx) for idx in i3]\n",
    "    )  # convert index of level 2 type to int and '' if NA\n",
    "\n",
    "    multi_col = pd.MultiIndex.from_arrays([i1, i2, i3])\n",
    "    multi_col.names = [\"qtuple\", \"rows\", \"idx\"]\n",
    "    df.columns = multi_col\n",
    "\n",
    "    df = df.set_index([\"vehicle\", \"driver\", \"episodestart\", df.index])  # type: ignore\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6114f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def decode_episode_batch_to_padded_arrays(\n",
    "    episodes: pd.DataFrame,\n",
    "    torque_table_row_names: list[str],\n",
    "    padding_value: float = -10000.0,\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    decode the dataframes to 3D numpy arrays [B, T, F] for states, actions, rewards, next_states\n",
    "    episodes with variable lengths will turn into ragged arrays with the same raggedness, thus the same maximum length\n",
    "    after padding the arrays will have the same shape and padding pattern.\n",
    "\n",
    "    episodes are not sorted and its internal index keeps the index order of the original episodes, not interleaved!\n",
    "    idx_len_list: list of lengths of each episode in the batch, use explicit segmentation to avoid the bug,\n",
    "    when the batch has duplicated episodes\n",
    "    \"\"\"\n",
    "\n",
    "    # episodestart_index = episodes.index.unique(level='episode_start')\n",
    "    # episodes.sort_index(inplace=False, axis=0).sort_index(inplace=True, axis=1)\n",
    "    # array of rewards for minibatch\n",
    "    # for ep in batch:\n",
    "    #     ep.sort_index(inplace=True, axis=1)\n",
    "    idx = pd.IndexSlice\n",
    "    rewards_list = [\n",
    "        episodes.loc[idx[i, :, :, :, :], idx[\"reward\", \"work\"]].values.tolist()  # type: ignore\n",
    "        for i in episodes.index.get_level_values(0)\n",
    "    ]  # type: ignore\n",
    "    r_n_t = tf.keras.utils.pad_sequences(\n",
    "        rewards_list, padding=\"post\", dtype=np.float32, value=padding_value\n",
    "    )\n",
    "\n",
    "    # array of states for minibatch\n",
    "    states_list = [\n",
    "        episodes.loc[idx[i, :, :, :, :], idx[\"state\", [\"velocity\", \"thrust\", \"brake\"]]].values.tolist()  # type: ignore\n",
    "        for i in episodes.index.get_level_values(0)\n",
    "    ]  # type: ignore\n",
    "    s_n_t = tf.keras.utils.pad_sequences(\n",
    "        states_list, padding=\"post\", dtype=np.float32, value=padding_value\n",
    "    )\n",
    "\n",
    "    # array of actions for minibatch\n",
    "    actions_list = [\n",
    "        episodes.loc[idx[i, :, :, :, :], idx[\"action\", torque_table_row_names]].values.tolist()  # type: ignore\n",
    "        for i in episodes.index.get_level_values(0)\n",
    "    ]  # type: ignore\n",
    "    a_n_t = tf.keras.utils.pad_sequences(\n",
    "        actions_list, padding=\"post\", dtype=np.float32, value=padding_value\n",
    "    )\n",
    "\n",
    "    # array of next_states for minibatch\n",
    "    nstates_list = [\n",
    "        episodes.loc[idx[i, :, :, :, :], idx[\"nstate\", [\"velocity\", \"thrust\", \"brake\"]]].values.tolist()  # type: ignore\n",
    "        for i in episodes.index.get_level_values(0)\n",
    "    ]  # type: ignore\n",
    "    ns_n_t = tf.keras.utils.pad_sequences(\n",
    "        nstates_list, padding=\"post\", dtype=np.float32, value=padding_value\n",
    "    )\n",
    "\n",
    "    return s_n_t, a_n_t, r_n_t, ns_n_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa72a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def encode_episode_dataframe_from_series(\n",
    "    observations: List[pd.Series],\n",
    "    torque_table_row_names: List[str],\n",
    "    episode_start_dt: datetime,\n",
    "    driver_str: str,\n",
    "    truck_str: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    encode the list of observations as a dataframe with multi-indexed columns\n",
    "    \"\"\"\n",
    "    episode = pd.concat(\n",
    "        observations, axis=1\n",
    "    ).transpose()  # concat along columns and transpose to DataFrame, columns not sorted as (s,a,r,s')\n",
    "    episode.columns.names = [\"tuple\", \"rows\", \"idx\"]\n",
    "    episode.set_index((\"timestamp\", \"\", 0), append=False, inplace=True)\n",
    "    episode.index.name = \"timestamp\"\n",
    "    episode.sort_index(axis=1, inplace=True)\n",
    "\n",
    "    # convert columns types to float where necessary\n",
    "    state_cols_float = [(\"state\", col) for col in [\"brake\", \"thrust\", \"velocity\"]]\n",
    "    action_cols_float = [\n",
    "        (\"action\", col) for col in [*torque_table_row_names, \"speed\", \"throttle\"]\n",
    "    ]\n",
    "    reward_cols_float = [(\"reward\", \"work\")]\n",
    "    nstate_cols_float = [(\"nstate\", col) for col in [\"brake\", \"thrust\", \"velocity\"]]\n",
    "    for col in (\n",
    "        action_cols_float + state_cols_float + reward_cols_float + nstate_cols_float\n",
    "    ):\n",
    "        episode[col[0], col[1]] = episode[col[0], col[1]].astype(\n",
    "            \"float\"\n",
    "        )  # float16 not allowed in parquet\n",
    "\n",
    "    # Create MultiIndex ('vehicle', 'driver', 'episodestart', 'timestamp')\n",
    "    ## Append index for the episode, in the order 'vehicle', 'driver', 'episodestart'\n",
    "    episode = pd.concat(\n",
    "        [episode],\n",
    "        keys=[episode_start_dt],\n",
    "        names=[\"episodestart\"],\n",
    "    )\n",
    "\n",
    "    episode = pd.concat([episode], keys=[driver_str], names=[\"driver\"])\n",
    "    episode = pd.concat([episode], keys=[truck_str], names=[\"vehicle\"])\n",
    "    episode.sort_index(inplace=True)  # sorting in the time order of timestamps\n",
    "\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8360d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def recover_episodestart_tzinfo_from_timestamp(\n",
    "    ts: pd.Timestamp, tzinfo: ZoneInfo\n",
    ") -> pd.Timestamp:\n",
    "    \"\"\"\n",
    "    recover the timezone information from the parquet folder name string\n",
    "    \"\"\"\n",
    "\n",
    "    ts = ts.tz_localize(\"UTC\").tz_convert(tzinfo)\n",
    "\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb43a1e1c00ebc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
