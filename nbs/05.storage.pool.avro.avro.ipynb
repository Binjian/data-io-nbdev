{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cc719a2d692e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771bc5629678d6fe",
   "metadata": {},
   "source": [
    "# Avro\n",
    "\n",
    "> AvroPool class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ac190d2f7da9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp storage.pool.avro.avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd34c46374345bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import dask.bag as db  # type: ignore\n",
    "import fastavro\n",
    "import pandas as pd  # type: ignore\n",
    "from dask.bag import Bag, random\n",
    "from dask.diagnostics import ProgressBar  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e9a4d931c7a83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# The following three imports is a nbdev workaround, since ParquetPool is a derived class of DaskPool, nbdev seems to not import it\n",
    "from configparser import ConfigParser\n",
    "from pathlib import Path\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121121430eb6c052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 14:19:49.605351: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-19 14:19:49.639601: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-19 14:19:49.639639: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-19 14:19:49.639669: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-19 14:19:49.646200: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-19 14:19:49.646618: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from data_io_nbdev.data.core import (\n",
    "    ObservationMeta,\n",
    "    PoolQuery,\n",
    "    veos_lifetime_end_date,\n",
    "    veos_lifetime_start_date,\n",
    ")\n",
    "from data_io_nbdev.data.location import locations_by_abbr\n",
    "from data_io_nbdev.data.external.pandas_utils import avro_ep_decoding, avro_ep_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ccd45ae4d5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from data_io_nbdev.storage.pool.dask import DaskPool  # type: ignore\n",
    "from data_io_nbdev.storage.pool.avro.schema import gen_episode_schema  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec71c90b69c43934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass(kw_only=True)\n",
    "class AvroPool(DaskPool):  # type: ignore   # pycharm bug\n",
    "    \"\"\"\n",
    "    AvroPool is the avro storage for pooling the real-time data from the cloud.\n",
    "\n",
    "    Features:\n",
    "        - It's supposed to support large local data pool with buffer capacity\n",
    "    only bounded by local system storage.\n",
    "\n",
    "        - It uses Dask Bag to store the data in memory and Dask DataFrame to process the data.\n",
    "\n",
    "        - Meta information is stored in avro metadata in each of avro file. Sampling\n",
    "\n",
    "        - Random episodes needs some care to reassure the randomness. It uses Dask Delayed to parallelize the data processing like sampling\n",
    "\n",
    "    Attributes:\n",
    "        - dbg: Dask Bag of episodes\n",
    "        - dbg_schema: schema for avro file decoding\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dbg: Optional[db.Bag] = None  # db.from_sequence([])  # dask DataFrame\n",
    "    dbg_schema: Optional[\n",
    "        dict\n",
    "    ] = None  # field(default_factory=dict)  # schema for avro file decoding\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Set up logger, post init of DaskPool and load the pool.\"\"\"\n",
    "        self.logger = self.logger.getChild(\"avro pool\")\n",
    "        self.dict_logger = self.dict_logger\n",
    "        super().__post_init__()\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"{{'header': 'Arrow pool stored', \"\n",
    "            f\"'path': '{self.pl_path}', \"\n",
    "            f\"'coll_type' : '{self.recipe['DEFAULT']['coll_type']}'}}\",\n",
    "            extra=self.dict_logger,\n",
    "        )\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"load EPISODE arrays from avro files in folder specified by the recipe\"\"\"\n",
    "\n",
    "        schema = gen_episode_schema(\n",
    "            self.meta\n",
    "        )  # schema for avro file decoding into episode bag\n",
    "\n",
    "        self.dbg_schema = fastavro.parse_schema(schema)\n",
    "        self.logger.info(\n",
    "            f\"{{'header': 'Arrow pool loaded', \",\n",
    "            f\"'path': '{self.pl_path}'}}\",\n",
    "            extra=self.dict_logger,\n",
    "        )\n",
    "        try:\n",
    "            with ProgressBar():\n",
    "                self.dbg = db.read_avro(\n",
    "                    str(self.pl_path / \"bag_episodes.*.avro\"),  # Path to str conversion\n",
    "                )\n",
    "                # # parquet file which is partitioned by a timestamp was converted to category,\n",
    "                # # when loaded to dask dataframe\n",
    "                # self.dbg[\"episodestart__\"] = self.dbg[\"episodestart__\"].astype(\n",
    "                #     \"datetime64[ns]\"\n",
    "                # )  # very important for indexing and slicing!!!\n",
    "            self.logger.info(\n",
    "                f\"{{'header': 'Loading bag from avro files.',  \"\n",
    "                f\"'path': '{self.pl_path}'}}\",\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "        except (FileNotFoundError, ValueError) as e:\n",
    "            self.logger.info(\n",
    "                f'Data folder ({self.recipe[\"DEFAULT\"][\"data_folder\"]}) is empty! parquet files not found: {e} ...'\n",
    "            )\n",
    "            self.logger.info(\n",
    "                f'Create data folder ({self.recipe[\"DEFAULT\"][\"data_folder\"]}) for Apache Arrow parquet files!'\n",
    "            )\n",
    "            self.pl_path.mkdir(parents=True, exist_ok=True)\n",
    "            self.cnt = 0\n",
    "            return\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Loading avro error: {e}\", extra=self.dict_logger)\n",
    "            raise e\n",
    "\n",
    "        # Deduplicate the bag, avro file take duplicated records.\n",
    "        # Otherwise, a separate task to clean up duplications periodically is required.\n",
    "        self.dbg = self.dbg.distinct(\n",
    "            lambda x: (\n",
    "                x[\"meta\"][\"episode_meta\"][\"driver\"],\n",
    "                x[\"meta\"][\"episode_meta\"][\"vehicle\"],\n",
    "                x[\"meta\"][\"episode_meta\"][\"episodestart\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # extract metadata stored in an avro record and compare with input metadata\n",
    "        meta_in_an_episode = self.dbg.take(1)[0][\n",
    "            \"meta\"\n",
    "        ]  # take the first episode, take return a tuple of dicts\n",
    "        episode_meta = meta_in_an_episode[\"episode_meta\"]\n",
    "        observation_meta = meta_in_an_episode[\"observation_meta\"]\n",
    "        observation_meta[\"site\"] = locations_by_abbr[observation_meta[\"site\"][\"abbr\"]]\n",
    "        pool_meta = ObservationMeta(**observation_meta)\n",
    "        self.logger.info(\n",
    "            f\"meta in avro file: {observation_meta}\", extra=self.dict_logger\n",
    "        )\n",
    "        assert self.meta.have_same_meta(\n",
    "            pool_meta\n",
    "        ), f\"meta information in avro file doesn't match with input meta information!\"\n",
    "        # TODO if different, raise warning and update meta information in parquet file\n",
    "\n",
    "        self.cnt = self._count(self.query)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"close the pool\"\"\"\n",
    "        self.logger.info(\n",
    "            f\"Nothing to be done for dask avro pool!\",  # neither arrow parquet nor avro need cleaning up.\n",
    "            extra=self.dict_logger,\n",
    "        )\n",
    "\n",
    "    def store(self, episode: pd.DataFrame) -> None:\n",
    "        \"\"\"Deposit an episode as a single item into avro.\"\"\"\n",
    "\n",
    "        episode_dict_nested = avro_ep_encoding(episode)\n",
    "        indices_dict = [\n",
    "            {episode.index.names[i]: level for i, level in enumerate(levels)}\n",
    "            for levels in episode.index\n",
    "        ]\n",
    "        episode_meta = indices_dict[0]  # only one row in the dataframe\n",
    "        try:\n",
    "            episode_meta.pop(\"timestamp\")\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"timestamp is not in index of episode dataframe!\")\n",
    "\n",
    "        episode_meta[\"episodestart\"] = (\n",
    "            episode_meta[\"episodestart\"].timestamp() * 1e6\n",
    "        )  # convert to microsecond\n",
    "        # convert the dict to a dask bag file\n",
    "        # observation_meta = ObservationMeta(\n",
    "        #     state_specs=self.meta.state_specs,\n",
    "        #     action_specs=self.meta.action_specs,\n",
    "        #     reward_specs=self.meta.reward_specs,\n",
    "        #     site=self.meta.site,\n",
    "        # )\n",
    "        records_episode_to_add = {\n",
    "            \"episodestart\": episode_meta[\"episodestart\"],\n",
    "            \"meta\": {\n",
    "                \"episode_meta\": episode_meta,\n",
    "                \"observation_meta\": self.meta.model_dump(),\n",
    "            },\n",
    "            \"sequence\": episode_dict_nested,\n",
    "        }\n",
    "\n",
    "        dask_bag_ep = db.from_sequence([records_episode_to_add], npartitions=1)\n",
    "        if self.dbg is not None:\n",
    "            self.dbg = db.concat([self.dbg, dask_bag_ep])\n",
    "        else:\n",
    "            self.dbg = dask_bag_ep\n",
    "\n",
    "        try:\n",
    "            with ProgressBar():\n",
    "                self.dbg.to_avro(\n",
    "                    self.pl_path / \"bag_episodes.*.avro\",\n",
    "                    schema=self.dbg_schema,\n",
    "                )\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Writing avro error: {e}\", extra=self.dict_logger)\n",
    "        else:\n",
    "            self.cnt = self.cnt + 1\n",
    "            self.logger.info(f\"deposit one episode in avro\", extra=self.dict_logger)\n",
    "        finally:\n",
    "            self.logger.info(f\"Done avro pool.\")\n",
    "\n",
    "    def get_query(self, query: Optional[PoolQuery] = None) -> Optional[Bag]:\n",
    "        \"\"\"\n",
    "        get query from dask dataframe\n",
    "\n",
    "        Arg:\n",
    "            query: `PoolQuery` object\n",
    "\n",
    "        return:\n",
    "            a Dask Bag with all episodes in the query range\n",
    "        \"\"\"\n",
    "        assert query is not None, f\"query is None!\"\n",
    "\n",
    "        if query.episodestart_start is None:\n",
    "            query.episodestart_start = veos_lifetime_start_date.to_pydatetime()\n",
    "\n",
    "        if query.episodestart_end is None:\n",
    "            query.episodestart_end = veos_lifetime_end_date.to_pydatetime()\n",
    "\n",
    "        if query.seq_len_from is None:\n",
    "            query.seq_len_from = 0\n",
    "\n",
    "        if not query.seq_len_to is None:\n",
    "            query.seq_len_to = int(\n",
    "                1e09\n",
    "            )  # 1 bio steps is enough as upper bound >74k Years\n",
    "        if self.dbg is None:\n",
    "            return None\n",
    "        queried = self.dbg.filter(\n",
    "            lambda x: x[\"meta\"][\"episode_meta\"][\"vehicle\"] == query.vehicle\n",
    "            and x[\"meta\"][\"episode_meta\"][\"driver\"] == query.driver\n",
    "            and (\n",
    "                pd.Timestamp(query.episodestart_start)\n",
    "                .tz_convert(tz=\"UTC\")\n",
    "                .tz_localize(None)\n",
    "                < pd.to_datetime(\n",
    "                    x[\"meta\"][\"episode_meta\"][\"episodestart\"], unit=\"us\"\n",
    "                )  # timestamp in avro is UTC in microsecond\n",
    "                < pd.Timestamp(query.episodestart_end)\n",
    "                .tz_convert(tz=\"UTC\")\n",
    "                .tz_localize(None)\n",
    "            )  # do timestamps from avro get need conversion? x[\"meta\"][\"episode_meta\"][\"episodestart\"]? x:\n",
    "            and (query.seq_len_from < len(x[\"sequence\"]) < query.seq_len_to)\n",
    "        )\n",
    "        assert isinstance(queried, Bag), f\"queried is not a bag!\"\n",
    "        return queried\n",
    "\n",
    "    def find(self, query: PoolQuery) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Find records by the `PoolQuery` object.\n",
    "        The down-stream task can use pandas dataframe unique() for index to get unique episodes.\n",
    "\n",
    "        Arg:\n",
    "            query: `PoolQuery` object\n",
    "\n",
    "        Return:\n",
    "            A multi-indexed DataFrame with all episodes in the query range.\n",
    "        \"\"\"\n",
    "\n",
    "        queried_dict = self.get_query(query).compute()\n",
    "        df_episodes = avro_ep_decoding(queried_dict, tz_info=query.episodestart_start.tzinfo)  # type: ignore\n",
    "\n",
    "        return df_episodes\n",
    "\n",
    "    def delete(self, idx) -> None:\n",
    "        \"\"\"\n",
    "        Delete a record by item id.\n",
    "\n",
    "        Not yet implemented for arrow pool!\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def remove_episode(self, query: PoolQuery) -> None:\n",
    "        \"\"\"\n",
    "        remove episodes in the query from bag, but not from avro file!\n",
    "\n",
    "        Delete all episodes in the query range. Modify the bag in place.\n",
    "\n",
    "        Arg:\n",
    "            query: `PoolQuery` object\n",
    "\n",
    "        Return:\n",
    "                None\n",
    "        \"\"\"\n",
    "\n",
    "        self.dbg = self.dbg.remove(\n",
    "            lambda x: x[\"meta\"][\"episode_meta\"][\"vehicle\"] == query.vehicle\n",
    "            and x[\"meta\"][\"episode_meta\"][\"driver\"] == query.driver\n",
    "            and (\n",
    "                pd.Timestamp(query.episodestart_start)\n",
    "                .tz_convert(tz=\"UTC\")\n",
    "                .tz_localize(None)\n",
    "                < pd.to_datetime(x[\"meta\"][\"episode_meta\"][\"episodestart\"], unit=\"us\")\n",
    "                < pd.Timestamp(query.episodestart_end)\n",
    "                .tz_convert(tz=\"UTC\")\n",
    "                .tz_localize(None)\n",
    "            )\n",
    "        )  # do timestamps from avro need conversion? x[\"meta\"][\"episode_meta\"][\"episodestart\"]\n",
    "        old_cnt = self.cnt\n",
    "        self.cnt = self._count(self.query)\n",
    "        self.logger.info(\n",
    "            f\"Avro pool decreases in {old_cnt-self.cnt} episosdes.\",\n",
    "            extra=self.dict_logger,\n",
    "        )\n",
    "\n",
    "    def sample(\n",
    "        self,\n",
    "        size: int = 4,  # desired size of the samples\n",
    "        *,\n",
    "        query: Optional[PoolQuery] = None,  # query for sampling\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Sample a batch of episodes from Apache avro pool.\n",
    "\n",
    "        downstream can use pandas DataFrame unique() for index to extract single episodes.\n",
    "        since return is a dataframe, downstream can use pandas dataframe unique() for index to get unique episodes.\n",
    "        Therefore, decoding to DataFrame have to be done in avro pool\n",
    "\n",
    "        Args:\n",
    "            size: number of episodes to sample\n",
    "            query: `PoolQuery` object\n",
    "\n",
    "        Return:\n",
    "            A DataFrame with all episodes\n",
    "        \"\"\"\n",
    "\n",
    "        if query == self.query:\n",
    "            cnt = self.cnt\n",
    "        else:\n",
    "            cnt = self._count(query)\n",
    "\n",
    "        queried = self.get_query(query)\n",
    "\n",
    "        if cnt >= size:\n",
    "            sampled = db.random.sample(\n",
    "                population=queried, k=size, split_every=8\n",
    "            )  # todo: split_every=8 is the default number, to be investigated\n",
    "        else:\n",
    "            sampled = db.random.choices(population=queried, k=size, split_every=8)\n",
    "\n",
    "        # query.timestamp_start.tzinfo is Optional[tzinfo], avro_ep_decoding() requires tzinfo as optional\n",
    "        df_episodes = avro_ep_decoding(\n",
    "            sampled.compute(),  # bag to nested lists\n",
    "            tz_info=query.episodestart_start.tzinfo,  # type: ignore\n",
    "        )\n",
    "\n",
    "        return df_episodes\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (record for record in self.dbg.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ef262bc179fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa90fb33b4ea9886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/avro/avro.py#L57){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### AvroPool.__post_init__\n\n>      AvroPool.__post_init__ ()\n\nSet up logger, post init of DaskPool and load the pool.",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/avro/avro.py#L57){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### AvroPool.__post_init__\n\n>      AvroPool.__post_init__ ()\n\nSet up logger, post init of DaskPool and load the pool."
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AvroPool.__post_init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60197ae5886dd6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/avro/avro.py#L247){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### AvroPool.find\n\n>      AvroPool.find (query:data_io_nbdev.data.core.PoolQuery)\n\nFind records by the `PoolQuery` object.\nThe down-stream task can use pandas dataframe unique() for index to get unique episodes.\n\nArg:\n    query: `PoolQuery` object \n\nReturn: \n    A multi-indexed DataFrame with all episodes in the query range.",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/avro/avro.py#L247){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### AvroPool.find\n\n>      AvroPool.find (query:data_io_nbdev.data.core.PoolQuery)\n\nFind records by the `PoolQuery` object.\nThe down-stream task can use pandas dataframe unique() for index to get unique episodes.\n\nArg:\n    query: `PoolQuery` object \n\nReturn: \n    A multi-indexed DataFrame with all episodes in the query range."
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AvroPool.find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac64e322c71262f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/avro/avro.py#L71){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### AvroPool.load\n\n>      AvroPool.load ()\n\nload EPISODE arrays from avro files in folder specified by the recipe",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/avro/avro.py#L71){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### AvroPool.load\n\n>      AvroPool.load ()\n\nload EPISODE arrays from avro files in folder specified by the recipe"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AvroPool.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6983db99a6b6b8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/avro/avro.py#L141){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### AvroPool.close\n\n>      AvroPool.close ()\n\nclose the pool",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/avro/avro.py#L141){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### AvroPool.close\n\n>      AvroPool.close ()\n\nclose the pool"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AvroPool.close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2722632747af952c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/avro/avro.py#L306){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### AvroPool.sample\n\n>      AvroPool.sample (size:int=4,\n>                       query:Optional[data_io_nbdev.data.core.PoolQuery]=None)\n\nSample a batch of episodes from Apache avro pool.\n\ndownstream can use pandas DataFrame unique() for index to extract single episodes.\nsince return is a dataframe, downstream can use pandas dataframe unique() for index to get unique episodes.\nTherefore, decoding to DataFrame have to be done in avro pool\n\nArgs:\n    size: number of episodes to sample\n    query: `PoolQuery` object\n\nReturn: \n    A DataFrame with all episodes\n\n|    | **Type** | **Default** | **Details** |\n| -- | -------- | ----------- | ----------- |\n| size | int | 4 | desired size of the samples |\n| query | Optional[PoolQuery] | None |  |\n| **Returns** | **pd.DataFrame** |  | **query for sampling** |",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/avro/avro.py#L306){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### AvroPool.sample\n\n>      AvroPool.sample (size:int=4,\n>                       query:Optional[data_io_nbdev.data.core.PoolQuery]=None)\n\nSample a batch of episodes from Apache avro pool.\n\ndownstream can use pandas DataFrame unique() for index to extract single episodes.\nsince return is a dataframe, downstream can use pandas dataframe unique() for index to get unique episodes.\nTherefore, decoding to DataFrame have to be done in avro pool\n\nArgs:\n    size: number of episodes to sample\n    query: `PoolQuery` object\n\nReturn: \n    A DataFrame with all episodes\n\n|    | **Type** | **Default** | **Details** |\n| -- | -------- | ----------- | ----------- |\n| size | int | 4 | desired size of the samples |\n| query | Optional[PoolQuery] | None |  |\n| **Returns** | **pd.DataFrame** |  | **query for sampling** |"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AvroPool.sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0b559ec1985a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/avro/avro.py#L148){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### AvroPool.store\n\n>      AvroPool.store (episode:pandas.core.frame.DataFrame)\n\nDeposit an episode as a single item into avro.",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/avro/avro.py#L148){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### AvroPool.store\n\n>      AvroPool.store (episode:pandas.core.frame.DataFrame)\n\nDeposit an episode as a single item into avro."
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AvroPool.store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34b904b0b5a5dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/avro/avro.py#L201){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### AvroPool.get_query\n\n>      AvroPool.get_query\n>                          (query:Optional[data_io_nbdev.data.core.PoolQuery]=No\n>                          ne)\n\nget query from dask dataframe\n\nArg:\n    query: `PoolQuery` object\n\nreturn: \n    a Dask Bag with all episodes in the query range",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/avro/avro.py#L201){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### AvroPool.get_query\n\n>      AvroPool.get_query\n>                          (query:Optional[data_io_nbdev.data.core.PoolQuery]=No\n>                          ne)\n\nget query from dask dataframe\n\nArg:\n    query: `PoolQuery` object\n\nreturn: \n    a Dask Bag with all episodes in the query range"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AvroPool.get_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3922069b162111f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/avro/avro.py#L273){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### AvroPool.remove_episode\n\n>      AvroPool.remove_episode (query:data_io_nbdev.data.core.PoolQuery)\n\nremove episodes in the query from bag, but not from avro file!\n\nDelete all episodes in the query range. Modify the bag in place.\n\nArg:\n    query: `PoolQuery` object\n\nReturn: \n        None",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/pool/avro/avro.py#L273){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### AvroPool.remove_episode\n\n>      AvroPool.remove_episode (query:data_io_nbdev.data.core.PoolQuery)\n\nremove episodes in the query from bag, but not from avro file!\n\nDelete all episodes in the query range. Modify the bag in place.\n\nArg:\n    query: `PoolQuery` object\n\nReturn: \n        None"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(AvroPool.remove_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a6fcba004995e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
