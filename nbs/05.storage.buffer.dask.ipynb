{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d1a17a3ddd6c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b786dace50f80",
   "metadata": {},
   "source": [
    "# Dask\n",
    "\n",
    "> Dask Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb3b6f81566794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp storage.buffer.dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb2ce6836afe844",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from configparser import ConfigParser\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d4583917515971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 14:20:19.198547: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-19 14:20:19.251343: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-19 14:20:19.251380: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-19 14:20:19.251427: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-19 14:20:19.269807: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-19 14:20:19.290091: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from data_io_nbdev.config.drivers import Driver\n",
    "from data_io_nbdev.config.vehicles import Truck\n",
    "from data_io_nbdev.storage.pool.avro.avro import AvroPool\n",
    "from data_io_nbdev.storage.pool.parquet import ParquetPool\n",
    "from data_io_nbdev.data.core import (\n",
    "    ObservationMeta,\n",
    "    PoolQuery,\n",
    "    veos_lifetime_end_date,\n",
    "    veos_lifetime_start_date,\n",
    ")\n",
    "from data_io_nbdev.data.external.pandas_utils import (\n",
    "    decode_episode_batch_to_padded_arrays,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c033784556d343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from data_io_nbdev.storage.buffer.buffer import Buffer  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb90b24f47f3bdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass(kw_only=True)\n",
    "class DaskBuffer(Buffer[pd.DataFrame]):\n",
    "    \"\"\"\n",
    "    A Buffer connected with a data array file pool\n",
    "    \n",
    "    Args:\n",
    "        recipe:  ConfigParser containing a folder for the data files and the ObservationMeta\n",
    "        batch_size: the batch size for sampling\n",
    "        driver: the driver\n",
    "        truck: the subject of the experiment\n",
    "        meta: the metadata of the overservation `ObservationMeta`\n",
    "        torque_table_row_names: the names of the torque table rows, e.g. ['r0, r1, r2, r3, r4, r5, r6, r7, r8, r9']\n",
    "        pool: the pool to sample from, default is `ParquetPool`\n",
    "        query: the query to sample from the pool, default is `PoolQuery`\n",
    "        logger: the logger\n",
    "        dict_logger: the dictionary logger\n",
    "    \"\"\"\n",
    "\n",
    "    recipe: ConfigParser  # field(default_factory=get_filemeta_config)\n",
    "    batch_size: int  # 0\n",
    "    driver: Driver  # field(default_factory=Driver)\n",
    "    truck: Truck  # field(default_factory=Truck)\n",
    "    meta: ObservationMeta  # field(default_factory=ObservationMeta)\n",
    "    torque_table_row_names: list[str]  # field(default_factory=list)\n",
    "    pool: Optional[\n",
    "        Union[ParquetPool, AvroPool]\n",
    "    ] = None  # field(default_factory=ParquetPool)  # cannot initialize an ABC of DaskPool\n",
    "    query: Optional[PoolQuery] = None  # field(default_factory=PoolQuery)\n",
    "    logger: Optional[logging.Logger] = None\n",
    "    dict_logger: Optional[dict] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"set logger and load the pool to the buffer \"\"\"\n",
    "        self.logger = self.logger.getChild(\"main\").getChild(\"arrow buffer\")\n",
    "        self.logger.propagate = True\n",
    "        if not self.torque_table_row_names:\n",
    "            self.torque_table_row_names = self.meta.get_torque_table_row_names()\n",
    "        super().__post_init__()\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"load the pool to the buffer\"\"\"\n",
    "        # check if datafolder exists and have valid ini file for recipe\n",
    "        # if not, create a new one\n",
    "        # if yes, load the recipe and compare with the realtime truck signal specs\n",
    "        # if not matching, raise error\n",
    "        # if matching, continue\n",
    "\n",
    "        # DaskPool differs from MongoPool in that it stores RECORD in parquet files and EPISODE in avro files,\n",
    "        # and it uses different query classes for each\n",
    "        if self.recipe[\"DEFAULT\"][\"coll_type\"] == \"RECORD\":\n",
    "            self.query = PoolQuery(\n",
    "                vehicle=self.truck.vid,\n",
    "                driver=self.driver.pid,\n",
    "                episodestart_start=veos_lifetime_start_date.to_pydatetime(),\n",
    "                episodestart_end=veos_lifetime_end_date.to_pydatetime(),\n",
    "                timestamp_start=veos_lifetime_start_date.to_pydatetime(),\n",
    "                timestamp_end=veos_lifetime_end_date.to_pydatetime(),\n",
    "            )\n",
    "            self.pool = ParquetPool(\n",
    "                recipe=self.recipe,\n",
    "                query=self.query,\n",
    "                meta=self.meta,\n",
    "                logger=self.logger,\n",
    "                dict_logger=self.dict_logger,\n",
    "            )\n",
    "        else:  # coll_type == \"EPISODE\"\n",
    "            self.query = PoolQuery(\n",
    "                vehicle=self.truck.vid,\n",
    "                driver=self.driver.pid,\n",
    "                episodestart_start=veos_lifetime_start_date.to_pydatetime(),\n",
    "                episodestart_end=veos_lifetime_end_date.to_pydatetime(),\n",
    "                seq_len_from=0,\n",
    "                seq_len_to=int(1e9),\n",
    "            )\n",
    "            self.pool = AvroPool(\n",
    "                recipe=self.recipe,\n",
    "                query=self.query,\n",
    "                meta=self.meta,\n",
    "                logger=self.logger,\n",
    "                dict_logger=self.dict_logger,\n",
    "            )\n",
    "\n",
    "        if self.pool.cnt != 0:\n",
    "            batch_1 = self.pool.sample(size=1, query=self.query)\n",
    "\n",
    "        number_states, number_actions = self.meta.get_number_of_states_actions()\n",
    "        self.logger.info(\n",
    "            f'Connected to DaskPool {self.recipe[\"DEFAULT\"][\"data_folder\"]}, '\n",
    "            f\"record number {self.pool.cnt}, \"\n",
    "            f\"num_states: {number_states}, num_actions: {number_actions}\",\n",
    "            extra=self.dict_logger,\n",
    "        )\n",
    "\n",
    "    def decode_batch_records(\n",
    "        self, batch: pd.DataFrame\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Decode the batch records from dask DataFrame to numpy arrays\n",
    "        \n",
    "        sample from parquet pool through dask give dask DataFrame, no heavy decoding required just slicing and converting to numpy array\n",
    "        \n",
    "        Arg:\n",
    "        \n",
    "            batch: the batch of records from dask DataFrame\n",
    "            \n",
    "        Return:\n",
    "            \n",
    "                states: the states of the batch\n",
    "                actions: the actions of the batch\n",
    "                rewards: the rewards of the batch\n",
    "                nstates: the next states of the batch\n",
    "        \"\"\"\n",
    "\n",
    "        idx = pd.IndexSlice\n",
    "        npa_states = batch.loc[\n",
    "            :, idx[\"state\", [\"velocity\", \"thrust\", \"brake\"]]  # type: ignore\n",
    "        ].values.astype(\n",
    "            np.float32\n",
    "        )  # same order as inference!! transpose values not necessary!\n",
    "        # as each row of batch already corresponds to a tuple with the timestamp\n",
    "\n",
    "        npa_actions = batch.loc[\n",
    "            :, idx[\"action\", self.torque_table_row_names]  # type: ignore\n",
    "        ].values.astype(np.float32)\n",
    "\n",
    "        npa_rewards = batch.loc[:, idx[\"reward\", \"work\"]].values.astype(np.float32)  # type: ignore\n",
    "\n",
    "        npa_nstates = batch.loc[\n",
    "            :, idx[\"nstate\", [\"velocity\", \"thrust\", \"brake\"]]  # type: ignore\n",
    "        ].values.astype(\n",
    "            np.float32\n",
    "        )  # same order as inference!! transpose values not necessary!\n",
    "        # as each row of batch already corresponds to a tuple with the timestamp\n",
    "\n",
    "        return npa_states, npa_actions, npa_rewards, npa_nstates\n",
    "\n",
    "    def sample(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Sampling from the MongoDB pool\n",
    "        \"\"\"\n",
    "\n",
    "        batch = self.pool.sample(size=self.batch_size, query=self.query)\n",
    "\n",
    "        if self.recipe[\"DEFAULT\"][\"coll_type\"] == \"RECORD\":\n",
    "            states, actions, rewards, nstates = self.decode_batch_records(batch)\n",
    "        else:  # coll_type == \"EPISODE\"\n",
    "            (\n",
    "                states,\n",
    "                actions,\n",
    "                rewards,\n",
    "                nstates,\n",
    "            ) = decode_episode_batch_to_padded_arrays(\n",
    "                batch, self.torque_table_row_names\n",
    "            )\n",
    "\n",
    "        return states, actions, rewards, nstates\n",
    "\n",
    "    def close(self):\n",
    "        self.pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55728054ac25750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e1528a23ddffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/buffer/dask.py#L65){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### DaskBuffer.__post_init__\n\n>      DaskBuffer.__post_init__ ()\n\nset logger and load the pool to the buffer",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/buffer/dask.py#L65){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### DaskBuffer.__post_init__\n\n>      DaskBuffer.__post_init__ ()\n\nset logger and load the pool to the buffer"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DaskBuffer.__post_init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a9274426c2813e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/buffer/dask.py#L74){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### DaskBuffer.load\n\n>      DaskBuffer.load ()\n\nload the pool to the buffer",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/buffer/dask.py#L74){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### DaskBuffer.load\n\n>      DaskBuffer.load ()\n\nload the pool to the buffer"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DaskBuffer.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3720011b960574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/buffer/dask.py#L171){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### DaskBuffer.sample\n\n>      DaskBuffer.sample ()\n\nSampling from the MongoDB pool",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/buffer/dask.py#L171){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### DaskBuffer.sample\n\n>      DaskBuffer.sample ()\n\nSampling from the MongoDB pool"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DaskBuffer.sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc810f730d52ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/buffer/dask.py#L192){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### DaskBuffer.close\n\n>      DaskBuffer.close ()\n\nclose the pool, for destructor",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/buffer/dask.py#L192){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### DaskBuffer.close\n\n>      DaskBuffer.close ()\n\nclose the pool, for destructor"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DaskBuffer.close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0fabe54296cffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/buffer/dask.py#L128){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### DaskBuffer.decode_batch_records\n\n>      DaskBuffer.decode_batch_records (batch:pandas.core.frame.DataFrame)\n\nDecode the batch records from dask DataFrame to numpy arrays\n\nsample from parquet pool through dask give dask DataFrame, no heavy decoding required just slicing and converting to numpy array\n\nArg:\n\n    batch: the batch of records from dask DataFrame\n\nReturn:\n\n        states: the states of the batch\n        actions: the actions of the batch\n        rewards: the rewards of the batch\n        nstates: the next states of the batch",
      "text/plain": "---\n\n[source](https://github.com/Binjian/data-io-nbdev/tree/main/blob/main/data_io_nbdev/storage/buffer/dask.py#L128){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n\n### DaskBuffer.decode_batch_records\n\n>      DaskBuffer.decode_batch_records (batch:pandas.core.frame.DataFrame)\n\nDecode the batch records from dask DataFrame to numpy arrays\n\nsample from parquet pool through dask give dask DataFrame, no heavy decoding required just slicing and converting to numpy array\n\nArg:\n\n    batch: the batch of records from dask DataFrame\n\nReturn:\n\n        states: the states of the batch\n        actions: the actions of the batch\n        rewards: the rewards of the batch\n        nstates: the next states of the batch"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(DaskBuffer.decode_batch_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457720cad847abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
