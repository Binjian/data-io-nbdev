{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b0d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933c3bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from configparser import ConfigParser\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcde250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db  # type: ignore\n",
    "import dask.dataframe as dd  # type: ignore\n",
    "import pandas as pd  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c5d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eos.data_io.eos_struct import ObservationMeta, PoolQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8868acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from .pool import Pool  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only=True)\n",
    "class DaskPool(Pool[pd.DataFrame]):\n",
    "    \"\"\"\n",
    "    This is the numpy array storage for pooling the real-time data from the cloud.\n",
    "    RecordDataframePool is supposed to support large local data pool with buffer capacity\n",
    "    only bounded by local system storage.\n",
    "    In one single folder.\n",
    "    It uses pyarrow parquet for data storage and dask dataframe for data processing.\n",
    "    meta information is stored in parquet metadata (in footer of parquet file).\n",
    "\n",
    "    TODO alternative will be save experience in episodes\n",
    "    TODO sample random quadruples will need some care to reassure the randomness\n",
    "\n",
    "    TODO using dask dataframe for data processing\n",
    "    TODO using dask delayed to parallelize the data processing like sampling, while appending data\n",
    "    for each\n",
    "    NPAStore provides the following features:\n",
    "    - location: can be provided to change the default location in recipe\n",
    "    - recipe: the config file for the pool\n",
    "    \"\"\"\n",
    "\n",
    "    recipe: ConfigParser  # field(default_factory=get_filemeta_config)\n",
    "    query: PoolQuery  # field(default_factory=PoolQuery)  # search record based on query in arrays is very inefficient\n",
    "    meta: ObservationMeta  # field(default_factory=ObservationMeta)  # meta information for the data collection\n",
    "    pl_path: Optional[\n",
    "        Path\n",
    "    ] = None  # Path('.')  # path to parquet file for RECORD, to avro file for EPISODE\n",
    "    # in record file pool, query is mostly ignored for sample, only for checking.\n",
    "    # list of dask DataFrame with the target vehicle and driver\n",
    "    logger: Optional[logging.Logger] = None\n",
    "    dict_logger: Optional[dict] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "\n",
    "        # all required specification for the data collection must be available\n",
    "        assert all(\n",
    "            key in self.recipe[\"DEFAULT\"]\n",
    "            for key in [\n",
    "                \"data_folder\",\n",
    "                \"recipe_file_name\",\n",
    "                \"coll_type\",\n",
    "            ]\n",
    "        ), f\"recipe specification incomplete!\"\n",
    "        self.pl_path = (\n",
    "            Path(self.recipe[\"DEFAULT\"][\"data_folder\"])\n",
    "            / self.recipe[\"DEFAULT\"][\"coll_type\"]\n",
    "        )  # coll_type used as part of the path of the parquet storage location,\n",
    "        # for example, 'data_folder'/'RECORD' or 'data_folder'/'EPISODE'\n",
    "\n",
    "    def find(self, query: PoolQuery) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Find records by PoolQuery with\n",
    "            - vehicle\n",
    "            - driver\n",
    "            - episodestart_start\n",
    "            - episodestart_end\n",
    "            - timestamp_start\n",
    "            - timestamp_end\n",
    "\n",
    "        return: a DataFrame with all records in the query time range\n",
    "        \"\"\"\n",
    "\n",
    "        df = self.get_query(query).compute()\n",
    "        assert type(df) == pd.DataFrame, f\"df is not a pandas DataFrame\"\n",
    "        return df\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_query(\n",
    "        self, query: Optional[PoolQuery] = None\n",
    "    ) -> Optional[Union[dd.DataFrame, db.Bag]]:\n",
    "        \"\"\"\n",
    "        Get records by PoolQuery with\n",
    "            - vehicle\n",
    "            - driver\n",
    "            - episodestart_start\n",
    "            - episodestart_end\n",
    "            - timestamp_start\n",
    "            - timestamp_end\n",
    "\n",
    "        return: a DataFrame with all records in the query time range\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def _count(self, query: Optional[PoolQuery] = None):\n",
    "        \"\"\"\n",
    "        Count the number of records in the db.\n",
    "        rule is an optional dictionary specifying a rule or\n",
    "        a pipeline in mongodb\n",
    "        query = {\n",
    "            - vehicle\n",
    "            - driver\n",
    "            - episodestart_start\n",
    "            - episodestart_end\n",
    "            - timestamp_start\n",
    "            - timestamp_end\n",
    "        }\n",
    "        \"\"\"\n",
    "        items = self.get_query(query)  # either a dask dataframe or a dask bag\n",
    "        if items is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return len(items.compute())\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def sample(self, size: int, *, query: Optional[PoolQuery] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Sample a batch of data from the pool\n",
    "        returns a pandas dataframe\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca6e829",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
