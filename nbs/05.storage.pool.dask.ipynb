{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c55a791d59b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af9544514c7579",
   "metadata": {},
   "source": [
    "# Dask\n",
    "\n",
    "> DaskPool class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccae633adbb03976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp storage.pool.dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d41e48c9875232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import abc\n",
    "from configparser import ConfigParser\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union\n",
    "import logging\n",
    "import dask.bag as db  # type: ignore\n",
    "import dask.dataframe as dd  # type: ignore\n",
    "import pandas as pd  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1851f8d3b7dcbe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from data_io_nbdev.data.core import ObservationMeta, PoolQuery\n",
    "from data_io_nbdev.storage.pool.pool import Pool  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d6c63678a9764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass(kw_only=True)\n",
    "class DaskPool(Pool[pd.DataFrame]):\n",
    "    \"\"\"\n",
    "    The pool Class to be derived from as shared common interfaces and attributes for ParquetPool and AvroPool\n",
    "\n",
    "    It has with the following features:\n",
    "\n",
    "        - use Dask dataframe for lazy data processing\n",
    "        - using dask delayed to parallelize the data processing like sampling,\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "        - recipe: a config file for the pool\n",
    "        - pl_path: the pool path, a Path object to the parquet file for RECORD, to avro file for EPISODE\n",
    "        - query: a PoolQuery object\n",
    "        - meta: the meta information for the data collection\n",
    "        - logger: a logger object\n",
    "        - dict_logger: a dictionary logger object\n",
    "    \"\"\"\n",
    "\n",
    "    recipe: ConfigParser  # field(default_factory=get_filemeta_config)\n",
    "    query: PoolQuery  # field(default_factory=PoolQuery)  # search record based on query in arrays is very inefficient\n",
    "    meta: ObservationMeta  # field(default_factory=ObservationMeta)  # meta information for the data collection\n",
    "    pl_path: Optional[\n",
    "        Path\n",
    "    ] = None  # Path('.')  # path to parquet file for RECORD, to avro file for EPISODE\n",
    "    # in record file pool, query is mostly ignored for sample, only for checking.\n",
    "    # list of dask DataFrame with the target vehicle and driver\n",
    "    logger: Optional[logging.Logger] = None\n",
    "    dict_logger: Optional[dict] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Parsing the recipe and set the pool path\"\"\"\n",
    "        super().__post_init__()\n",
    "\n",
    "        # all required specification for the data collection must be available\n",
    "        assert all(\n",
    "            key in self.recipe[\"DEFAULT\"]\n",
    "            for key in [\n",
    "                \"data_folder\",\n",
    "                \"recipe_file_name\",\n",
    "                \"coll_type\",\n",
    "            ]\n",
    "        ), f\"recipe specification incomplete!\"\n",
    "        self.pl_path = (\n",
    "            Path(self.recipe[\"DEFAULT\"][\"data_folder\"])\n",
    "            / self.recipe[\"DEFAULT\"][\"coll_type\"]\n",
    "        )  # coll_type used as part of the path of the parquet storage location,\n",
    "        # for example, 'data_folder'/'RECORD' or 'data_folder'/'EPISODE'\n",
    "\n",
    "    def find(self, query: PoolQuery) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Find records by `PoolQuery` with\n",
    "\n",
    "        Args:\n",
    "\n",
    "            query: a `PoolQuery` object\n",
    "\n",
    "        return:\n",
    "            a DataFrame with all records matching query specification\n",
    "        \"\"\"\n",
    "\n",
    "        df = self.get_query(query).compute()\n",
    "        assert type(df) == pd.DataFrame, f\"df is not a pandas DataFrame\"\n",
    "        return df\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_query(\n",
    "        self, query: Optional[PoolQuery] = None\n",
    "    ) -> Optional[Union[dd.DataFrame, db.Bag]]:\n",
    "        \"\"\"\n",
    "        Get records by `PoolQuery`\n",
    "\n",
    "        Args:\n",
    "\n",
    "            query: a `PoolQuery` object\n",
    "\n",
    "        return:\n",
    "\n",
    "            a DataFrame with all records in the query time range\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def _count(self, query: Optional[PoolQuery] = None):\n",
    "        \"\"\"\n",
    "        Count the number of records in the db.\n",
    "\n",
    "        Args:\n",
    "\n",
    "            query: a `PoolQuery` object\n",
    "\n",
    "        Return:\n",
    "\n",
    "                the number of records in the db\n",
    "        \"\"\"\n",
    "        items = self.get_query(query)  # either a dask dataframe or a dask bag\n",
    "        if items is None:\n",
    "            return 0\n",
    "        else:\n",
    "            return len(items.compute())\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def sample(\n",
    "        self,\n",
    "        size: int,  # required size of samples\n",
    "        *,\n",
    "        query: Optional[PoolQuery] = None,  # `PoolQuery` object, query specification\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Sample a batch of data from the pool\n",
    "\n",
    "        An abstract method to be implemented by the derived class `ParquetPool` and `AvroPool`\n",
    "\n",
    "        Args:\n",
    "\n",
    "            size: the number of records to be sampled\n",
    "            query: a `PoolQuery` object\n",
    "\n",
    "        Return:\n",
    "            a Pandas DataFrame\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c645748809c61b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb5930179dfb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DaskPool.get_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967cd1e7f8e8f628",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DaskPool.sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62513f899c72a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DaskPool._count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d51d06effc28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DaskPool.__post_init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94764d3afb9c1620",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(DaskPool.find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e3ed0549ac0baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
