{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b664c04e48f8dc76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7e068a1e87d115",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e0109056eb50bf",
   "metadata": {},
   "source": [
    "# RDPG\n",
    "\n",
    "> RDPG class\n",
    "> \n",
    ">Title: RDPG for VEOS\n",
    ">Author: Binjian Xin\n",
    ">Date created: 2021/12/07\n",
    ">Last modified: 2021/12/07\n",
    ">Description: Adapted from DDPG\n",
    ">\n",
    ">\n",
    ">Title: Memory-based control with recurrent neural networks (RDPG)\n",
    ">Author: Nicolas Hees, Jonathan J Hunt, Timothy P Lillicrap, David Silver\n",
    ">Description: Implementing RDPG algorithm on VEOS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e976ad40d5ea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp agent.rdpg.rdpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2014c26021aa468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from typeguard import check_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e8e4b31fadbfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from tspace.agent.utils.hyperparams import HyperParamRDPG\n",
    "from tspace.storage.buffer.dask import DaskBuffer\n",
    "from tspace.storage.buffer.mongo import MongoBuffer  # type: ignore\n",
    "from tspace.data.core import PoolQuery  # type: ignore\n",
    "from tspace.data.time import veos_lifetime_end_date, veos_lifetime_start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc2a383d6f77573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from tspace.agent.dpg import DPG  # type: ignore\n",
    "from tspace.agent.rdpg.seq_actor import SeqActor  # type: ignore\n",
    "from tspace.agent.rdpg.seq_critic import SeqCritic  # type: ignore\n",
    "from tspace.agent.utils.hyperparams import HyperParamRDPG  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d526e383936d8661",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#| hide\n",
    "from tspace.config.vehicles import Truck, TruckInCloud, trucks_by_id\n",
    "from tspace.config.drivers import Driver\n",
    "import logging\n",
    "from typing import Union\n",
    "from tspace.data.core import (\n",
    "    RE_RECIPEKEY,\n",
    "    ActionSpecs,\n",
    "    ObservationMetaCloud,\n",
    "    ObservationMetaECU,\n",
    "    RewardSpecs,\n",
    "    StateSpecsCloud,\n",
    "    StateSpecsECU,\n",
    "    get_filemeta_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65853ec37c435f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class RDPG(DPG):\n",
    "    \"\"\"RDPG agent for VEOS.\n",
    "\n",
    "    Abstracts:\n",
    "\n",
    "        data interface:\n",
    "            - pool in mongodb\n",
    "            - buffer in memory (numpy array)\n",
    "        model interface:\n",
    "            - actor network\n",
    "            - critic network\n",
    "\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "        actor_net: actor network\n",
    "        critic_net: critic network\n",
    "        target_actor_net: target actor network\n",
    "        target_critic_net: target critic network\n",
    "        _ckpt_actor_dir: checkpoint directory for actor\n",
    "        _ckpt_critic_dir: checkpoint directory for critic\n",
    "    \"\"\"\n",
    "\n",
    "    # Following are derived\n",
    "    actor_net: Optional[SeqActor] = None  # actor_net_default\n",
    "    critic_net: Optional[SeqCritic] = None  # critic_net_default\n",
    "    target_actor_net: Optional[SeqActor] = None  # actor_net_default\n",
    "    target_critic_net: Optional[SeqCritic] = None  # critic_net_default\n",
    "    _ckpt_actor_dir: Optional[Path] = None  # Path(\"\")\n",
    "    _ckpt_critic_dir: Optional[Path] = None  # Path(\"\")\n",
    "\n",
    "    def __post_init__(\n",
    "        self,\n",
    "    ):\n",
    "        \"\"\"initialize the rdpg agent.\n",
    "\n",
    "        args:\n",
    "            truck.ObservationNumber (int): dimension of the state space.\n",
    "            padding_value (float): value to pad the state with, impossible value for observation, action or re\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger = self.logger.getChild(\"eos\").getChild(self.__str__())\n",
    "        self.logger.propagate = True\n",
    "\n",
    "        super().__post_init__()  # call DPG post_init for pool init and plot init\n",
    "        self.coll_type = \"EPISODE\"\n",
    "        self.hyper_param = HyperParamRDPG(\n",
    "            HiddenDimension=256,\n",
    "            PaddingValue=-10000,\n",
    "            tbptt_k1=200,\n",
    "            tbptt_k2=200,\n",
    "            BatchSize=4,\n",
    "            NStates=self.truck.observation_numel,\n",
    "            NActions=self.truck.torque_flash_numel,\n",
    "            ActionBias=self.truck.torque_bias,\n",
    "            NLayerActor=2,\n",
    "            NLayerCritic=2,\n",
    "            Gamma=0.99,\n",
    "            TauActor=0.005,\n",
    "            TauCritic=0.005,\n",
    "            ActorLR=0.001,\n",
    "            CriticLR=0.001,\n",
    "            CkptInterval=5,\n",
    "        )\n",
    "\n",
    "        self.buffer.query = PoolQuery(\n",
    "            vehicle=self.truck.vid,\n",
    "            driver=self.driver.pid,\n",
    "            episodestart_start=veos_lifetime_start_date,\n",
    "            episodestart_end=veos_lifetime_end_date,\n",
    "            seq_len_from=1,  # from 10  # sample sequence with a length from 1 to 200\n",
    "            seq_len_to=self.hyper_param.tbptt_k1 + 100,  # to 300\n",
    "        )\n",
    "        self.buffer.pool.query = self.buffer.query\n",
    "\n",
    "        # actor network (w/ target network)\n",
    "        self.init_checkpoint()\n",
    "\n",
    "        self.actor_net = SeqActor(\n",
    "            self.truck.observation_numel,\n",
    "            self.truck.torque_flash_numel,\n",
    "            self.hyper_param.HiddenDimension,  # 256\n",
    "            self.hyper_param.NLayerActor,  # 2\n",
    "            self.hyper_param.BatchSize,  # 4\n",
    "            self.hyper_param.PaddingValue,  # -10000\n",
    "            self.hyper_param.TauActor,  # 0.005\n",
    "            self.hyper_param.ActorLR,  # 0.001\n",
    "            self._ckpt_actor_dir,\n",
    "            self.hyper_param.CkptInterval,  # 5\n",
    "            self.logger,\n",
    "            self.dict_logger,\n",
    "        )\n",
    "\n",
    "        self.target_actor_net = SeqActor(\n",
    "            self.truck.observation_numel,\n",
    "            self.truck.torque_flash_numel,\n",
    "            self.hyper_param.HiddenDimension,  # 256\n",
    "            self.hyper_param.NLayerActor,  # 2\n",
    "            self.hyper_param.BatchSize,  # 4\n",
    "            self.hyper_param.PaddingValue,  # -10000\n",
    "            self.hyper_param.TauActor,  # 0.005\n",
    "            self.hyper_param.ActorLR,  # 0.001\n",
    "            self._ckpt_actor_dir,\n",
    "            self.hyper_param.CkptInterval,  # 5\n",
    "            self.logger,\n",
    "            self.dict_logger,\n",
    "        )\n",
    "        # clone necessary for the first time training\n",
    "        self.target_actor_net.clone_weights(self.actor_net)\n",
    "\n",
    "        # critic network (w/ target network)\n",
    "\n",
    "        self.critic_net = SeqCritic(\n",
    "            self.truck.observation_numel,\n",
    "            self.truck.torque_flash_numel,\n",
    "            self.hyper_param.HiddenDimension,  # 256\n",
    "            self.hyper_param.NLayerCritic,  # 2\n",
    "            self.hyper_param.BatchSize,  # 4\n",
    "            self.hyper_param.PaddingValue,  # -10000\n",
    "            self.hyper_param.TauCritic,  # 0.005\n",
    "            self.hyper_param.CriticLR,  # 0.002\n",
    "            self._ckpt_critic_dir,\n",
    "            self.hyper_param.CkptInterval,  # 5\n",
    "            self.logger,\n",
    "            self.dict_logger,\n",
    "        )\n",
    "\n",
    "        self.target_critic_net = SeqCritic(\n",
    "            self.truck.observation_numel,\n",
    "            self.truck.torque_flash_numel,\n",
    "            self.hyper_param.HiddenDimension,  # 256\n",
    "            self.hyper_param.NLayerCritic,  # 2\n",
    "            self.hyper_param.BatchSize,  # 4\n",
    "            self.hyper_param.PaddingValue,  # -10000\n",
    "            self.hyper_param.TauCritic,  # 0.005\n",
    "            self.hyper_param.CriticLR,  # 0.002\n",
    "            self._ckpt_critic_dir,\n",
    "            self.hyper_param.CkptInterval,  # 5\n",
    "            self.logger,\n",
    "            self.dict_logger,\n",
    "        )\n",
    "        # clone necessary for the first time training\n",
    "        self.target_critic_net.clone_weights(self.critic_net)\n",
    "        self.touch_gpu()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"RDPG({self.truck.vid}, {self.driver.pid})\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"RDPG\"\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.__repr__())\n",
    "\n",
    "    def touch_gpu(self):\n",
    "        \"\"\"touch the gpu to avoid the first time delay\"\"\"\n",
    "\n",
    "        # tf.summary.trace_on(graph=true, profiler=true)\n",
    "        # ignites manual loading of tensorflow library, \\\n",
    "        # to guarantee the real-time processing of first data in main thread\n",
    "        init_states = pd.Series(\n",
    "            np.random.rand(self.truck.observation_numel)\n",
    "        )  # state must have 30 (speed, throttle, current, voltage) 5 tuple\n",
    "\n",
    "        # init_states = tf.expand_dims(input_array, 0)  # motion states is 30*2 matrix\n",
    "\n",
    "        _ = self.actor_predict(init_states)\n",
    "        self.logger.info(\n",
    "            f\"manual load tf library by calling convert_to_tensor\",\n",
    "            extra=self.dict_logger,\n",
    "        )\n",
    "\n",
    "        self.actor_net.ou_noise.reset()\n",
    "\n",
    "        # warm up the gpu training graph execution pipeline\n",
    "        if self.buffer.pool.cnt != 0:\n",
    "            if not self.infer_mode:\n",
    "                self.logger.info(\n",
    "                    f\"rdpg warm up training!\",\n",
    "                    extra=self.dict_logger,\n",
    "                )\n",
    "                (_, _) = self.train()\n",
    "\n",
    "                self.logger.info(\n",
    "                    f\"rdpg warm up training done!\",\n",
    "                    extra=self.dict_logger,\n",
    "                )\n",
    "\n",
    "    def init_checkpoint(self):\n",
    "        \"\"\"create or restore from checkpoint\"\"\"\n",
    "\n",
    "        # actor create or restore from checkpoint\n",
    "        # add checkpoints manager\n",
    "        self._ckpt_actor_dir = Path(self.data_folder).joinpath(\n",
    "            \"tf_ckpts-\"\n",
    "            + self.__str__()\n",
    "            + \"-\"\n",
    "            + self.truck.vid\n",
    "            + \"-\"\n",
    "            + self.driver.pid\n",
    "            + \"_\"\n",
    "            + \"/actor\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            os.makedirs(self._ckpt_actor_dir)\n",
    "            self.logger.info(\n",
    "                \"created checkpoint directory for actor: %s\",\n",
    "                self._ckpt_actor_dir,\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "        except FileExistsError:\n",
    "            self.logger.info(\n",
    "                \"actor checkpoint directory already exists: %s\",\n",
    "                self._ckpt_actor_dir,\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "\n",
    "        # critic create or restore from checkpoint\n",
    "        # add checkpoints manager\n",
    "        self._ckpt_critic_dir = Path(self.data_folder).joinpath(\n",
    "            \"tf_ckpts-\"\n",
    "            + self.__str__()\n",
    "            + \"-\"\n",
    "            + self.truck.vid\n",
    "            + \"-\"\n",
    "            + self.driver.pid\n",
    "            + \"_\"\n",
    "            + \"/critic\"\n",
    "        )\n",
    "        try:\n",
    "            os.makedirs(self._ckpt_critic_dir)\n",
    "            self.logger.info(\n",
    "                f\"created checkpoint directory for critic: %s\",\n",
    "                self._ckpt_critic_dir,\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "        except FileExistsError:\n",
    "            self.logger.info(\n",
    "                f\"critic checkpoint directory already exists: %s\",\n",
    "                self._ckpt_critic_dir,\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "\n",
    "    # TODO for infer only mode, implement a method without noisy exploration.\n",
    "    def actor_predict(\n",
    "        self, state: pd.Series  # state sequence of the current episode\n",
    "    ) -> np.ndarray:  # action sequence of the current episode\n",
    "        \"\"\"get the action given a single observations by inference\n",
    "\n",
    "        batch size cannot be 1.\n",
    "        For LSTM to be stateful, batch size must match the training scheme.\n",
    "        \"\"\"\n",
    "\n",
    "        # get the current episode so far from self.observations stored by DPG.deposit()\n",
    "        # self.state_t = np.ones((1, t + 1, self._num_states))\n",
    "        # self.state_t[0, 0, :] = obs\n",
    "        # expand the batch dimension and turn obs_t into a numpy array\n",
    "\n",
    "        # expand states to 3D tensor [4, 1, 600] for cloud / [4, 1, 90] for kvaser\n",
    "        states = tf.convert_to_tensor(\n",
    "            np.expand_dims(\n",
    "                np.outer(\n",
    "                    np.ones((self.hyper_param.BatchSize, 1)),  # type: ignore\n",
    "                    state.values,  # type: ignore\n",
    "                ),\n",
    "                axis=1,\n",
    "            ),\n",
    "            dtype=tf.float32,\n",
    "        )\n",
    "\n",
    "        # expand actions to 3D tensor [4, 1, 68] for cloud / [4, 1, 68] for kvaser\n",
    "        idx = pd.IndexSlice\n",
    "        try:\n",
    "            last_actions = tf.convert_to_tensor(\n",
    "                np.expand_dims(\n",
    "                    np.outer(\n",
    "                        np.ones(self.hyper_param.BatchSize),\n",
    "                        self.observations[-1]  # last observation contains last action!\n",
    "                        .sort_index()\n",
    "                        .loc[idx[\"action\", self.torque_table_row_names, :]]\n",
    "                        .values.astype(np.float32),  # type convert to float32\n",
    "                    ),\n",
    "                    axis=1,  # observation (with subpart action is Multi-Indexed Series, its values are flatted\n",
    "                ),  # get last_actions from last observation,\n",
    "                dtype=tf.float32,  # and add batch and time dimension twice at axis 0\n",
    "            )  # so that last_actions is a 3D tensor\n",
    "        except (\n",
    "            IndexError,\n",
    "            TypeError,\n",
    "        ):  # if no last action in case of the first step of the episode, then use zeros\n",
    "            last_actions = tf.zeros(\n",
    "                shape=(\n",
    "                    self.hyper_param.BatchSize,\n",
    "                    1,\n",
    "                    self.truck.torque_flash_numel,\n",
    "                ),  # [1, 1, 4*17]\n",
    "                dtype=tf.float32,\n",
    "            )  # first zero last_actions is a 3D tensor\n",
    "        # self.logger.info(\n",
    "        #     f\"states.shape: {states.shape}; last_actions.shape: {last_actions.shape}\",\n",
    "        #     extra=self.dict_logger,\n",
    "        # )\n",
    "        # action = self.actor_net.predict(input_array)\n",
    "        actions = self.actor_predict_step(\n",
    "            states, last_actions\n",
    "        )  # both states and last_actions are 3d tensors [B,T,D]\n",
    "        action = actions.numpy()[\n",
    "            0, :\n",
    "        ]  # [1, 68] for cloud / [1, 68] for kvaser, squeeze the batch dimension\n",
    "        # self.logger.info(f\"action.shape: {action.shape}\", extra=self.dict_logger)\n",
    "        assert (\n",
    "            type(action) == np.ndarray\n",
    "        ), f\"action type {type(action)} is not np.ndarray\"\n",
    "        return action\n",
    "\n",
    "    @tf.function(\n",
    "        input_signature=[\n",
    "            tf.TensorSpec(\n",
    "                shape=[None, None, DPG.truck_type.observation_numel],\n",
    "                dtype=tf.float32,\n",
    "            ),  # [None, None, 600] for cloud / [None, None, 90] for kvaser\n",
    "            tf.TensorSpec(\n",
    "                shape=[None, None, DPG.truck_type.torque_flash_numel], dtype=tf.float32\n",
    "            ),  # [None, None, 68] for both cloud and kvaser\n",
    "        ]\n",
    "    )\n",
    "    def actor_predict_step(\n",
    "        self,\n",
    "        states: tf.Tensor,  # state, dimension: [B,T,D]\n",
    "        last_actions: tf.Tensor,  # last action, dimension [B,T,D]\n",
    "    ) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        evaluate the actors given a single observations.\n",
    "\n",
    "        batch size is 1.\n",
    "        \"\"\"\n",
    "        # logger.info(f\"tracing\", extra=self.dict_logger)\n",
    "        print(\"tracing!\")\n",
    "        action = self.actor_net.predict(\n",
    "            states, last_actions\n",
    "        )  # already un-squeezed inside Actor function\n",
    "        assert isinstance(\n",
    "            action, tf.Tensor\n",
    "        ), f\"action type {type(action)} is not tf.Tensor\"\n",
    "        return action\n",
    "\n",
    "    def train(self) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        train the actor and critic moving network with truncated Backpropagation through time (TBPTT)\n",
    "\n",
    "\n",
    "        with k1 = k2 = self.hyperparam.tbptt_k1 (keras)\n",
    "\n",
    "        Return:\n",
    "            tuple: (actor_loss, critic_loss)\n",
    "        \"\"\"\n",
    "\n",
    "        # reset the states of the stateful moving and target networks at the beginning of the training\n",
    "        self.actor_net.eager_model.reset_states()\n",
    "        self.critic_net.eager_model.reset_states()\n",
    "        self.target_actor_net.eager_model.reset_states()\n",
    "        self.target_critic_net.eager_model.reset_states()\n",
    "        actor_loss = tf.constant(0.0)\n",
    "        critic_loss = tf.constant(0.0)\n",
    "        s_n_t, a_n_t, r_n_t, ns_n_t = self.buffer.sample()  # ignore next state for now\n",
    "        # split_num = (\n",
    "        #     s_n_t.shape[1] // check_type(self.hyper_param, HyperParamRDPG).tbptt_k1\n",
    "        #     + 1\n",
    "        #     # after padding all observations have the same length (the length of  the longest episode)\n",
    "        # )  # 18//20+1=1, 50//20+1=3, for short episode if tbptt_k1> episode length, no split\n",
    "        # self.logger.info(\n",
    "        #     f\"{{'header': 'Batch splitting', \" f\"'split_num': '{split_num}'}}\",\n",
    "        #     extra=self.dict_logger,\n",
    "        # )\n",
    "        # if split_num <= 0:\n",
    "        #     raise ValueError(\"split_num <= 0, check tbptt_k1 and episode length\")\n",
    "        # for i, batch_t in enumerate(\n",
    "        #         zip(\n",
    "        #             np.array_split(\n",
    "        #                 s_n_t, split_num, axis=1\n",
    "        #             ),  # split on the time axis (axis=1)\n",
    "        #             np.array_split(a_n_t, split_num, axis=1),\n",
    "        #             np.array_split(r_n_t, split_num, axis=1),\n",
    "        #             np.array_split(ns_n_t, split_num, axis=1),\n",
    "        #         )\n",
    "        # ):  # all actor critic have stateful LSTMs so that the LSTM states are kept between sub-batches,\n",
    "        # # while trainings extend only to the end of each sub-batch by default of train_step\n",
    "        # # out of tf.GradientTape() context, the tensors are detached like .detach() in pytorch\n",
    "\n",
    "        # if s_n_t.shape[1] > check_type(self.hyper_param, HyperParamRDPG).tbptt_k1:\n",
    "        ind_split = (\n",
    "            np.arange(\n",
    "                s_n_t.shape[1] // check_type(self.hyper_param, HyperParamRDPG).tbptt_k1\n",
    "            )\n",
    "            + 1\n",
    "        ) * check_type(\n",
    "            self.hyper_param, HyperParamRDPG\n",
    "        ).tbptt_k1  # split index is np.arange(l//s)[1:]*s\n",
    "        # else: for l==s, split in original array and np.array([])\n",
    "        # for l<s    ind_split = np.array([])  # no split for l <= split size\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"{{'header': 'Batch splitting', \" f\"'split_num': '{ind_split}'}}\",\n",
    "            extra=self.dict_logger,\n",
    "        )\n",
    "\n",
    "        for i, batch_t in enumerate(  # split on the time axis (axis=1)\n",
    "            zip(\n",
    "                np.array_split(s_n_t, ind_split, axis=1),\n",
    "                np.array_split(a_n_t, ind_split, axis=1),\n",
    "                np.array_split(r_n_t, ind_split, axis=1),\n",
    "                np.array_split(ns_n_t, ind_split, axis=1),\n",
    "            )\n",
    "        ):  # all actor critic have stateful LSTMs so that the LSTM states are kept between sub-batches,\n",
    "            # while trainings extend only to the end of each sub-batch by default of train_step\n",
    "            # out of tf.GradientTape() context, the tensors are detached like .detach() in pytorch\n",
    "            s_n_t_sub, a_n_t_sub, r_n_t_sub, ns_n_t_sub = batch_t\n",
    "            if s_n_t_sub is np.array([]):\n",
    "                self.logger.warning(\n",
    "                    f\"batch sub sequence s_n_t: {i} is empty!\", extra=self.dict_logger\n",
    "                )\n",
    "                continue\n",
    "            else:\n",
    "                self.logger.info(\n",
    "                    f\"batch sub sequences s_n_t: {i} is valid. \", extra=self.dict_logger\n",
    "                )\n",
    "\n",
    "            actor_loss, critic_loss = self.train_step(\n",
    "                s_n_t_sub, a_n_t_sub, r_n_t_sub, ns_n_t_sub\n",
    "            )\n",
    "            self.logger.info(\n",
    "                f\"batch actor loss: {actor_loss.numpy()}; batch critic loss: {critic_loss.numpy()}\",\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "\n",
    "        # return the last actor and critic loss\n",
    "        # return actor_loss.numpy()[0], critic_loss.numpy()[0]\n",
    "        return actor_loss.numpy(), critic_loss.numpy()\n",
    "\n",
    "    @tf.function(\n",
    "        input_signature=[\n",
    "            tf.TensorSpec(\n",
    "                shape=[\n",
    "                    DPG.rdpg_hyper_type.BatchSize,\n",
    "                    None,\n",
    "                    DPG.truck_type.observation_numel,\n",
    "                ],\n",
    "                dtype=tf.float32,\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                shape=[\n",
    "                    DPG.rdpg_hyper_type.BatchSize,\n",
    "                    None,\n",
    "                    DPG.truck_type.torque_flash_numel,\n",
    "                ],\n",
    "                dtype=tf.float32,\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                shape=[DPG.rdpg_hyper_type.BatchSize, None, 1], dtype=tf.float32\n",
    "            ),\n",
    "            tf.TensorSpec(\n",
    "                shape=[\n",
    "                    DPG.rdpg_hyper_type.BatchSize,\n",
    "                    None,\n",
    "                    DPG.truck_type.observation_numel,\n",
    "                ],\n",
    "                dtype=tf.float32,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    def train_step(self, s_n_t, a_n_t, r_n_t, ns_n_t) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "        \"\"\"train in one step the critic using bptt\"\"\"\n",
    "        print(\"tracing train_step!\")\n",
    "        self.logger.info(f\"start train_step with tracing\")\n",
    "        # logger.info(f\"start train_step\")\n",
    "\n",
    "        gamma = tf.convert_to_tensor(self.hyper_param.Gamma, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # actions at h_t+1\n",
    "            self.logger.info(f\"start evaluate_actions\")\n",
    "            t_a_ht1 = self.target_actor_net.evaluate_actions(\n",
    "                ns_n_t[:, 1:, :],  # s_1, s_2, ..., s_n, ...\n",
    "                a_n_t[:, 1:, :],  # a_0, a_1, ..., a_{n-1}, ...\n",
    "            )  # t_actor(h_{t+1}): [h_1(a_0, s_1), h_2(a_1, s_2), ..., h_n(a_{n-1}, s_n), ...]\n",
    "            print(f\"t_a_ht1.shape: {t_a_ht1.shape}, type: {t_a_ht1.dtype}\")\n",
    "\n",
    "            # state action value at h_t+1\n",
    "            # logger.info(f\"o_n_t.shape: {self.o_n_t.shape}\")\n",
    "            # logger.info(f\"t_a_ht1.shape: {self.t_a_ht1.shape}\")\n",
    "            self.logger.info(f\"start critic evaluate_q\")\n",
    "            t_q_ht1 = self.target_critic_net.evaluate_q(\n",
    "                ns_n_t[:, 1:, :], a_n_t[:, 1:, :], t_a_ht1\n",
    "            )  # t_critic(h_{t+1}, t_actor(h_{t+1})): [h_1(s_1, a_0), h_2(s_2, a_1), ..., h_n(s_n, a_{n-1}), ...]\n",
    "            # self.logger.info(f\"critic evaluate_q done, t_q_ht1.shape: {t_q_ht1.shape}\")\n",
    "            print(\n",
    "                f\"critic evaluate_q done, t_q_ht1.shape: {t_q_ht1[:, :-1,:].shape}, type: {t_q_ht1.dtype}\"\n",
    "            )\n",
    "            print(f\"r_n_t.shape: {r_n_t[:,1:,:].shape}, type: {r_n_t.dtype}\")\n",
    "\n",
    "            # logger.info(f\"t_q_ht_bl.shape: {t_q_ht_bl.shape}\")\n",
    "            # y_n_t shape (batch_size, seq_len, 1), target value\n",
    "            y_n_t = (\n",
    "                r_n_t[:, 1:, :] + gamma * t_q_ht1  # fix t_q_ht1[:, :-1, :]!\n",
    "            )  # y0(r_0, Q(h_1,mu(h_1))), y1(r_1, Q(h_2,mu(h_2)), ...)\n",
    "            # self.logger.info(f\"y_n_t.shape: {y_n_t.shape}\")\n",
    "            print(f\"y_n_t.shape: {y_n_t.shape}\")\n",
    "\n",
    "            # scalar value, average over the batch, time steps\n",
    "            critic_loss = tf.math.reduce_mean(\n",
    "                y_n_t\n",
    "                - self.critic_net.evaluate_q(\n",
    "                    s_n_t[:, 1:, :], a_n_t[:, :-1, :], a_n_t[:, 1:, :]\n",
    "                )  # Q(s_t, a_{t-1}, a_t): Q(s_0, a_-1, a_0), Q(s_1, a_0, a_1), ..., Q(s_n, a_{n-1}, a_n)\n",
    "            )\n",
    "        critic_grad = tape.gradient(\n",
    "            critic_loss, self.critic_net.eager_model.trainable_variables\n",
    "        )\n",
    "        self.critic_net.optimizer.apply_gradients(\n",
    "            zip(critic_grad, self.critic_net.eager_model.trainable_variables)\n",
    "        )\n",
    "        # self.logger.info(f\"applied critic gradient\", extra=self.dict_logger)\n",
    "        print(f\"applied critic gradient\")\n",
    "\n",
    "        # train actor using bptt\n",
    "        with tf.GradientTape() as tape:\n",
    "            # self.logger.info(f\"start actor evaluate_actions\", extra=self.dict_logger)\n",
    "            print(f\"start actor evaluate_actions\")\n",
    "            a_ht = self.actor_net.evaluate_actions(\n",
    "                s_n_t[:, 1:, :], a_n_t[:, :-1, :]\n",
    "            )  # states, last actions: (s_0, a_-1), (s_1, a_0), ..., (s_n, a_{n-1})\n",
    "            # self.logger.info(\n",
    "            #     f\"actor evaluate_actions done, a_ht.shape: {a_ht.shape}\",\n",
    "            #     extra=self.dict_logger,\n",
    "            # )\n",
    "            print(f\"actor evaluate_actions done, a_ht.shape: {a_ht.shape}\")\n",
    "            q_ht = self.critic_net.evaluate_q(s_n_t[:, 1:, :], a_n_t[:, :-1, :], a_ht)\n",
    "            # self.logger.info(\n",
    "            #     f\"actor evaluate_q done, q_ht.shape: {q_ht.shape}\",\n",
    "            #     extra=self.dict_logger,\n",
    "            # )\n",
    "            print(f\"actor evaluate_q done, q_ht.shape: {q_ht.shape}\")\n",
    "            # logger.info(f\"a_ht.shape: {self.a_ht.shape}\")\n",
    "            # logger.info(f\"q_ht.shape: {self.q_ht.shape}\")\n",
    "            # -1 because we want to maximize the q_ht\n",
    "            # scalar value, average over the batch and time steps\n",
    "            actor_loss = -tf.math.reduce_mean(q_ht)\n",
    "\n",
    "        actor_grad = tape.gradient(\n",
    "            actor_loss, self.actor_net.eager_model.trainable_variables\n",
    "        )\n",
    "        # logger.info(f\"action_gradients: {action_gradients}\")\n",
    "        # logger.info(f\"actor_grad_weight: {actor_grad_weight} vs actor_grad: {actor_grad}\")\n",
    "        # logger.info(f\"the grad diff: {actor_grad - actor_grad_weight}\")\n",
    "        self.actor_net.optimizer.apply_gradients(\n",
    "            zip(actor_grad, self.actor_net.eager_model.trainable_variables)\n",
    "        )\n",
    "        # self.logger.info(f\"applied actor gradient\", extra=self.dict_logger)\n",
    "        print(f\"applied actor gradient\")\n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def end_episode(self):\n",
    "        \"\"\"end the episode by depositing the episdoe and resetting the states of the actor and critic networks\"\"\"\n",
    "        super().end_episode()\n",
    "        # reset the states of the actor and critic networks\n",
    "        self.actor_net.eager_model.reset_states()\n",
    "        self.critic_net.eager_model.reset_states()\n",
    "        self.target_actor_net.eager_model.reset_states()\n",
    "        self.target_critic_net.eager_model.reset_states()\n",
    "\n",
    "    def get_losses(self):\n",
    "        \"\"\"get the losses of the actor and critic networks\"\"\"\n",
    "        pass\n",
    "\n",
    "    def notrain(self):\n",
    "        \"\"\"\n",
    "        purely evaluate the actor and critic networks to  return the losses without training.\n",
    "\n",
    "        Return:\n",
    "            tuple: (actor_loss, critic_loss)\n",
    "        \"\"\"\n",
    "\n",
    "        s_n_t, a_n_t, r_n_t, ns_n_t = self.buffer.sample()  # ignore the next state\n",
    "\n",
    "        # get critic loss\n",
    "        # actions at h_t+1\n",
    "        t_a_ht1 = self.target_actor_net.evaluate_actions(\n",
    "            ns_n_t[:, 1:, :],  # s_1, s_2, ..., s_n\n",
    "            a_n_t[:, 1:, :],  # a_0, a_1, ..., a_{n-1}\n",
    "        )  # t_actor(h_{t+1}, s_{t+1}): [h_1(s_1), h_2(s_2), ..., h_n(s_n), ...]\n",
    "\n",
    "        # state action value at h_t+1\n",
    "        t_q_ht1 = self.target_critic_net.evaluate_q(\n",
    "            ns_n_t[:, 1:, :], a_n_t[:, 1:, :], t_a_ht1\n",
    "        )  # t_critic(h_{t+1}, t_actor(h_{t+1})): [h_1(s_1, a_0), h_2(s_2, a_1), ..., h_n(s_n, a_{n-1}), ...]\n",
    "\n",
    "        # y_n_t shape (batch_size, seq_len, 1)\n",
    "        y_n_t = (\n",
    "            r_n_t[:, 1:, :] + self.hyper_param.Gamma * t_q_ht1\n",
    "        )  # y0(r_0, Q(h_1,mu(h_1))), y1(r_1, Q(h_2, mu(h_2)), ...)\n",
    "\n",
    "        # scalar value, average over the batch, time steps\n",
    "        critic_loss = tf.math.reduce_mean(\n",
    "            y_n_t\n",
    "            - self.critic_net.evaluate_q(\n",
    "                s_n_t[:, 1:, :], a_n_t[:, :-1, :], a_n_t[:, 1:, :]\n",
    "            )\n",
    "        )  # Q(s_t, a_{t-1}, a_t): Q(s_0, a_-1, a_0), Q(s_1, a_0, a_1), ..., Q(s_n, a_{n-1}, a_n)\n",
    "\n",
    "        # get  actor loss\n",
    "        a_ht = self.actor_net.evaluate_actions(\n",
    "            s_n_t[:, 1:, :], a_n_t[:, :-1, :]\n",
    "        )  # states, last actions: (s_0, a_-1), (s_1, a_0), ..., (s_n, a_{n-1})\n",
    "        q_ht = self.critic_net.evaluate_q(s_n_t[:, 1:, :], a_n_t[:, :-1, :], a_ht)\n",
    "\n",
    "        # -1 because we want to maximize the q_ht\n",
    "        # scalar value, average over the batch and time steps\n",
    "        actor_loss = tf.math.reduce_mean(-q_ht)\n",
    "\n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def soft_update_target(self):\n",
    "        \"\"\"\n",
    "        update target networks with tiny tau value,\n",
    "\n",
    "        typical value 0.001. done after each batch, slowly update target by polyak averaging.\n",
    "        \"\"\"\n",
    "        self.target_critic_net.soft_update(self.critic_net)\n",
    "        self.target_actor_net.soft_update(self.actor_net)\n",
    "\n",
    "    def save_ckpt(self):\n",
    "        \"\"\"save the checkpoints of the actor and critic networks\"\"\"\n",
    "        self.actor_net.save_ckpt()\n",
    "        self.critic_net.save_ckpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346feb975914f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c73c330d624d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RDPG.__post_init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94de4aabb36b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RDPG.__repr__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc02ea545ffa1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RDPG.__str__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe86f79727c6201",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RDPG.__hash__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907c654c64226a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RDPG.touch_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd908cd83bd0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RDPG.init_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf7c29c86c9f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RDPG.actor_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe48e8184460f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RDPG.actor_predict_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7acda039fa53438",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RDPG.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2738b7df20b0e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RDPG.train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279dbb31365b32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RDPG.end_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b68f68ad288f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RDPG.get_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec15c22b1a40768",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RDPG.notrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c672512720c25ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RDPG.soft_update_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9cc187183f0554",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(RDPG.save_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3258b2e848c56b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
