{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7af22e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cb2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db  # type: ignore\n",
    "import fastavro\n",
    "import pandas as pd  # type: ignore\n",
    "from dask.bag import Bag, random\n",
    "from dask.diagnostics import ProgressBar  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e35b6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eos.data_io.eos_struct import (\n",
    "    ObservationMeta,\n",
    "    PoolQuery,\n",
    "    veos_lifetime_end_date,\n",
    "    veos_lifetime_start_date,\n",
    "    locations_by_abbr,\n",
    ")\n",
    "from eos.data_io.utils import avro_ep_decoding, avro_ep_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e7557",
   "metadata": {},
   "outputs": [],
   "source": [
    "from .dask_pool import DaskPool  # type: ignore\n",
    "from .episode_avro_schema import gen_episode_schema  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e046e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(kw_only=True)\n",
    "class AvroPool(DaskPool):  # type: ignore   # pycharm bug\n",
    "    \"\"\"\n",
    "    AvroPool is the avro storage for pooling the real-time data from the cloud.\n",
    "    It's supposed to support large local data pool with buffer capacity\n",
    "    only bounded by local system storage.\n",
    "    In one single folder.\n",
    "    It uses pyarrow parquet for data storage and dask dataframe for data processing.\n",
    "    meta information is stored in parquet metadata (in footer of parquet file).\n",
    "\n",
    "    TODO alternative will be save experience in episodes\n",
    "    TODO sample random quadruples will need some care to reassure the randomness\n",
    "\n",
    "    TODO using dask dataframe for data processing\n",
    "    TODO using dask delayed to parallelize the data processing like sampling, while appending data\n",
    "    for each\n",
    "    NPAStore provides the following features:\n",
    "    - location: can be provided to change the default location in recipe\n",
    "    - recipe: the config file for the pool\n",
    "    \"\"\"\n",
    "\n",
    "    dbg: Optional[db.Bag] = None  # db.from_sequence([])  # dask DataFrame\n",
    "    dbg_schema: Optional[\n",
    "        dict\n",
    "    ] = None  # field(default_factory=dict)  # schema for avro file decoding\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.logger = self.logger.getChild(\"avro pool\")\n",
    "        self.dict_logger = self.dict_logger\n",
    "        super().__post_init__()\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"{{'header': 'Arrow pool stored', \"\n",
    "            f\"'path': '{self.pl_path}', \"\n",
    "            f\"'coll_type' : '{self.recipe['DEFAULT']['coll_type']}'}}\",\n",
    "            extra=self.dict_logger,\n",
    "        )\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"load EPISODE arrays from avro files in folder specified by the recipe\"\"\"\n",
    "\n",
    "        schema = gen_episode_schema(\n",
    "            self.meta\n",
    "        )  # schema for avro file decoding into episode bag\n",
    "\n",
    "        self.dbg_schema = fastavro.parse_schema(schema)\n",
    "        self.logger.info(\n",
    "            f\"{{'header': 'Arrow pool loaded', \",\n",
    "            f\"'path': '{self.pl_path}'}}\",\n",
    "            extra=self.dict_logger,\n",
    "        )\n",
    "        try:\n",
    "            with ProgressBar():\n",
    "                self.dbg = db.read_avro(\n",
    "                    str(self.pl_path / \"bag_episodes.*.avro\"),  # Path to str conversion\n",
    "                )\n",
    "                # # parquet file which is partitioned by a timestamp was converted to category,\n",
    "                # # when loaded to dask dataframe\n",
    "                # self.dbg[\"episodestart__\"] = self.dbg[\"episodestart__\"].astype(\n",
    "                #     \"datetime64[ns]\"\n",
    "                # )  # very important for indexing and slicing!!!\n",
    "            self.logger.info(\n",
    "                f\"{{'header': 'Loading bag from avro files.',  \"\n",
    "                f\"'path': '{self.pl_path}'}}\",\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "        except (FileNotFoundError, ValueError) as e:\n",
    "            self.logger.info(\n",
    "                f'Data folder ({self.recipe[\"DEFAULT\"][\"data_folder\"]}) is empty! parquet files not found: {e} ...'\n",
    "            )\n",
    "            self.logger.info(\n",
    "                f'Create data folder ({self.recipe[\"DEFAULT\"][\"data_folder\"]}) for Apache Arrow parquet files!'\n",
    "            )\n",
    "            self.pl_path.mkdir(parents=True, exist_ok=True)\n",
    "            self.cnt = 0\n",
    "            return\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Loading avro error: {e}\", extra=self.dict_logger)\n",
    "            raise e\n",
    "\n",
    "        # Deduplicate the bag, avro file take duplicated records.\n",
    "        # Otherwise, a separate task to clean up duplications periodically is required.\n",
    "        self.dbg = self.dbg.distinct(\n",
    "            lambda x: (\n",
    "                x[\"meta\"][\"episode_meta\"][\"driver\"],\n",
    "                x[\"meta\"][\"episode_meta\"][\"vehicle\"],\n",
    "                x[\"meta\"][\"episode_meta\"][\"episodestart\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # extract metadata stored in an avro record and compare with input metadata\n",
    "        meta_in_an_episode = self.dbg.take(1)[0][\n",
    "            \"meta\"\n",
    "        ]  # take the first episode, take return a tuple of dicts\n",
    "        episode_meta = meta_in_an_episode[\"episode_meta\"]\n",
    "        observation_meta = meta_in_an_episode[\"observation_meta\"]\n",
    "        observation_meta[\"site\"] = locations_by_abbr[observation_meta[\"site\"][\"abbr\"]]\n",
    "        pool_meta = ObservationMeta(**observation_meta)\n",
    "        self.logger.info(\n",
    "            f\"meta in avro file: {observation_meta}\", extra=self.dict_logger\n",
    "        )\n",
    "        assert self.meta.have_same_meta(\n",
    "            pool_meta\n",
    "        ), f\"meta information in avro file doesn't match with input meta information!\"\n",
    "        # TODO if different, raise warning and update meta information in parquet file\n",
    "\n",
    "        self.cnt = self._count(self.query)\n",
    "\n",
    "    def close(self):\n",
    "        self.logger.info(\n",
    "            f\"Nothing to be done for dask avro pool!\",  # neither arrow parquet nor avro need cleaning up.\n",
    "            extra=self.dict_logger,\n",
    "        )\n",
    "\n",
    "    def store(self, episode: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Deposit an episode as a single item into avro.\n",
    "        \"\"\"\n",
    "\n",
    "        episode_dict_nested = avro_ep_encoding(episode)\n",
    "        indices_dict = [\n",
    "            {episode.index.names[i]: level for i, level in enumerate(levels)}\n",
    "            for levels in episode.index\n",
    "        ]\n",
    "        episode_meta = indices_dict[0]  # only one row in the dataframe\n",
    "        try:\n",
    "            episode_meta.pop(\"timestamp\")\n",
    "        except KeyError:\n",
    "            raise KeyError(f\"timestamp is not in index of episode dataframe!\")\n",
    "\n",
    "        episode_meta[\"episodestart\"] = (\n",
    "            episode_meta[\"episodestart\"].timestamp() * 1e6\n",
    "        )  # convert to microsecond\n",
    "        # convert the dict to a dask bag file\n",
    "        # observation_meta = ObservationMeta(\n",
    "        #     state_specs=self.meta.state_specs,\n",
    "        #     action_specs=self.meta.action_specs,\n",
    "        #     reward_specs=self.meta.reward_specs,\n",
    "        #     site=self.meta.site,\n",
    "        # )\n",
    "        records_episode_to_add = {\n",
    "            \"episodestart\": episode_meta[\"episodestart\"],\n",
    "            \"meta\": {\n",
    "                \"episode_meta\": episode_meta,\n",
    "                \"observation_meta\": self.meta.model_dump(),\n",
    "            },\n",
    "            \"sequence\": episode_dict_nested,\n",
    "        }\n",
    "\n",
    "        dask_bag_ep = db.from_sequence([records_episode_to_add], npartitions=1)\n",
    "        if self.dbg is not None:\n",
    "            self.dbg = db.concat([self.dbg, dask_bag_ep])\n",
    "        else:\n",
    "            self.dbg = dask_bag_ep\n",
    "\n",
    "        try:\n",
    "            with ProgressBar():\n",
    "                self.dbg.to_avro(\n",
    "                    self.pl_path / \"bag_episodes.*.avro\",\n",
    "                    schema=self.dbg_schema,\n",
    "                )\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Writing avro error: {e}\", extra=self.dict_logger)\n",
    "        else:\n",
    "            self.cnt = self.cnt + 1\n",
    "            self.logger.info(f\"deposit one episode in avro\", extra=self.dict_logger)\n",
    "        finally:\n",
    "            self.logger.info(f\"Done avro pool.\")\n",
    "\n",
    "    def get_query(self, query: Optional[PoolQuery] = None) -> Optional[Bag]:\n",
    "        \"\"\"\n",
    "        get query from dask dataframe\n",
    "            - vehicle: str\n",
    "            - driver: str\n",
    "            - episodestart_start: datetime\n",
    "            - episodestart_end: datetime\n",
    "            - timestamp_start: datetime\n",
    "            - timestamp_end: datetime\n",
    "\n",
    "        return: a Dask DataFrame with all records in the query time range\n",
    "        \"\"\"\n",
    "        assert query is not None, f\"query is None!\"\n",
    "\n",
    "        if query.episodestart_start is None:\n",
    "            query.episodestart_start = veos_lifetime_start_date.to_pydatetime()\n",
    "\n",
    "        if query.episodestart_end is None:\n",
    "            query.episodestart_end = veos_lifetime_end_date.to_pydatetime()\n",
    "\n",
    "        if query.seq_len_from is None:\n",
    "            query.seq_len_from = 0\n",
    "\n",
    "        if not query.seq_len_to is None:\n",
    "            query.seq_len_to = int(\n",
    "                1e09\n",
    "            )  # 1 bio steps is enough as upper bound >74k Years\n",
    "        if self.dbg is None:\n",
    "            return None\n",
    "        queried = self.dbg.filter(\n",
    "            lambda x: x[\"meta\"][\"episode_meta\"][\"vehicle\"] == query.vehicle\n",
    "            and x[\"meta\"][\"episode_meta\"][\"driver\"] == query.driver\n",
    "            and (\n",
    "                pd.Timestamp(query.episodestart_start)\n",
    "                .tz_convert(tz='UTC')\n",
    "                .tz_localize(None)\n",
    "                < pd.to_datetime(\n",
    "                    x[\"meta\"][\"episode_meta\"][\"episodestart\"], unit=\"us\"\n",
    "                )  # timestamp in avro is UTC in microsecond\n",
    "                < pd.Timestamp(query.episodestart_end)\n",
    "                .tz_convert(tz='UTC')\n",
    "                .tz_localize(None)\n",
    "            )  # do timestamps from avro get need conversion? x[\"meta\"][\"episode_meta\"][\"episodestart\"]? x:\n",
    "            and (query.seq_len_from < len(x[\"sequence\"]) < query.seq_len_to)\n",
    "        )\n",
    "        assert isinstance(queried, Bag), f\"queried is not a bag!\"\n",
    "        return queried\n",
    "\n",
    "    def find(self, query: PoolQuery) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Find records by PoolQuery with\n",
    "            - vehicle: str\n",
    "            - driver\n",
    "            - episodestart_start: datetime\n",
    "            - episodestart_end\n",
    "            - timestamp_start\n",
    "            - timestamp_end\n",
    "\n",
    "        return: a DataFrame with all episodes in the query range.\n",
    "        downstream can use pandas dataframe unique() for index to get unique episodes.\n",
    "        \"\"\"\n",
    "\n",
    "        queried_dict = self.get_query(query).compute()\n",
    "        df_episodes = avro_ep_decoding(queried_dict, tz_info=query.episodestart_start.tzinfo)  # type: ignore\n",
    "\n",
    "        return df_episodes\n",
    "\n",
    "    def delete(self, idx) -> None:\n",
    "        \"\"\"\n",
    "        Delete a record by item id.\n",
    "        not implemented for arrow pool\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def remove_episode(self, query: PoolQuery) -> None:\n",
    "        \"\"\"\n",
    "        remove episodes in the query from bag, but not from avro file!\n",
    "\n",
    "        Expected:\n",
    "        Delete all records in an episode by PoolQuery with\n",
    "            - vehicle\n",
    "            - driver\n",
    "            - episodestart_start\n",
    "            - episodestart_end\n",
    "        \"\"\"\n",
    "\n",
    "        self.dbg = self.dbg.remove(\n",
    "            lambda x: x[\"meta\"][\"episode_meta\"][\"vehicle\"] == query.vehicle\n",
    "            and x[\"meta\"][\"episode_meta\"][\"driver\"] == query.driver\n",
    "            and (\n",
    "                pd.Timestamp(query.episodestart_start)\n",
    "                .tz_convert(tz='UTC')\n",
    "                .tz_localize(None)\n",
    "                < pd.to_datetime(x[\"meta\"][\"episode_meta\"][\"episodestart\"], unit=\"us\")\n",
    "                < pd.Timestamp(query.episodestart_end)\n",
    "                .tz_convert(tz='UTC')\n",
    "                .tz_localize(None)\n",
    "            )\n",
    "        )  # do timestamps from avro need conversion? x[\"meta\"][\"episode_meta\"][\"episodestart\"]\n",
    "        old_cnt = self.cnt\n",
    "        self.cnt = self._count(self.query)\n",
    "        self.logger.info(\n",
    "            f\"Avro pool decreases in {old_cnt-self.cnt} episosdes.\",\n",
    "            extra=self.dict_logger,\n",
    "        )\n",
    "\n",
    "    def sample(\n",
    "        self, size: int = 4, *, query: Optional[PoolQuery] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Sample a batch of episodes from Apache avro pool.\n",
    "        return the DataFrame with all episodes\n",
    "        downstream can use pandas DataFrame unique() for index to extract single episodes.\n",
    "        since return is a dataframe, downstream can use pandas dataframe unique() for index to get unique episodes.\n",
    "        Therefore, decoding to DataFrame have to be done in avro pool\n",
    "        \"\"\"\n",
    "\n",
    "        if query == self.query:\n",
    "            cnt = self.cnt\n",
    "        else:\n",
    "            cnt = self._count(query)\n",
    "\n",
    "        queried = self.get_query(query)\n",
    "\n",
    "        if cnt >= size:\n",
    "            sampled = db.random.sample(\n",
    "                population=queried, k=size, split_every=8\n",
    "            )  # todo: split_every=8 is the default number, to be investigated\n",
    "        else:\n",
    "            sampled = db.random.choices(population=queried, k=size, split_every=8)\n",
    "\n",
    "        # query.timestamp_start.tzinfo is Optional[tzinfo], avro_ep_decoding() requires tzinfo as optional\n",
    "        df_episodes = avro_ep_decoding(\n",
    "            sampled.compute(),  # bag to nested lists\n",
    "            tz_info=query.episodestart_start.tzinfo,  # type: ignore\n",
    "        )\n",
    "\n",
    "        return df_episodes\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (record for record in self.dbg.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1bfc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
