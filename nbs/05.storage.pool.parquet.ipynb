{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde508acb1e71e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269ff87995cd5b",
   "metadata": {},
   "source": [
    "# Parquet\n",
    "\n",
    "> ParquetPool class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1fb655411dde66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp storage.pool.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207a42519afffc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import dask.dataframe as dd  # type: ignore\n",
    "import pandas as pd  # type: ignore\n",
    "import pyarrow as pa  # type: ignore\n",
    "import pyarrow.parquet as pq  # type: ignore\n",
    "from dacite import from_dict  # type: ignore\n",
    "from dask.diagnostics import ProgressBar  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ad113abcfd5639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# The following three imports is a nbdev workaround, since ParquetPool is a derived class of DaskPool, nbdev seems to not import it\n",
    "from configparser import ConfigParser\n",
    "from pathlib import Path\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f156c922a6e51446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from data_io_nbdev.storage.pool.dask import DaskPool\n",
    "from data_io_nbdev.data.core import (\n",
    "    ObservationMeta,\n",
    "    PoolQuery,\n",
    "    veos_lifetime_end_date,\n",
    "    veos_lifetime_start_date,\n",
    ")\n",
    "from data_io_nbdev.data.location import locations_by_abbr\n",
    "from data_io_nbdev.data.external.pandas_utils import encode_dataframe_from_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f7ecce903ee9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass(kw_only=True)\n",
    "class ParquetPool(  # type: ignore\n",
    "    DaskPool\n",
    "):  # new feature of python 3.10, not an error. IDE fall behind\n",
    "    \"\"\"\n",
    "    The pool class for storing and retrieving records in Apache Arrow parquet files.\n",
    "\n",
    "    It uses Pandas backend for Parquet, PyArrow Parquet interface for meta data storage, and Dask DataFrame for data processing.\n",
    "    meta information is stored in parquet metadata (in footer of parquet file).\n",
    "\n",
    "    Sample random observation quadruples will need some care to reassure the randomness.\n",
    "    Here we apply dask DataFrame sample method. We use Dask Delayed to parallelize the data processing like sampling\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "        pl_path: `Path` to the parquet file folder\n",
    "        meta: meta information of the pool\n",
    "        query: `PoolQuery` object to the pool\n",
    "        cnt: number of records in the pool\n",
    "        ddf: dask DataFrame object\n",
    "    \"\"\"\n",
    "\n",
    "    ddf: Optional[\n",
    "        dd.DataFrame\n",
    "    ] = None  # dd.from_pandas(pd.DataFrame(), npartitions=1)  # dask DataFrame\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"load parquet pool from parquet files in folder specified by the recipe and validate meta information\"\"\"\n",
    "        self.logger = self.logger.getChild(\"parquet pool\")\n",
    "        self.dict_logger = self.dict_logger\n",
    "        super().__post_init__()\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"{{'header': 'Parquet pool stored', \"\n",
    "            f\"'path': '{self.pl_path}', \"\n",
    "            f\"'coll_type' : '{self.recipe['DEFAULT']['coll_type']}'}}\",\n",
    "            extra=self.dict_logger,\n",
    "        )\n",
    "        self.load()\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"load RECORD arrays from parquet files in folder specified by the recipe\"\"\"\n",
    "        try:\n",
    "            with ProgressBar():\n",
    "                self.ddf = dd.read_parquet(\n",
    "                    str(self.pl_path),  # Path to str conversion\n",
    "                    engine=\"pyarrow\",\n",
    "                    compression=\"snappy\",\n",
    "                    ignore_metadata_file=False,\n",
    "                    split_row_groups=\"infer\"\n",
    "                    # infer_division=True,\n",
    "                )\n",
    "                # parquet file which is partitioned by a timestamp was converted to category,\n",
    "                # when loaded to dask dataframe\n",
    "\n",
    "        except (FileNotFoundError, ValueError) as e:\n",
    "            self.logger.info(\n",
    "                f'Data folder ({self.recipe[\"DEFAULT\"][\"data_folder\"]}) is empty! parquet files not found: {e} ...'\n",
    "            )\n",
    "            self.logger.info(\n",
    "                f'Create data folder ({self.recipe[\"DEFAULT\"][\"data_folder\"]}) for Apache Arrow parquet files!'\n",
    "            )\n",
    "            self.pl_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            self.cnt = 0\n",
    "\n",
    "            return\n",
    "            # # find and select the target partition with vehicle id and driver id in the Query\n",
    "            # self.ddf_list = []\n",
    "            # for n in range(self.ddf.npartitions):\n",
    "            #     self.ddf_list.append(self.ddf.get_partition(n))\n",
    "            # for i, ddf in enumerate(self.ddf_list):\n",
    "            #     # if dfs_episode_new['vehicle']\n",
    "            #     print(f\"{i}\")\n",
    "            #     # df = ddf.compute()\n",
    "            #     # ddf.columns\n",
    "            #     # ddf.dtypes\n",
    "            #     vehicle = ddf['vehicle__'].compute()[0]\n",
    "            #     driver = ddf['driver__'].compute()[0]\n",
    "            #     # df_episode_new = df_episode_new.append(ddf)\n",
    "            #     if self.query['vehicle'] == vehicle and self.query['driver'] == driver:\n",
    "            #         bHit = True\n",
    "            #         print(\n",
    "            #             f' hit {i}{\"st\" if i == 1 else \"nd\" if i == 2 else \"rd\" if i == 3 else \"th\"} partition'\n",
    "            #         )\n",
    "            #         # k = i\n",
    "            #         self.ddf_list.append(i)\n",
    "            #\n",
    "            #         # ddf_db_list[i] = dd.concat([ddf, df], axis=0)\n",
    "            #         # append ddf to dfs_episode_new\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Loading Parquet error: {e}\", extra=self.dict_logger)\n",
    "            raise e\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"{{'header': 'Loading dataframe from parquet files.',  \"\n",
    "            f\"'path': '{self.pl_path}'}}\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            # load meta information from parquet metadata and compare with input metadata\n",
    "            table_meta = pq.read_metadata(self.pl_path / \"_common_metadata\")\n",
    "        except FileNotFoundError as e:\n",
    "            self.logger.warning(\n",
    "                f\"{{'header': 'eos meta data not found in parquet file', \"\n",
    "                f\"'path': '{self.pl_path}'}}\",\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "            raise e\n",
    "        pq_meta_info = json.loads(\n",
    "            (table_meta.metadata[b\"eos\"]).decode().replace(\"'\", '\"')\n",
    "        )\n",
    "        pq_meta_info[\"site\"] = locations_by_abbr[pq_meta_info[\"site\"][\"abbr\"]]\n",
    "        meta_from_pq = ObservationMeta(**pq_meta_info)\n",
    "        for key, val in pq_meta_info.items():\n",
    "            self.logger.info(\n",
    "                f\"{{'header': 'eos meta data', \" f\"'{key}': '{val}'}}\",\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "        assert self.meta.have_same_meta(\n",
    "            meta_from_pq\n",
    "        ), f\"meta information in parquet file doesn't match with input meta information!\"\n",
    "\n",
    "        self.cnt = self._count(self.query)\n",
    "        # TODO if different, raise warning and update meta information in parquet file\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"close the pool\"\"\"\n",
    "        self.logger.info(\n",
    "            f\"Nothing to be done for dask parquet pool!\",  # neither arrow parquet nor avro need cleaning up.\n",
    "            extra=self.dict_logger,\n",
    "        )\n",
    "\n",
    "    def store(self, episode: pd.DataFrame) -> None:\n",
    "        \"\"\"Deposit an episode with all records in every time step into arrow parquet.\"\"\"\n",
    "\n",
    "        # reset index level vehicle, driver, episodestart to columns, so that the only index is timestamp\n",
    "        episode_flat = episode.reset_index(level=[\"vehicle\", \"driver\", \"episodestart\"])\n",
    "        # convert vehicle and driver to category\n",
    "        # episode_flat[\"vehicle\"] = episode_flat[\"vehicle\"].astype(\"category\")\n",
    "        # episode_flat[\"driver\"] = episode_flat[\"driver\"].astype(\"category\")\n",
    "        # episode_flat[\"episodestart\"] = episode_flat[\"episodestart\"].astype(\n",
    "        #     \"datetime64[ns]\")\n",
    "\n",
    "        # Convert Input DataFrame to flat-indexed DataFrame both in rows and columns\n",
    "        # encoding MultiIndex columns index to string\n",
    "        episode_flat.columns = pd.Index(\n",
    "            [\n",
    "                f\"{x[0]}_{x[1]}_{x[2]}\"\n",
    "                if (x[1] != \"\" and x[2] != \"\")\n",
    "                else f\"{x[0]}_{x[1]}_\"\n",
    "                if (x[1] != \"\" and x[2] == \"\")\n",
    "                else f\"{x[0]}__{x[2]}\"  # !!! dunder!!!\n",
    "                if (x[2] != \"\")\n",
    "                else f\"{x[0]}__\"\n",
    "                for x in episode_flat.columns.to_flat_index()\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # episode_flat.set_index('timestamp__', inplace=True)\n",
    "        # self.logger.info(f\"{{'header': 'episode_flat index', \"\n",
    "        #                 f\"\\'ddf.index\\': {episode_flat.index}}}\",\n",
    "        #                 extra=self.dict_logger)\n",
    "        # episode_flat[\"episodestart__\"] = episode_flat[\"episodestart__\"].astype(\"datetime64[ns]\")\n",
    "\n",
    "        self.logger.info(f\"deposit one episode in Parquet\", extra=self.dict_logger)\n",
    "        try:\n",
    "            episode_flat.to_parquet(\n",
    "                self.pl_path,\n",
    "                engine=\"pyarrow\",\n",
    "                compression=\"snappy\",\n",
    "                index=True,\n",
    "                partition_cols=[\"vehicle__\", \"driver__\"],\n",
    "            )\n",
    "            #         write_metadata_file=True,  # write metadata file, default is True\n",
    "            #         custom_metadata=self.input_metadata,  # write input meta information to parquet metadata\n",
    "            #         overwrite=False,  # if not empty, not overwrite and append\n",
    "            #         allow_truncated_timestamps=True,  # allow Timestamp to be truncated from ns to ms\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Writing Parquet error: {e}\", extra=self.dict_logger)\n",
    "            raise e\n",
    "\n",
    "        # if the first parquet file, generate from pyarrow Table and store _common_metadata\n",
    "        if self.ddf is None:\n",
    "            table = pa.Table.from_pandas(df=episode_flat)\n",
    "            input_metadata = {\"eos\": str(self.meta.model_dump()).replace(\"'\", '\"')}\n",
    "            schema = table.schema.with_metadata(input_metadata)\n",
    "            pq.write_metadata(schema, str(self.pl_path / \"_common_metadata\"))\n",
    "\n",
    "        try:\n",
    "            with ProgressBar():\n",
    "                self.ddf = dd.read_parquet(\n",
    "                    str(self.pl_path),  # Path to str conversion\n",
    "                    engine=\"pyarrow\",\n",
    "                    compression=\"snappy\",\n",
    "                    ignore_metadata_file=False,\n",
    "                    split_row_groups=\"infer\"\n",
    "                    # infer_division=True,\n",
    "                )\n",
    "                # parquet file which is partitioned by a timestamp was converted to category,\n",
    "                # when loaded to dask dataframe\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Loading Parquet error: {e}\", extra=self.dict_logger)\n",
    "            raise e\n",
    "\n",
    "        last_cnt = self.cnt\n",
    "        # self.cnt = self._count(self.query)\n",
    "        self.cnt = last_cnt + len(episode)\n",
    "        self.logger.info(f\"Pool size: {self.cnt} records.\", extra=self.dict_logger)\n",
    "\n",
    "    def delete(self, idx) -> None:\n",
    "        \"\"\"\n",
    "        Delete a record by item id.\n",
    "\n",
    "        not implemented for arrow pool\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def delete_episode(self, query: PoolQuery) -> None:\n",
    "        \"\"\"\n",
    "        delete records specified by `PoolQuery`\n",
    "\n",
    "        Caution:\n",
    "        TODO wrong implementation, since Dask does not support drop rows for the moment\n",
    "        TODO Integrate PYSpark and save to Apache Delta Lake\n",
    "        So now Arrow pool does not support delete episode or rows in general\n",
    "        but you can use filter and groupby!!\n",
    "\n",
    "\n",
    "        Expected:\n",
    "        Delete all records in an episode by PoolQuery with\n",
    "            - vehicle\n",
    "            - driver\n",
    "            - episodestart_start\n",
    "            - episodestart_end\n",
    "\n",
    "        \"\"\"\n",
    "        res = self.get_query(query)\n",
    "        old_cnt = self.cnt\n",
    "        self.ddf.drop(res.index, inplace=True)\n",
    "        self.cnt = self._count(self.query)\n",
    "\n",
    "        try:\n",
    "            input_metadata = {\n",
    "                b\"eos\": str(self.meta.model_dump()).replace(\"'\", '\"').encode()\n",
    "            }\n",
    "            with ProgressBar():\n",
    "                self.ddf.to_parquet(\n",
    "                    self.pl_path,\n",
    "                    engine=\"pyarrow\",\n",
    "                    compression=\"snappy\",\n",
    "                    partition_on=[\n",
    "                        \"vehicle__\",\n",
    "                        \"driver__\",\n",
    "                    ],\n",
    "                    write_index=True,  # write index to parquet, default is True\n",
    "                    custom_metadata=input_metadata,  # write input meta information to parquet metadata\n",
    "                    append=False,\n",
    "                    overwrite=True,\n",
    "                    # allow_truncated_timestamps=True,\n",
    "                )\n",
    "        except Exception as e:\n",
    "            self.logger.warning(\n",
    "                f\"Writing Parquet error while deleting episodes: {e}\",\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "        else:\n",
    "            self.logger.info(\n",
    "                f\"deleted {old_cnt - self.cnt} records in Parquet\",\n",
    "                extra=self.dict_logger,\n",
    "            )\n",
    "        finally:\n",
    "            self.logger.info(f\"The parquet pool contains now {self.cnt} records.\")\n",
    "\n",
    "    def get_query(self, query: Optional[PoolQuery] = None) -> Optional[dd.DataFrame]:\n",
    "        \"\"\"\n",
    "        get query from dask dataframe parquet storage\n",
    "\n",
    "        Arg:\n",
    "            query: `PoolQuery` object to the pool\n",
    "\n",
    "        Return:\n",
    "\n",
    "            A Dask DataFrame with all records in the query time range\n",
    "        \"\"\"\n",
    "        assert query is not None, f\"query is None!\"\n",
    "\n",
    "        if query.episodestart_start is None:\n",
    "            query.episodestart_start = veos_lifetime_start_date.to_pydatetime()\n",
    "\n",
    "        if query.episodestart_end is None:\n",
    "            query.episodestart_end = veos_lifetime_end_date.to_pydatetime()\n",
    "\n",
    "        res = self.ddf.loc[\n",
    "            (\n",
    "                self.ddf[\"vehicle__\"] == query.vehicle\n",
    "            )  # comparing category type and str seems to work!\n",
    "            & (self.ddf[\"driver__\"] == query.driver)\n",
    "            & (\n",
    "                self.ddf[\"episodestart__\"] >= pd.Timestamp(query.episodestart_start)\n",
    "            )  # .tz_convert('UTC'))  #.tz_convert(None))\n",
    "            & (\n",
    "                self.ddf[\"episodestart__\"] <= pd.Timestamp(query.episodestart_end)\n",
    "            )  # .tz_convert('UTC'))  #.tz_convert(None))\n",
    "        ].loc[\n",
    "            pd.Timestamp(query.timestamp_start) : pd.Timestamp(query.timestamp_end)  # type: ignore\n",
    "        ]\n",
    "        assert isinstance(res, dd.DataFrame), f\"res is not a dask DataFrame!\"\n",
    "        return res\n",
    "\n",
    "    def sample(self, size: int = 4, *, query: PoolQuery) -> pd.DataFrame:  # type: ignore\n",
    "        \"\"\"\n",
    "        Sample a batch of records from arrow parquet pool with fractional sampling.\n",
    "\n",
    "        Args:\n",
    "            size: number of records in the batch\n",
    "            query: `PoolQuery` object to the pool\n",
    "\n",
    "        Return:\n",
    "            A Pandas DataFrame with all records in the query range\n",
    "        \"\"\"\n",
    "        if query == self.query:\n",
    "            cnt = self.cnt\n",
    "        else:\n",
    "            cnt = self._count(query)\n",
    "        assert (\n",
    "            0.0 < (size / cnt) <= 1.0\n",
    "        ), f\"sampling a dask dataframe must be fractional!\"\n",
    "\n",
    "        res = self.get_query(query)\n",
    "        if size < 0.1 * cnt:\n",
    "            rough_pick = res.sample(frac=0.15).compute()\n",
    "            flat_batch = rough_pick.sample(n=size, replace=False, axis=0)\n",
    "        else:\n",
    "            self.logger.warning(\n",
    "                f\"sample size {size} is not smaller than 10% of total records {cnt}!\"\n",
    "            )\n",
    "            rough_pick = res.compute()\n",
    "            flat_batch = rough_pick.sample(n=size, replace=False, axis=0)\n",
    "\n",
    "        assert len(flat_batch) == size, f\"batch size is not {size}!\"\n",
    "        assert isinstance(flat_batch, pd.DataFrame), f\"batch is not a pandas DataFrame!\"\n",
    "        batch = encode_dataframe_from_parquet(flat_batch)\n",
    "        return batch\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (record for record in self.ddf.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954f077af55fd532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f148066fda97c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_doc(ParquetPool.__post_init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f01929556ef58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ParquetPool.find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b5f78077dcb270",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ParquetPool.get_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8284424abd9aace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ParquetPool.sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a96f418ea72312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ParquetPool.store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1067217d1977ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ParquetPool.close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666998ba1cf21d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(ParquetPool.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74724b5de0f85623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
