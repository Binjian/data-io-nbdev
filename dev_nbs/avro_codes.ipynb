{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c26be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from eos import proj_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from fastavro import writer, reader, parse_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf6b16c5358ec90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb3c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from eos import proj_root\n",
    "\n",
    "os.chdir(proj_root / \"data\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea55958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from functools import reduce\n",
    "\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ordered_set import OrderedSet\n",
    "from eos.data_io.config import drivers, trucks_by_id, drivers_by_id\n",
    "from datetime import datetime\n",
    "\n",
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "ts = pd.Timestamp.now(tz=\"Asia/Shanghai\")\n",
    "\n",
    "\n",
    "ts_ind = ts + pd.to_timedelta(np.arange(0, 4 * 20, 20), \"ms\")\n",
    "ss = np.arange(12)\n",
    "a1 = ss[:4]\n",
    "a2 = ss[4:8]\n",
    "a3 = ss[8:]\n",
    "df_ss = pd.DataFrame(\n",
    "    {\"timestep\": ts_ind, \"velocity\": a1, \"thrust\": a2, \"brake\": a3}\n",
    ")  # .set_index('timestep')\n",
    "df_ss.columns.name = \"qtuple\"\n",
    "\n",
    "state = df_ss.stack().swaplevel(0, 1)\n",
    "state.name = \"state\"\n",
    "state.index.names = [\"rows\", \"idx\"]\n",
    "state.sort_index(inplace=True)\n",
    "\n",
    "a = len(ss) + np.arange(15)\n",
    "speed_ser = pd.Series(np.linspace(40, 60, 3), name=\"speed\")\n",
    "row_array = a.reshape(3, 5).transpose()\n",
    "rows_df = pd.DataFrame(row_array)\n",
    "rows_df.columns = [f\"r{i}\" for i in np.arange(3)]\n",
    "\n",
    "ts_ind = ts + pd.to_timedelta(np.arange(5 * 20, 8 * 20, 20), \"ms\")\n",
    "ts_ser = pd.Series(ts_ind, name=\"timestep\")\n",
    "throttle_ser = pd.Series(np.linspace(0, 1.0, 5), name=\"throttle\")\n",
    "# throttle_ser\n",
    "dfs = [rows_df, ts_ser, speed_ser, throttle_ser]\n",
    "action = (\n",
    "    reduce(\n",
    "        lambda left, right: pd.merge(\n",
    "            left, right, how=\"outer\", left_index=True, right_index=True\n",
    "        ),\n",
    "        dfs,\n",
    "    )\n",
    "    .stack()\n",
    "    .swaplevel(0, 1)\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "action.name = \"action\"\n",
    "action.index.names = [\"rows\", \"idx\"]\n",
    "\n",
    "reward = (\n",
    "    pd.DataFrame({\"work\": len(ss) + len(a), \"timestep\": ts_ind[0]}, index=[0])\n",
    "    .stack()\n",
    "    .swaplevel(0, 1)\n",
    "    .sort_index()\n",
    ")\n",
    "reward.index.names = [\"rows\", \"idx\"]\n",
    "reward.name = \"reward\"\n",
    "ts_ind = ts + pd.to_timedelta(5, \"s\") + pd.to_timedelta(np.arange(0, 4 * 20, 20), \"ms\")\n",
    "ss = (\n",
    "    np.arange(12) + len(ss) + len(a) + len(reward) - 1\n",
    ")  # exclude the timestamp in reward\n",
    "a1 = ss[:4]\n",
    "a2 = ss[4:8]\n",
    "a3 = ss[8:]\n",
    "\n",
    "nstate = (\n",
    "    pd.DataFrame({\"timestep\": ts_ind, \"velocity\": a1, \"thrust\": a2, \"brake\": a3})\n",
    "    .stack()\n",
    "    .swaplevel(0, 1)\n",
    "    .sort_index()\n",
    ")\n",
    "nstate.name = \"nstate\"\n",
    "nstate.index.names = [\"rows\", \"idx\"]\n",
    "\n",
    "timestamp = pd.Series([ts], name=\"timestamp\")\n",
    "timestamp.index = pd.MultiIndex.from_product(\n",
    "    [timestamp.index, [0]], names=[\"rows\", \"idx\"]\n",
    ")\n",
    "\n",
    "timestamp_index = (timestamp.name, \"\", 0)\n",
    "state_index = [(state.name, *i) for i in state.index]\n",
    "reward_index = [(reward.name, *i) for i in reward.index]\n",
    "action_index = [(action.name, *i) for i in action.index]\n",
    "nstate_index = [(nstate.name, *i) for i in nstate.index]\n",
    "multiindex = pd.MultiIndex.from_tuples(\n",
    "    [timestamp_index, *state_index, *action_index, *reward_index, *nstate_index]\n",
    ")\n",
    "observation_list = [timestamp, state, action, reward, nstate]\n",
    "observation = pd.concat(observation_list)\n",
    "observation.index = multiindex\n",
    "\n",
    "observation0 = observation.copy()\n",
    "observation0.loc[\"timestamp\", \"\", 0] = ts + pd.Timedelta(1, \"h\")\n",
    "observation1 = observation.copy()\n",
    "observation1.loc[\"timestamp\", \"\", 0] = ts + pd.Timedelta(2, \"h\")\n",
    "observation2 = observation.copy()\n",
    "observation2.loc[\"timestamp\", \"\", 0] = ts + pd.Timedelta(3, \"h\")\n",
    "observation3 = observation.copy()\n",
    "observation3.loc[\"timestamp\", \"\", 0] = ts + pd.Timedelta(4, \"h\")\n",
    "observation4 = observation.copy()\n",
    "observation4.loc[\"timestamp\", \"\", 0] = ts + pd.Timedelta(5, \"h\")\n",
    "observation_list = [\n",
    "    observation0,\n",
    "    observation1,\n",
    "    observation2,\n",
    "    observation3,\n",
    "    observation4,\n",
    "]\n",
    "\n",
    "dfs_epi = pd.concat(observation_list, axis=1).transpose()\n",
    "dfs_epi.columns.names = [\"qtuple\", \"rows\", \"idx\"]\n",
    "\n",
    "dfs_episode = dfs_epi.copy()\n",
    "dfs_episode.set_index((\"timestamp\", \"\", 0), inplace=True)\n",
    "dfs_episode.sort_index(axis=1, inplace=True)\n",
    "dfs_episode.index.name = \"timestamp\"\n",
    "\n",
    "state_cols_float = [(\"state\", col) for col in [\"brake\", \"thrust\", \"velocity\"]]\n",
    "action_cols_float = [(\"action\", col) for col in [\"r0\", \"r1\", \"r2\", \"speed\", \"throttle\"]]\n",
    "reward_cols_float = [(\"reward\", \"work\")]\n",
    "nstate_cols_float = [(\"nstate\", col) for col in [\"brake\", \"thrust\", \"velocity\"]]\n",
    "for col in action_cols_float + state_cols_float + reward_cols_float + nstate_cols_float:\n",
    "    dfs_episode[col[0], col[1]] = dfs_episode[col[0], col[1]].astype(\n",
    "        \"float\"\n",
    "    )  # float16 not allowed in parquet\n",
    "dfs_episode = pd.concat(\n",
    "    [dfs_episode], keys=[drivers_by_id[\"wang-cheng\"].pid], names=[\"driver\"]\n",
    ")\n",
    "dfs_episode = pd.concat(\n",
    "    [dfs_episode], keys=[trucks_by_id[\"VB7\"].vid], names=[\"vehicle\"]\n",
    ")\n",
    "dfs_episode = pd.concat([dfs_episode], keys=[ts], names=[\"episodestart\"])\n",
    "dfs_episode = dfs_episode.swaplevel(1, 0, axis=0)\n",
    "dfs_episode = dfs_episode.swaplevel(1, 2, axis=0)\n",
    "dfs_episode.sort_index(inplace=True)\n",
    "dfs_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bcc8498d9554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_new = pd.to_datetime((datetime.now()))\n",
    "episodestart = ts_new - pd.Timedelta(2, \"d\")\n",
    "\n",
    "dfs_episode0 = dfs_episode.copy()\n",
    "dfs_episode0.index = dfs_episode0.index.set_levels([episodestart], level=\"episodestart\")\n",
    "dfs_episode0.index = dfs_episode0.index.set_levels(\n",
    "    [[trucks_by_id[\"VB7\"].vid], [drivers_by_id[\"zheng-longfei\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "ts_index = dfs_episode0.index.unique(level=\"timestamp\")\n",
    "idx_num = len(ts_index)\n",
    "drop_num = np.random.randint(low=1, high=idx_num - 1)\n",
    "ts_index_to_drop = np.random.choice(ts_index, drop_num, replace=False)\n",
    "dfs_episode0 = dfs_episode0.drop(index=ts_index_to_drop, level=\"timestamp\")\n",
    "# srs_episode0 = dfs_episode0.stack(level=['qtuple', 'rows', 'idx'])\n",
    "\n",
    "dfs_episode1 = dfs_episode.copy()\n",
    "dfs_episode1.index = dfs_episode1.index.set_levels(\n",
    "    [episodestart - pd.Timedelta(3, \"d\")], level=\"episodestart\"\n",
    ")\n",
    "dfs_episode1.index = dfs_episode1.index.set_levels(\n",
    "    [[trucks_by_id[\"MP73\"].vid], [drivers_by_id[\"wang-cheng\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "ts_index = dfs_episode1.index.unique(level=\"timestamp\")\n",
    "idx_num = len(ts_index)\n",
    "drop_num = np.random.randint(low=1, high=idx_num - 1)\n",
    "ts_index_to_drop = np.random.choice(ts_index, drop_num, replace=False)\n",
    "dfs_episode1 = dfs_episode1.drop(index=ts_index_to_drop, level=\"timestamp\")\n",
    "# srs_episode1 = dfs_episode1.stack(level=['qtuple', 'rows', 'idx'])\n",
    "\n",
    "dfs_episode2 = dfs_episode.copy()\n",
    "dfs_episode2.index = dfs_episode2.index.set_levels(\n",
    "    [episodestart - pd.Timedelta(4, \"d\")], level=\"episodestart\"\n",
    ")\n",
    "dfs_episode2.index = dfs_episode2.index.set_levels(\n",
    "    [[trucks_by_id[\"VB7\"].vid], [drivers_by_id[\"wang-cheng\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "ts_index = dfs_episode2.index.unique(level=\"timestamp\")\n",
    "idx_num = len(ts_index)\n",
    "drop_num = np.random.randint(low=1, high=idx_num - 1)\n",
    "ts_index_to_drop = np.random.choice(ts_index, drop_num, replace=False)\n",
    "dfs_episode2 = dfs_episode2.drop(index=ts_index_to_drop, level=\"timestamp\")\n",
    "# srs_episode2 = dfs_episode2.stack(level=['qtuple', 'rows', 'idx'])\n",
    "\n",
    "dfs_episode3 = dfs_episode.copy()\n",
    "dfs_episode3.index = dfs_episode3.index.set_levels(\n",
    "    [episodestart - pd.Timedelta(5, \"d\")], level=\"episodestart\"\n",
    ")\n",
    "dfs_episode3.index = dfs_episode3.index.set_levels(\n",
    "    [[trucks_by_id[\"MP73\"].vid], [drivers_by_id[\"zheng-longfei\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "ts_index = dfs_episode3.index.unique(level=\"timestamp\")\n",
    "idx_num = len(ts_index)\n",
    "drop_num = np.random.randint(low=1, high=idx_num - 1)\n",
    "ts_index_to_drop = np.random.choice(ts_index, drop_num, replace=False)\n",
    "dfs_episode3 = dfs_episode3.drop(index=ts_index_to_drop, level=\"timestamp\")\n",
    "# srs_episode3 = dfs_episode3.stack(level=['qtuple', 'rows', 'idx'])\n",
    "from functools import reduce\n",
    "\n",
    "episodes = [dfs_episode, dfs_episode0, dfs_episode1, dfs_episode2, dfs_episode3]\n",
    "try:\n",
    "    dfs_episode_all = reduce(\n",
    "        lambda left, right,: pd.concat([left, right], axis=0), episodes\n",
    "    )\n",
    "    dfs_episode_all.sort_index(inplace=True)\n",
    "    dfs_episode_all = dfs_episode_all[[\"state\", \"action\", \"reward\", \"nstate\"]]\n",
    "    dfs_episode_all\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "# dfs_episode_all.sort_index(inplace=True)\n",
    "# dfs_episode_all = dfs_episode_all.swaplevel(1, 0, axis=0)\n",
    "# dfs_episode_all = dfs_episode_all.swaplevel(1, 2, axis=0)\n",
    "# dfs_episode_all.index\n",
    "# dfs_episode_all.columns#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6685bc4d6efeb096",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode\n",
    "# episodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d35874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "indices = dfs_episode.index\n",
    "# ep_start = indices.get_level_values(level='episodestart')[0]\n",
    "indices_dict = [\n",
    "    {indices.names[i]: level for i, level in enumerate(levels)} for levels in indices\n",
    "]\n",
    "episode_meta = indices_dict[0].copy()\n",
    "episode_meta[\"episodestart\"] = episode_meta[\"episodestart\"].timestamp() * 1e6\n",
    "episode_meta[\"timestamp\"] = episode_meta[\"timestamp\"].timestamp() * 1e6\n",
    "try:\n",
    "    episode_meta.pop(\"timestamp\")\n",
    "except KeyError:\n",
    "    print(f\"Key 'timestamp' not found\")\n",
    "# episode_meta\n",
    "\n",
    "# episodes_indices\n",
    "episodes_indices_dict = [\n",
    "    [{indices.names[i]: level for i, level in enumerate(levels)} for levels in df.index]\n",
    "    for df in episodes\n",
    "]\n",
    "for indices in episodes_indices_dict:\n",
    "    indices[0].pop(\"timestamp\")\n",
    "\n",
    "episodes_meta_dict = [indices[0] for indices in episodes_indices_dict]\n",
    "\n",
    "for ep_mt in episodes_meta_dict:\n",
    "    ep_mt[\"episodestart\"] = ep_mt[\"episodestart\"].timestamp() * 1e6\n",
    "episodes_meta_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf6cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eos.data_io.eos_struct import (\n",
    "    ObservationMeta,\n",
    "    StateSpecs,\n",
    "    StateUnitCodes,\n",
    "    ActionSpecs,\n",
    "    RewardSpecs,\n",
    ")\n",
    "from eos.data_io.utils.eos_pandas import ep_nest, df_to_ep_nested_dict, avro_ep_encoding\n",
    "from eos.data_io.config import trucks_by_id\n",
    "\n",
    "# from pydantic import schema_json_of, schema_of\n",
    "\n",
    "observation_meta = ObservationMeta(\n",
    "    state_specs=StateSpecs(\n",
    "        state_unit_codes=StateUnitCodes(),\n",
    "        state_number=3,\n",
    "        unit_number_per_state=trucks_by_id[\"VB7\"].tbox_unit_number,  # 4\n",
    "        unit_duration=trucks_by_id[\"VB7\"].tbox_unit_duration,  # 1s\n",
    "        frequency=trucks_by_id[\"VB7\"].tbox_signal_frequency,  # 50 hz\n",
    "    ),\n",
    "    action_specs=ActionSpecs(\n",
    "        action_unit_code=\"nm\",\n",
    "        action_row_number=trucks_by_id[\"VB7\"].torque_table_row_num_flash,\n",
    "        action_column_number=len(trucks_by_id[\"VB7\"].pedal_scale),\n",
    "    ),\n",
    "    reward_specs=RewardSpecs(reward_unit_code=\"wh\", reward_number=1),\n",
    "    site=trucks_by_id[\"VB7\"].site,\n",
    ")\n",
    "\n",
    "\n",
    "dict_nested = avro_ep_encoding(dfs_episode)\n",
    "dict_nested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af806b55d9b963",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ep = {\n",
    "    \"episodestart\": episode_meta[\"episodestart\"],\n",
    "    \"meta\": {\n",
    "        \"episode_meta\": episode_meta,\n",
    "        \"observation_meta\": observation_meta.model_dump(),\n",
    "    },\n",
    "    \"sequence\": dict_nested,\n",
    "}\n",
    "dict_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2abdda2260db324",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_dict_nested = [avro_ep_encoding(ep) for ep in episodes]\n",
    "episodes_dict_nested[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc40e701f9e9ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_nested_states = [[step[\"state\"] for step in ep] for ep in episodes_dict_nested]\n",
    "dict_nested_nstates = [[step[\"nstate\"] for step in ep] for ep in episodes_dict_nested]\n",
    "dict_nested_rewards = [[step[\"reward\"] for step in ep] for ep in episodes_dict_nested]\n",
    "dict_nested_actions = [[step[\"action\"] for step in ep] for ep in episodes_dict_nested]\n",
    "dict_nested_timestamps = [\n",
    "    [step[\"timestamp\"] for step in ep] for ep in episodes_dict_nested\n",
    "]\n",
    "\n",
    "arr_states = [\n",
    "    [{\"ts\": ts, \"state\": state_arr[i]} for i, ts in enumerate(ts_arr)]\n",
    "    for (ts_arr, state_arr) in zip(dict_nested_timestamps, dict_nested_states)\n",
    "]\n",
    "arr_states[0]\n",
    "# dict_nested_timestamps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d962acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastavro\n",
    "import json\n",
    "\n",
    "state_unit_fields_schema = [\n",
    "    {\"name\": \"velocity_unit_code\", \"type\": \"string\"},\n",
    "    {\"name\": \"thrust_unit_code\", \"type\": \"string\"},\n",
    "    {\"name\": \"brake_unit_code\", \"type\": \"string\"},\n",
    "]\n",
    "state_specs_fields_schema = [\n",
    "    {\n",
    "        \"name\": \"state_unit_codes\",\n",
    "        \"type\": {\n",
    "            \"type\": \"record\",\n",
    "            \"name\": \"state_unit_codes_\",\n",
    "            \"fields\": state_unit_fields_schema,\n",
    "        },\n",
    "    },\n",
    "    {\"name\": \"state_number\", \"type\": \"int\"},\n",
    "    {\"name\": \"unit_number_per_state\", \"type\": \"int\"},\n",
    "    {\"name\": \"unit_duration\", \"type\": \"int\"},\n",
    "    {\"name\": \"frequency\", \"type\": \"int\"},\n",
    "]\n",
    "action_specs_fields_schema = [\n",
    "    {\"name\": \"action_unit_code\", \"type\": \"string\"},\n",
    "    {\"name\": \"action_row_number\", \"type\": \"int\"},\n",
    "    {\"name\": \"action_column_number\", \"type\": \"int\"},\n",
    "]\n",
    "reward_specs_fields_schema = [{\"name\": \"reward_unit_code\", \"type\": \"string\"}]\n",
    "\n",
    "episode_meta_fields_schema = [\n",
    "    {\"name\": \"vehicle\", \"type\": \"string\"},\n",
    "    {\"name\": \"driver\", \"type\": \"string\"},\n",
    "    {\"name\": \"episodestart\", \"type\": \"long\", \"logicalType\": \"timestamp-micros\"},\n",
    "]\n",
    "\n",
    "\n",
    "state_fields_schema = [\n",
    "    {\"name\": \"velocity\", \"type\": {\"type\": \"array\", \"items\": \"float\"}},\n",
    "    {\"name\": \"thrust\", \"type\": {\"type\": \"array\", \"items\": \"float\"}},\n",
    "    {\"name\": \"brake\", \"type\": {\"type\": \"array\", \"items\": \"float\"}},\n",
    "    {\n",
    "        \"name\": \"timestep\",\n",
    "        \"type\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"long\", \"logicalType\": \"timestamp-micros\"},\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "torque_table_row_names = [\"r0\", \"r1\", \"r2\"]\n",
    "action_fields_schema = [\n",
    "    {\"name\": r, \"type\": {\"type\": \"array\", \"items\": \"float\"}}\n",
    "    for r in torque_table_row_names\n",
    "]\n",
    "action_fields_schema += [\n",
    "    {\"name\": \"speed\", \"type\": {\"type\": \"array\", \"items\": \"float\"}},\n",
    "    {\"name\": \"throttle\", \"type\": {\"type\": \"array\", \"items\": \"float\"}},\n",
    "    {\n",
    "        \"name\": \"timestep\",\n",
    "        \"type\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"long\", \"logicalType\": \"timestamp-micros\"},\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "reward_fields_schema = [\n",
    "    {\"name\": \"work\", \"type\": {\"type\": \"array\", \"items\": \"float\"}},\n",
    "    {\n",
    "        \"name\": \"timestep\",\n",
    "        \"type\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"long\", \"logicalType\": \"timestamp-micros\"},\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "site_fields_schema = [\n",
    "    {\"name\": \"abbr\", \"type\": \"string\"},\n",
    "    {\"name\": \"name\", \"type\": \"string\"},\n",
    "    {\"name\": \"cname\", \"type\": \"string\"},\n",
    "    {\"name\": \"tz\", \"type\": \"string\"},\n",
    "]\n",
    "episode_array_fields_schema = [\n",
    "    {\n",
    "        \"type\": \"long\",\n",
    "        \"name\": \"timestamp\",\n",
    "        \"logicalType\": \"timestamp-micros\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"state\",\n",
    "        \"type\": {\n",
    "            \"type\": \"record\",\n",
    "            \"name\": \"state_\",\n",
    "            \"fields\": state_fields_schema,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"action\",\n",
    "        \"type\": {\n",
    "            \"type\": \"record\",\n",
    "            \"name\": \"action_\",\n",
    "            \"fields\": action_fields_schema,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"reward\",\n",
    "        \"type\": {\n",
    "            \"type\": \"record\",\n",
    "            \"name\": \"reward_\",\n",
    "            \"fields\": reward_fields_schema,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"nstate\",\n",
    "        \"type\": {\n",
    "            \"type\": \"record\",\n",
    "            \"name\": \"nstate_\",\n",
    "            \"fields\": state_fields_schema,\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "observation_meta_fields_schema = [\n",
    "    {\n",
    "        \"name\": \"state_specs\",\n",
    "        \"type\": {\n",
    "            \"type\": \"record\",\n",
    "            \"name\": \"state_specs_\",\n",
    "            \"fields\": state_specs_fields_schema,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"action_specs\",\n",
    "        \"type\": {\n",
    "            \"type\": \"record\",\n",
    "            \"name\": \"action_specs_\",\n",
    "            \"fields\": action_specs_fields_schema,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"reward_specs\",\n",
    "        \"type\": {\n",
    "            \"type\": \"record\",\n",
    "            \"name\": \"reward_specs_\",\n",
    "            \"fields\": reward_specs_fields_schema,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"site\",\n",
    "        \"type\": {\n",
    "            \"type\": \"record\",\n",
    "            \"name\": \"site_\",\n",
    "            \"fields\": site_fields_schema,\n",
    "        },\n",
    "    },\n",
    "]\n",
    "schema_episode = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"episode\",\n",
    "    \"doc\": \"episode data with a timestamp, meta description and an array of episode steps\",\n",
    "    \"fields\": [\n",
    "        {\"type\": \"long\", \"name\": \"episodestart\", \"logicalType\": \"timestamp-micros\"},\n",
    "        {\n",
    "            \"name\": \"meta\",\n",
    "            \"type\": {\n",
    "                \"type\": \"record\",\n",
    "                \"name\": \"meta_\",\n",
    "                \"fields\": [\n",
    "                    {\n",
    "                        \"name\": \"episode_meta\",\n",
    "                        \"type\": {\n",
    "                            \"type\": \"record\",\n",
    "                            \"name\": \"episode_meta_\",\n",
    "                            \"fields\": episode_meta_fields_schema,\n",
    "                        },\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"observation_meta\",\n",
    "                        \"type\": {\n",
    "                            \"type\": \"record\",\n",
    "                            \"name\": \"observation_meta_\",\n",
    "                            \"fields\": observation_meta_fields_schema,\n",
    "                        },\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"sequence\",\n",
    "            \"type\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"name\": \"step\",  # not used in constructing the episode observation array data\n",
    "                    \"type\": \"record\",\n",
    "                    \"fields\": episode_array_fields_schema,\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "parsed_schema_episode = fastavro.schema.parse_schema(schema_episode)\n",
    "print(json.dumps(schema_episode, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c4e2385b3428f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# records_episodes = [\n",
    "#     {u'episodestart': episodes_indices_dict[0]['episodestart'],\n",
    "#      u'meta': {'episode_meta': episode_meta, 'observation_meta': observation_meta.dict()},\n",
    "#      u'sequence': episodes_dict_nested[0]},\n",
    "#     {u'episodestart': episodes_indices_dict[1]['episodestart'],\n",
    "#      u'meta': {'episode_meta': episode_meta, 'observation_meta': observation_meta.dict()},\n",
    "#      u'sequence': episodes_dict_nested[1]},\n",
    "#     {u'episodestart': episodes_indices_dict[2]['episodestart'],\n",
    "#      u'meta': {'episode_meta': episode_meta, 'observation_meta': observation_meta.dict()},\n",
    "#      u'sequence': episodes_dict_nested[2]},\n",
    "#     {u'episodestart': episodes_indices_dict[3]['episodestart'],\n",
    "#      u'meta': {'episode_meta': episode_meta, 'observation_meta': observation_meta.dict()},\n",
    "#      u'sequence': episodes_dict_nested[3]},\n",
    "#     {u'episodestart': episodes_indices_dict[4]['episodestart'],\n",
    "#      u'meta': {'episode_meta': episode_meta, 'observation_meta': observation_meta.dict()},\n",
    "#      u'sequence': episodes_dict_nested[4]},\n",
    "# ]\n",
    "\n",
    "\n",
    "records_episodes = [\n",
    "    {\n",
    "        \"episodestart\": episodes_meta_dict[i][\"episodestart\"],\n",
    "        \"meta\": {\n",
    "            \"episode_meta\": episodes_meta_dict[i],\n",
    "            \"observation_meta\": observation_meta.model_dump(),\n",
    "        },\n",
    "        \"sequence\": episodes_dict_nested[i],\n",
    "    }\n",
    "    for i in range(4)\n",
    "]\n",
    "\n",
    "records_episodes[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c8cf88308e409",
   "metadata": {},
   "source": [
    "# Write the episodes to a bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a73b7a05e1bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "# os.chdir('bags')\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c222166b61c228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "\n",
    "b_episodes = db.from_sequence(records_episodes)\n",
    "print(b_episodes.npartitions)\n",
    "data = b_episodes.take(4, npartitions=4)\n",
    "len(data)\n",
    "b_episodes.to_avro(\"bag_episodes1.*.avro\", schema=parsed_schema_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28327d8995650baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_episodes_read = db.read_avro(\"bag_episodes1.*.avro\")\n",
    "rec = b_episodes_read.take(4, npartitions=4)\n",
    "len(rec)\n",
    "print(rec[0][\"meta\"][\"episode_meta\"][\"vehicle\"])\n",
    "print(rec[0][\"meta\"][\"episode_meta\"][\"driver\"])\n",
    "print(rec[0][\"sequence\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67951ffa30bbbfc9",
   "metadata": {},
   "source": [
    "# Add an episode to an existing bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496af4eaa4089209",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_episode_to_add = [\n",
    "    {\n",
    "        \"episodestart\": episodes_meta_dict[4][\"episodestart\"],\n",
    "        \"meta\": {\n",
    "            \"episode_meta\": episodes_meta_dict[4],\n",
    "            \"observation_meta\": observation_meta.model_dump(),\n",
    "        },\n",
    "        \"sequence\": episodes_dict_nested[4],\n",
    "    }\n",
    "]\n",
    "# records_episode_to_add\n",
    "\n",
    "b_episodes_new = db.concat([b_episodes_read, db.from_sequence(records_episode_to_add)])\n",
    "print(b_episodes_new.npartitions)\n",
    "b_episodes_new.to_avro(\"bag_episodes.*.avro\", schema=parsed_schema_episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9039a243b8c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_episodes_read = db.read_avro(\"bag_episodes.*.avro\")\n",
    "print(b_episodes_read.npartitions)\n",
    "b_episodes_read.take(5, npartitions=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c2fc61ab50638",
   "metadata": {},
   "outputs": [],
   "source": [
    "an_episode = b_episodes_read.take(2, npartitions=6)\n",
    "type(an_episode[0])\n",
    "len(an_episode)\n",
    "ep_meta = an_episode[0][\"meta\"][\"episode_meta\"]\n",
    "obs_meta = an_episode[0][\"meta\"][\"observation_meta\"]\n",
    "\n",
    "obs_meta1 = an_episode[1][\"meta\"][\"observation_meta\"]\n",
    "for key, val in (ep_meta | obs_meta).items():\n",
    "    print(key, val)\n",
    "\n",
    "obs_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213448aff10963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "{**obs_meta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcb8159c3ce0059",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from eos.data_io.eos_struct.eos_data import ObservationMeta\n",
    "from eos.data_io.eos_struct import locations_by_abbr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db187ebf8bdf3999",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obs_meta1['site']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a244d168cc03ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obs_meta1['site'] = locations_by_abbr[obs_meta1['site']['abbr']]\n",
    "obs_meta1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4db60745c063d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta1 = ObservationMeta(**obs_meta1)\n",
    "meta1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a44ac27b2669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obs_meta['site'] = locations_by_abbr[obs_meta['site']['abbr']]\n",
    "meta = ObservationMeta(**obs_meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f1da9faa69065",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta1.have_same_meta(meta)\n",
    "meta1.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa12924df7394c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_episodes_read = db.read_avro(\"bag_episodes.*.avro\")\n",
    "print(b_episodes_read.npartitions)\n",
    "b_episodes_read.take(5, npartitions=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be763d7b2851808f",
   "metadata": {},
   "source": [
    "# Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50094c665ab78370",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = b_episodes_read.compute()\n",
    "print(f\"length: {len(dicts)}\")\n",
    "for b in dicts:\n",
    "    b[\"episodestart\"] = pd.to_datetime(b[\"episodestart\"], unit=\"us\")\n",
    "    print(\n",
    "        f\"vehicle: {b['meta']['episode_meta']['vehicle']}; \"\n",
    "        f\"driver: {b['meta']['episode_meta']['driver']}; \"\n",
    "        f\"episodestart: {b['episodestart']} \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b2ba22bc74a898",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = b_episodes_read.distinct(\n",
    "    lambda x: (\n",
    "        x[\"meta\"][\"episode_meta\"][\"driver\"],\n",
    "        x[\"meta\"][\"episode_meta\"][\"vehicle\"],\n",
    "        x[\"meta\"][\"episode_meta\"][\"episodestart\"],\n",
    "    )\n",
    ").compute()\n",
    "for b in dicts:\n",
    "    b[\"episodestart\"] = pd.to_datetime(b[\"episodestart\"], unit=\"us\")\n",
    "    print(\n",
    "        f\"vehicle: {b['meta']['episode_meta']['vehicle']}; \"\n",
    "        f\"driver: {b['meta']['episode_meta']['driver']}; \"\n",
    "        f\"episodestart: {b['episodestart']} \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecd6ccef6560e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781ee1dcd636b3bf",
   "metadata": {},
   "source": [
    "# decode nested dicts to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dbb71bf1030e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "anep = dicts[0]\n",
    "anep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43519dbe98d8eb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anep['meta']['episode_meta']\n",
    "# for k,v in anep['sequence']\n",
    "len(anep[\"sequence\"])\n",
    "# anep['sequence'][0]\n",
    "for qtuple, obs in anep[\"sequence\"][0].items():\n",
    "    print(f\"qtuple: {qtuple};\\nobs: {obs}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf602495400052cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_observations = {(meta['vehicle'], meta['driver'], meta['episodestart'],\n",
    "#                       pd.to_datetime(obs['timestamp'],unit='us'), qtuple): value\n",
    "#                         for meta, obs in zip(anep['meta']['episode_meta'],anep['sequence'])\n",
    "#                         for qtuple, value in obs.items() if qtuple != 'timestamp'}\n",
    "\n",
    "dict_observations = [\n",
    "    {\n",
    "        (\n",
    "            anep[\"meta\"][\"episode_meta\"][\"vehicle\"],\n",
    "            anep[\"meta\"][\"episode_meta\"][\"driver\"],\n",
    "            pd.to_datetime(\n",
    "                anep[\"meta\"][\"episode_meta\"][\"episodestart\"], unit=\"us\", utc=True\n",
    "            ).tz_convert(\"Asia/Shanghai\"),\n",
    "            pd.to_datetime(step[\"timestamp\"], unit=\"us\", utc=True).tz_convert(\n",
    "                \"Asia/Shanghai\"\n",
    "            ),\n",
    "            qtuple,\n",
    "            rows,\n",
    "            idx,\n",
    "        ): item\n",
    "        if rows != \"timestep\"\n",
    "        else pd.to_datetime(item, utc=True).tz_convert(\"Asia/Shanghai\")\n",
    "        for qtuple, obs in step.items()\n",
    "        if qtuple != \"timestamp\"\n",
    "        for rows, value in obs.items()\n",
    "        for idx, item in enumerate(value)\n",
    "    }\n",
    "    for step in anep[\"sequence\"]\n",
    "]\n",
    "\n",
    "dict_ep = {k: v for d in dict_observations for k, v in d.items()}\n",
    "\n",
    "\n",
    "# d = dict_observations[0]\n",
    "ser_decoded = pd.Series(dict_ep)\n",
    "ser_decoded.index.names = [\n",
    "    \"vehicle\",\n",
    "    \"driver\",\n",
    "    \"episodestart\",\n",
    "    \"timestamp\",\n",
    "    \"qtuple\",\n",
    "    \"rows\",\n",
    "    \"idx\",\n",
    "]\n",
    "df_decoded = ser_decoded.unstack(level=[\"qtuple\", \"rows\", \"idx\"])\n",
    "df_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42a923bce5130cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb25a5526ac9a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "df_episodes_list = []\n",
    "for ep in dicts:\n",
    "    dict_observations = [\n",
    "        {\n",
    "            (\n",
    "                ep[\"meta\"][\"episode_meta\"][\"vehicle\"],\n",
    "                ep[\"meta\"][\"episode_meta\"][\"driver\"],\n",
    "                pd.to_datetime(\n",
    "                    ep[\"meta\"][\"episode_meta\"][\"episodestart\"], unit=\"us\", utc=True\n",
    "                ).tz_convert(\"Asia/Shanghai\"),\n",
    "                pd.to_datetime(step[\"timestamp\"], unit=\"us\", utc=True).tz_convert(\n",
    "                    \"Asia/Shanghai\"\n",
    "                ),\n",
    "                qtuple,\n",
    "                rows,\n",
    "                idx,\n",
    "            ): item\n",
    "            if rows != \"timestep\"\n",
    "            else pd.to_datetime(item, utc=True).tz_convert(\"Asia/Shanghai\")\n",
    "            for qtuple, obs in step.items()\n",
    "            if qtuple != \"timestamp\"\n",
    "            for rows, value in obs.items()\n",
    "            for idx, item in enumerate(value)\n",
    "        }\n",
    "        for step in ep[\"sequence\"]\n",
    "    ]\n",
    "\n",
    "    dict_ep = {k: v for d in dict_observations for k, v in d.items()}\n",
    "\n",
    "    ser_decoded = pd.Series(dict_ep)\n",
    "    ser_decoded.index.names = [\n",
    "        \"vehicle\",\n",
    "        \"driver\",\n",
    "        \"episodestart\",\n",
    "        \"timestamp\",\n",
    "        \"qtuple\",\n",
    "        \"rows\",\n",
    "        \"idx\",\n",
    "    ]\n",
    "    df_decoded = ser_decoded.unstack(level=[\"qtuple\", \"rows\", \"idx\"])\n",
    "    df_decoded.sort_index(inplace=True, axis=1)\n",
    "    df_episodes_list.append(df_decoded)\n",
    "    # df_decoded\n",
    "\n",
    "# try:\n",
    "#     df_episodes = reduce(\n",
    "#         lambda left,right: pd.concat([left,right], axis=0, ignore_index=False),df_episodes_list\n",
    "#     )\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "\n",
    "index_names = df_episodes_list[0].index.names\n",
    "df_episodes = pd.concat(\n",
    "    df_episodes_list, keys=range(len(df_episodes_list)), names=[\"batch\"] + index_names\n",
    ")\n",
    "\n",
    "df_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5836d89cbe5ee817",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_episodes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc34fb7235b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# idx = pd.IndexSlice\n",
    "# episodestart_index = df_episodes.index.unique(level='episodestart')\n",
    "# episodestart_index\n",
    "\n",
    "# array of rewards for minibatch\n",
    "# batch = df_episodes.sort_index(inplace=False, axis=1)\n",
    "idx = pd.IndexSlice\n",
    "# df_rewards = batch.loc[\n",
    "#     :, idx['reward', 'work']\n",
    "# ]\n",
    "rewards_list = [\n",
    "    df_episodes.loc[idx[i, :, :, :, :], idx[\"reward\", \"work\"]].values.tolist()\n",
    "    for i in df_episodes.index.levels[0]\n",
    "]\n",
    "r_n_t = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    rewards_list, padding=\"post\", dtype=np.float32, value=-10000.0\n",
    ")\n",
    "print(f\"r_n_t: {rewards_list} {r_n_t.shape}\")\n",
    "\n",
    "# array of states for minibatch\n",
    "states_list = [\n",
    "    df_episodes.loc[idx[i, :, :, :, :], idx[\"state\", [\"velocity\", \"thrust\", \"brake\"]]].values.tolist()  # type: ignore\n",
    "    for i in df_episodes.index.levels[0]\n",
    "]\n",
    "s_n_t = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    states_list, padding=\"post\", dtype=np.float32, value=-10000.0\n",
    ")\n",
    "print(f\"s_n_t: {states_list} {s_n_t.shape}\")\n",
    "\n",
    "# array of actions for minibatch\n",
    "actions_list = [\n",
    "    df_episodes.loc[idx[i, :, :, :, :], idx[\"action\", torque_table_row_names]].values.tolist()  # type: ignore\n",
    "    for i in df_episodes.index.levels[0]\n",
    "]\n",
    "a_n_t = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    actions_list, padding=\"post\", dtype=np.float32, value=-10000.0\n",
    ")\n",
    "print(f\"a_n_t: {actions_list} {a_n_t.shape}\")\n",
    "\n",
    "# array of next_states for minibatch\n",
    "nstates_list = [\n",
    "    df_episodes.loc[idx[i, :, :, :, :], idx[\"nstate\", [\"velocity\", \"thrust\", \"brake\"]]].values.tolist()  # type: ignore\n",
    "    for i in df_episodes.index.levels[0]\n",
    "]\n",
    "ns_n_t = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    nstates_list, padding=\"post\", dtype=np.float32, value=-10000.0\n",
    ")\n",
    "\n",
    "print(f\"ns_n_t: {nstates_list} {ns_n_t.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa08082659c271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eos.data_io.utils.eos_pandas import (\n",
    "    decode_episode_batch_to_padded_arrays,\n",
    ")\n",
    "\n",
    "s, a, r, ns = decode_episode_batch_to_padded_arrays(\n",
    "    df_episodes, torque_table_row_names, padding_value=-10000\n",
    ")\n",
    "s\n",
    "a\n",
    "r\n",
    "ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9e2c26468c767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_n_t\n",
    "a_n_t\n",
    "s_n_t\n",
    "ns_n_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7386bce7f394265d",
   "metadata": {},
   "source": [
    "# sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48808af1afabf85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.bag import random\n",
    "\n",
    "print(f\"partitions: {b_episodes_read.npartitions}\")\n",
    "# bag = random.choices(b_episodes_read,16, split_every=4)\n",
    "bag = random.sample(b_episodes_read, 4, split_every=6)\n",
    "print(f\"partitions: {bag.npartitions}\")\n",
    "dicts = bag.compute()\n",
    "print(f\"length: {len(dicts)}\")\n",
    "for b in dicts:\n",
    "    b[\"episodestart\"] = pd.to_datetime(b[\"episodestart\"], unit=\"us\")\n",
    "    print(\n",
    "        f\"vehicle: {b['meta']['episode_meta']['vehicle']}; \"\n",
    "        f\"driver: {b['meta']['episode_meta']['driver']}; \"\n",
    "        f\"episodestart: {b['episodestart']}; \"\n",
    "        f\"site: {b['meta']['observation_meta']['site']}; \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef5a7f1919d7121",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts[0]['meta']['observation_meta']['site']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feafb2af5e17fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eos.data_io.utils.eos_pandas import avro_ep_decoding\n",
    "from zoneinfo import ZoneInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f46c5b42b53f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "tz_info = ZoneInfo(dicts[0]['meta']['observation_meta']['site']['tz'])\n",
    "pd.to_datetime(dicts[0][\"episodestart\"], unit=\"us\", utc=True).tz_convert(tz_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e3c5ecdd3d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_episodes_list = avro_ep_decoding(bag.compute(), tz_info=tz_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1525dc68eef312",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    states,\n",
    "    actions,\n",
    "    rewards,\n",
    "    nstates,\n",
    ") = decode_episode_batch_to_padded_arrays(\n",
    "    df_episodes_list, torque_table_row_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270913d79a34f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "quadruple = (states,actions,rewards,nstates)\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7fc016831f6fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     df_episodes = reduce(\n",
    "#         lambda left, right: pd.concat([left, right], axis=0, ignore_index=False),\n",
    "#         df_episodes_list,\n",
    "#     )\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "# \n",
    "# df_episodes.sort_index(inplace=True)\n",
    "# df_episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a0719bd71f7f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = b_episodes_read.compute()\n",
    "print(f\"length: {len(dicts)}\")\n",
    "for b in dicts:\n",
    "    b[\"episodestart\"] = pd.to_datetime(b[\"episodestart\"], unit=\"us\")\n",
    "    print(\n",
    "        f\"vehicle: {b['meta']['episode_meta']['vehicle']}; \"\n",
    "        f\"driver: {b['meta']['episode_meta']['driver']}; \"\n",
    "        f\"episodestart: {b['episodestart']} \"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89815b32c6309d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "ts0 = ts - DateOffset(days=5, hours=1)\n",
    "ts1 = ts + DateOffset(hours=1)\n",
    "type(ts)\n",
    "print(ts0, ts1)\n",
    "ts0.timestamp()\n",
    "ts0\n",
    "ts0_strip_tz = ts0.tz_convert(tz='UTC').tz_localize(None)\n",
    "ts0_strip_tz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da63d4e5983a6ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "queried = b_episodes_read.filter(\n",
    "    lambda x: pd.to_datetime(x[\"episodestart\"], unit=\"us\") > ts0_strip_tz\n",
    ")\n",
    "print(f\"ts0:{ts0}\")\n",
    "tz_info = ZoneInfo(dicts[0]['meta']['observation_meta']['site']['tz'])\n",
    "tz_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac0b75ec24d812",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dicts = queried.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1386ea8f433e47b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# bag = random.choices(queried,7)\n",
    "for b in dicts:\n",
    "    b[\"episodestart\"] = pd.to_datetime(b[\"episodestart\"], unit=\"us\").tz_localize(tz=\"UTC\").tz_convert(tz=tz_info)\n",
    "    # pd.to_datetime(b['episodestart'],unit='us')\n",
    "    print(\n",
    "        f\"vehicle: {b['meta']['episode_meta']['vehicle']}; \"\n",
    "        f\"driver: {b['meta']['episode_meta']['driver']}; \"\n",
    "        f\"episodestart: {b['episodestart']} \"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e83812312bc3252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queried = b_episodes_read.filter(lambda x: x['meta']['episode_meta']['vehicle'] == 'VB7' and\n",
    "#                                         x['meta']['episode_meta']['driver'] == 'wang-cheng' and\n",
    "#                                         pd.to_datetime(x['meta']['episode_meta']['episodestart'],unit='us') > ts0)\n",
    "\n",
    "print(ts0, ts1)\n",
    "ts0_stripped = ts0.tz_convert(tz='UTC').tz_localize(None)\n",
    "ts1_stripped = ts1.tz_convert(tz='UTC').tz_localize(None)\n",
    "queried = b_episodes_read.filter(\n",
    "    lambda x: x[\"meta\"][\"episode_meta\"][\"driver\"] == \"wang-cheng\"\n",
    "    and ts0_stripped \n",
    "              < pd.to_datetime(x[\"meta\"][\"episode_meta\"][\"episodestart\"], unit=\"us\") < \n",
    "        ts1_stripped\n",
    ")\n",
    "# bag = random.choices(queried,7)\n",
    "dicts = queried.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6798dca022c7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for b in dicts:\n",
    "    b[\"episodestart\"] = pd.to_datetime(b[\"episodestart\"], unit=\"us\").tz_localize(tz='UTC').tz_convert(tz_info)\n",
    "    # pd.to_datetime(b['episodestart'],unit='us')\n",
    "    print(\n",
    "        f\"vehicle: {b['meta']['episode_meta']['vehicle']}; \"\n",
    "        f\"driver: {b['meta']['episode_meta']['driver']}; \"\n",
    "        f\"episodestart: {b['episodestart']} \"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4989f8561673fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "\n",
    "a = db.from_sequence([1, 2, 3])\n",
    "b = db.from_sequence([4, 5, 6, 1, 2, 3])\n",
    "c = db.concat([a, b])\n",
    "c.compute()\n",
    "c.distinct().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbaed8c71f6f9bb",
   "metadata": {},
   "source": [
    "# filter by sequence length of the episode for efficient training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d323acf6363ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "queried = b_episodes_read.filter(lambda x: len(x[\"sequence\"]) <= 3)\n",
    "# bag = random.choices(queried,7)\n",
    "print(f\"ts0:{ts0}\")\n",
    "dicts = queried.compute()\n",
    "for b in dicts:\n",
    "    b[\"episodestart\"] = pd.to_datetime(b[\"episodestart\"], unit=\"us\").tz_localize(tz='UTC').tz_convert(tz=tz_info)\n",
    "    # pd.to_datetime(b['episodestart'],unit='us')\n",
    "    print(\n",
    "        f\"vehicle: {b['meta']['episode_meta']['vehicle']}; \"\n",
    "        f\"driver: {b['meta']['episode_meta']['driver']}; \"\n",
    "        f\"episodestart: {b['episodestart']} \"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
