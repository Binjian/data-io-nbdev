{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eos.data_io.config import drivers, trucks_by_id, drivers_by_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "ts = pd.to_datetime(datetime.now())\n",
    "ts_ind = ts + pd.to_timedelta(np.arange(2), \"H\")\n",
    "ts_ind\n",
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "df = pd.DataFrame(a, index=ts_ind, columns=[\"c1\", \"c2\", \"c3\"])\n",
    "df\n",
    "df.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts\n",
    "ts_end = pd.to_datetime(datetime.now())\n",
    "ts_end\n",
    "span = ts_end - ts\n",
    "span\n",
    "span_each_row = span / 4\n",
    "span_each_row\n",
    "np.linspace(0, 4, 5)\n",
    "ts_ser = ts + pd.to_timedelta(np.linspace(1, 4, 4) * span_each_row, unit=\"s\")\n",
    "ts_ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts = pd.DataFrame(np.array([ts]), columns=[\"timestamp\"])\n",
    "df_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.arange(10)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = np.arange(12)\n",
    "a1 = ss[:4].tolist()\n",
    "a2 = ss[4:8].tolist()\n",
    "a3 = ss[8:].tolist()\n",
    "ss = [a1, a2, a3]\n",
    "ss\n",
    "df_s = pd.DataFrame(ss, columns=[\"t\", \"v\", \"p\", \"b\"])  # .set_index('t')\n",
    "df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0, 4 * 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_ind = ts + pd.to_timedelta(np.arange(0, 4 * 20, 20), \"ms\")\n",
    "ts_ind\n",
    "ss = np.arange(12)\n",
    "a1 = ss[:4]\n",
    "a2 = ss[4:8]\n",
    "a3 = ss[8:]\n",
    "df_ss = pd.DataFrame(\n",
    "    {\"timestep\": ts_ind, \"velocity\": a1, \"thrust\": a2, \"brake\": a3}\n",
    ")  # .set_index('timestep')\n",
    "df_ss.columns.name = \"qtuple\"\n",
    "df_ss\n",
    "df_ss[[\"velocity\", \"thrust\", \"brake\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa_ss_flatten = df_ss[[\"velocity\", \"thrust\", \"brake\"]].to_numpy().flatten()\n",
    "npa_ss_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui_t = df_ss.loc[:, [\"velocity\", \"thrust\"]]\n",
    "ui_t\n",
    "power_t = ui_t.prod(axis=1)\n",
    "power_t\n",
    "power_t.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = df_ss.stack().swaplevel(0, 1)\n",
    "state.name = \"state\"\n",
    "state.index.names = [\"rows\", \"idx\"]\n",
    "state.sort_index(inplace=True)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(state)\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "a = len(ss) + np.arange(15)\n",
    "speed_ser = pd.Series(np.linspace(40, 60, 3), name=\"speed\")\n",
    "# row_ser\n",
    "row_dict = {f\"r{i}\": a[i * 5 : i * 5 + 5] for i in np.arange(3)}\n",
    "row_dict\n",
    "row_array = a.reshape(3, 5).transpose()\n",
    "row_array\n",
    "rows_df = pd.DataFrame(row_array)\n",
    "rows_df.columns = [f\"r{i}\" for i in np.arange(3)]\n",
    "rows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_ind = ts + pd.to_timedelta(np.arange(5 * 20, 8 * 20, 20), \"ms\")\n",
    "ts_ser = pd.Series(ts_ind, name=\"timestep\")\n",
    "throttle_ser = pd.Series(np.linspace(0, 1.0, 5), name=\"throttle\")\n",
    "# throttle_ser\n",
    "dfs = [rows_df, ts_ser, speed_ser, throttle_ser]\n",
    "action = (\n",
    "    reduce(\n",
    "        lambda left, right: pd.merge(\n",
    "            left, right, how=\"outer\", left_index=True, right_index=True\n",
    "        ),\n",
    "        dfs,\n",
    "    )\n",
    "    .stack()\n",
    "    .swaplevel(0, 1)\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "action.name = \"action\"\n",
    "action.index.names = [\"rows\", \"idx\"]\n",
    "action.index\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = (\n",
    "    pd.DataFrame({\"work\": len(ss) + len(a), \"timestep\": ts_ind[0]}, index=[0])\n",
    "    .stack()\n",
    "    .swaplevel(0, 1)\n",
    "    .sort_index()\n",
    ")\n",
    "# reward_index = (reward.name,  ts_ind[0], 0)\n",
    "reward.index.names = [\"rows\", \"idx\"]\n",
    "reward.name = \"reward\"\n",
    "reward.index\n",
    "reward\n",
    "# reward.name = 'reward'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward[\"work\"][0]\n",
    "reward.loc[(\"work\", 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_ind = ts + pd.to_timedelta(5, \"s\") + pd.to_timedelta(np.arange(0, 4 * 20, 20), \"ms\")\n",
    "ts_ind\n",
    "ss = (\n",
    "    np.arange(12) + len(ss) + len(a) + len(reward) - 1\n",
    ")  # exclude the timestamp in reward\n",
    "a1 = ss[:4]\n",
    "a2 = ss[4:8]\n",
    "a3 = ss[8:]\n",
    "nstate = (\n",
    "    pd.DataFrame({\"timestep\": ts_ind, \"velocity\": a1, \"thrust\": a2, \"brake\": a3})\n",
    "    # .set_index('timestamp')\n",
    "    .stack()\n",
    "    .swaplevel(0, 1)\n",
    "    .sort_index()\n",
    ")\n",
    "nstate.name = \"nstate\"\n",
    "nstate.index.names = [\"rows\", \"idx\"]\n",
    "len(nstate)\n",
    "nstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = pd.Series([ts], name=\"timestamp\")\n",
    "timestamp.index = pd.MultiIndex.from_product(\n",
    "    [timestamp.index, [0]], names=[\"rows\", \"idx\"]\n",
    ")\n",
    "timestamp.index\n",
    "state.index\n",
    "action.index\n",
    "reward.index\n",
    "nstate.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_index = (timestamp.name, \"\", 0)\n",
    "# timestamp_index\n",
    "state_index = [(state.name, *i) for i in state.index]\n",
    "# state_index\n",
    "reward_index = [(reward.name, *i) for i in reward.index]\n",
    "# reward_index\n",
    "action_index = [(action.name, *i) for i in action.index]\n",
    "# action_index\n",
    "nstate_index = [(nstate.name, *i) for i in nstate.index]\n",
    "# nstate_index\n",
    "\n",
    "multiindex = pd.MultiIndex.from_tuples(\n",
    "    [timestamp_index, *state_index, *action_index, *reward_index, *nstate_index]\n",
    ")\n",
    "multiindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_list = [timestamp, state, action, reward, nstate]\n",
    "observation = pd.concat(observation_list)\n",
    "observation.index = multiindex\n",
    "observation\n",
    "observation.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation0 = observation.copy()\n",
    "observation0.loc[\"timestamp\", \"\", 0] = ts + pd.Timedelta(1, \"h\")\n",
    "observation1 = observation.copy()\n",
    "observation1.loc[\"timestamp\", \"\", 0] = ts + pd.Timedelta(2, \"h\")\n",
    "observation2 = observation.copy()\n",
    "observation2.loc[\"timestamp\", \"\", 0] = ts + pd.Timedelta(3, \"h\")\n",
    "observation3 = observation.copy()\n",
    "observation3.loc[\"timestamp\", \"\", 0] = ts + pd.Timedelta(4, \"h\")\n",
    "observation4 = observation.copy()\n",
    "observation4.loc[\"timestamp\", \"\", 0] = ts + pd.Timedelta(5, \"h\")\n",
    "observation_list = [\n",
    "    observation0,\n",
    "    observation1,\n",
    "    observation2,\n",
    "    observation3,\n",
    "    observation4,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract state action reward nstate from list of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "observation_list[0]\n",
    "observation_list[0].index\n",
    "observation_list[0].loc[idx[\"state\", [\"velocity\", \"thrust\", \"brake\"]]].values\n",
    "observation_list[0].loc[idx[\"action\", [\"r0\", \"r1\", \"r2\"]]].values\n",
    "observation_list[0].loc[idx[\"reward\", [\"work\"]]].values\n",
    "observation_list[0].loc[idx[\"nstate\", [\"velocity\", \"thrust\", \"brake\"]]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "state = []\n",
    "action = []\n",
    "reward = []\n",
    "nstate = []\n",
    "for observation in observation_list:\n",
    "    state.append(observation.loc[idx[\"state\", [\"velocity\", \"thrust\", \"brake\"]]].values)\n",
    "    action.append(observation.loc[idx[\"action\", [\"r0\", \"r1\", \"r2\"]]].values)\n",
    "    reward.append(observation.loc[idx[\"reward\", [\"work\"]]].values)\n",
    "    nstate.append(\n",
    "        observation.loc[idx[\"nstate\", [\"velocity\", \"thrust\", \"brake\"]]].values\n",
    "    )\n",
    "npa_state = np.stack(state)\n",
    "npa_action = np.stack(action)\n",
    "npa_reward = np.stack(reward)\n",
    "npa_nstate = np.stack(nstate)\n",
    "npa_state\n",
    "npa_action\n",
    "npa_reward\n",
    "npa_nstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_epi = pd.concat(observation_list, axis=1).transpose()\n",
    "dfs_epi.columns.names = [\"qtuple\", \"rows\", \"idx\"]\n",
    "dfs_epi.columns\n",
    "dfs_epi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode1 = dfs_epi.copy().sort_index(axis=1)\n",
    "dfs_episode1.set_index((\"timestamp\", \"\", 0), inplace=True)\n",
    "dfs_episode1.index.name = \"timestamp\"\n",
    "idx = pd.IndexSlice\n",
    "dfs_episode1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode1.loc[:, idx[\"action\":\"state\", :, :, :]] = dfs_episode1.loc[\n",
    "    :, idx[\"action\":\"state\", :, :, :]\n",
    "].astype(\"int\")\n",
    "dfs_episode1\n",
    "dfs_episode1.dtypes\n",
    "dfs_episode1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert columns types to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_epi\n",
    "dfs_episode = dfs_epi.copy()\n",
    "dfs_episode.index\n",
    "dfs_episode.set_index((\"timestamp\", \"\", 0), inplace=True)\n",
    "dfs_episode.sort_index(axis=1, inplace=True)\n",
    "dfs_episode.index\n",
    "dfs_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode.index.name = \"timestamp\"\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "\n",
    "state_cols_float = [(\"state\", col) for col in [\"brake\", \"thrust\", \"velocity\"]]\n",
    "action_cols_float = [(\"action\", col) for col in [\"r0\", \"r1\", \"r2\", \"speed\", \"throttle\"]]\n",
    "reward_cols_float = [(\"reward\", \"work\")]\n",
    "nstate_cols_float = [(\"nstate\", col) for col in [\"brake\", \"thrust\", \"velocity\"]]\n",
    "for col in action_cols_float + state_cols_float + reward_cols_float + nstate_cols_float:\n",
    "    dfs_episode[col[0], col[1]] = dfs_episode[col[0], col[1]].astype(\n",
    "        \"float\"\n",
    "    )  # float16 not allowed in parquet\n",
    "dfs_episode\n",
    "dfs_episode.dtypes\n",
    "# dfs_episode.columns\n",
    "# dfs_epi\n",
    "# dfs_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepend two levels of index \"vehicle\" and \"driver\" to the DataFrame object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodestart = ts - pd.Timedelta(1, \"h\")\n",
    "dfs_epi = pd.concat([dfs_episode], keys=[ts], names=[\"episodestart\"])\n",
    "\n",
    "dfs_epi = pd.concat([dfs_epi], keys=[drivers_by_id[\"wang-cheng\"].pid], names=[\"driver\"])\n",
    "dfs_epi = pd.concat([dfs_epi], keys=[trucks_by_id[\"VB7\"].vid], names=[\"vehicle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_epi.index\n",
    "dfs_epi.columns\n",
    "dfs_epi\n",
    "dfs_epi.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a level of index for episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_episode = dfs_episode.swaplevel(1, 0, axis=0)\n",
    "# dfs_episode = dfs_episode.swaplevel(1, 2, axis=0)\n",
    "dfs_epi.sort_index(inplace=True)\n",
    "dfs_epi.index\n",
    "dfs_epi.columns\n",
    "dfs_epi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_epi.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vb7 = trucks_by_id[\"VB7\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eos.data_io.eos_struct import PoolQuery\n",
    "\n",
    "query = PoolQuery(\n",
    "    site=trucks_by_id[\"VB7\"].site,\n",
    "    vehicle=trucks_by_id[\"VB7\"].vid,\n",
    "    driver=drivers_by_id[\"zheng-longfei\"].pid,\n",
    "    start=episodestart,\n",
    "    end=ts,\n",
    ")\n",
    "isinstance(query, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_new = pd.to_datetime((datetime.now()))\n",
    "ts_new\n",
    "episodestart = ts_new - pd.Timedelta(2, \"d\")\n",
    "episodestart\n",
    "\n",
    "dfs_episode0 = dfs_epi.copy()\n",
    "dfs_episode0.index = dfs_episode0.index.set_levels([episodestart], level=\"episodestart\")\n",
    "dfs_episode0.index = dfs_episode0.index.set_levels(\n",
    "    [[trucks_by_id[\"VB7\"].vid], [drivers_by_id[\"zheng-longfei\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "dfs_episode1 = dfs_epi.copy()\n",
    "dfs_episode1.index = dfs_episode1.index.set_levels(\n",
    "    [episodestart - pd.Timedelta(3, \"d\")], level=\"episodestart\"\n",
    ")\n",
    "dfs_episode1.index = dfs_episode1.index.set_levels(\n",
    "    [[trucks_by_id[\"MP73\"].vid], [drivers_by_id[\"wang-cheng\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "dfs_episode2 = dfs_epi.copy()\n",
    "dfs_episode2.index = dfs_episode2.index.set_levels(\n",
    "    [episodestart - pd.Timedelta(4, \"d\")], level=\"episodestart\"\n",
    ")\n",
    "dfs_episode2.index = dfs_episode2.index.set_levels(\n",
    "    [[trucks_by_id[\"VB7\"].vid], [drivers_by_id[\"wang-cheng\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "dfs_episode3 = dfs_epi.copy()\n",
    "dfs_episode3.index = dfs_episode3.index.set_levels(\n",
    "    [episodestart - pd.Timedelta(5, \"d\")], level=\"episodestart\"\n",
    ")\n",
    "dfs_episode3.index = dfs_episode3.index.set_levels(\n",
    "    [[trucks_by_id[\"MP73\"].vid], [drivers_by_id[\"zheng-longfei\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "dfs_episode0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "episodes = [dfs_epi, dfs_episode0, dfs_episode1, dfs_episode2, dfs_episode3]\n",
    "try:\n",
    "    dfs_episode_all = reduce(\n",
    "        lambda left, right,: pd.concat([left, right], axis=0), episodes\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "# dfs_episode_all.sort_index(inplace=True)\n",
    "# dfs_episode_all = dfs_episode_all.swaplevel(1, 0, axis=0)\n",
    "# dfs_episode_all = dfs_episode_all.swaplevel(1, 2, axis=0)\n",
    "dfs_episode_all.sort_index(inplace=True)\n",
    "dfs_episode_all = dfs_episode_all[[\"state\", \"action\", \"reward\", \"nstate\"]]\n",
    "dfs_episode_all\n",
    "dfs_episode_all.index\n",
    "dfs_episode_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make MultiIndex to Rows by adding levels of \"Vehicle\" and \"Driver\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  reset index to move vehicle and driver to columns, preprocessing for dask manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episodes = dfs_episode_all.reset_index(\n",
    "    level=[\"vehicle\", \"driver\"]\n",
    ")  # unstack vehicle and driver to columns in level 0 with default '' in level 1\n",
    "dfs_episodes\n",
    "# dfs_episodes.columns\n",
    "# dfs_episodes.dtypes\n",
    "# dfs_episode_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episodes.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episodes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episodes[\"vehicle\"] = dfs_episodes[\"vehicle\"].astype(\"category\")\n",
    "dfs_episodes[\"driver\"] = dfs_episodes[\"driver\"].astype(\"category\")\n",
    "\n",
    "dfs_episodes_sorted = dfs_episodes.sort_index(\n",
    "    axis=1, level=[0, 1, 2], ascending=[True, True, True]\n",
    ")\n",
    "dfs_episodes_sorted.sort_index(axis=0, inplace=True)\n",
    "# dfs_episodes_sorted = dfs_episodes.sort_index(axis=1, level=[0,1], ascending=[True, True])\n",
    "# dfs_episodes_sorted = dfs_episodes.sort_index(axis=0, level=[0,1], ascending=[True, True])\n",
    "# dfs_episodes_sorted.sort_index(axis=1,inplace=True)\n",
    "dfs_episodes_sorted\n",
    "dfs_episodes_sorted.columns\n",
    "\n",
    "# dfs_episodes[['driver'], ['vehicle']] = dfs_episodes[['driver', 'vehicle']].apply(lambda x: x.cat.codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episodes_sorted\n",
    "# dfs_episode.loc[:,'state']\n",
    "dfs_slice = dfs_episodes_sorted.loc[\n",
    "    dfs_episodes_sorted[\"vehicle\"] == \"MP73\", [\"vehicle\", \"driver\", \"state\", \"reward\"]\n",
    "]\n",
    "dfs_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saved episodes as parquet in multi-level folders \"vehicle/driver/episode/tuple.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import platform\n",
    "\n",
    "print(\"Python: \", platform.python_version())\n",
    "print(\"pandas: \", pd.__version__)\n",
    "print(\"pyarrow: \", pa.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episodes_sorted\n",
    "dfs_episodes_sorted.index\n",
    "dfs_episodes_sorted.columns\n",
    "dfs_episodes_sorted.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_complete_episodes = dfs_episodes_sorted.reset_index(\"episodestart\")\n",
    "dfs_complete_episodes\n",
    "dfs_complete_episodes.index\n",
    "dfs_complete_episodes.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoding multiindex dataframe to flat index for Dask DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_complete_episodes.columns\n",
    "cols = dfs_complete_episodes.columns.to_flat_index()\n",
    "cols\n",
    "\n",
    "## 3 level index\n",
    "# cols_str = [\n",
    "#     f'{x[0]}_{x[1]}_{x[2]}'\n",
    "#     if (x[1] != '')\n",
    "#     else f'{x[0]}__{x[2]}'\n",
    "#     if (x[2] != '')\n",
    "#     else f'{x[0]}'\n",
    "#     for x in cols\n",
    "# ]\n",
    "\n",
    "## 4 level index\n",
    "cols_str = [\n",
    "    f\"{x[0]}_{x[1]}_{x[2]}\"\n",
    "    if (x[1] != \"\" and x[2] != \"\")\n",
    "    else f\"{x[0]}_{x[1]}_\"\n",
    "    if (x[1] != \"\" and x[2] == \"\")\n",
    "    else f\"{x[0]}__{x[2]}\"\n",
    "    if (x[2] != \"\")\n",
    "    else f\"{x[0]}__\"\n",
    "    for x in cols\n",
    "]\n",
    "cols_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding index and columns\n",
    "dfs_complete_episodes_flat = dfs_complete_episodes.copy()\n",
    "cols = dfs_complete_episodes.columns.to_flat_index()\n",
    "dfs_complete_episodes_flat.columns = [\n",
    "    f\"{x[0]}_{x[1]}_{x[2]}\"\n",
    "    if (x[1] != \"\" and x[2] != \"\")\n",
    "    else f\"{x[0]}_{x[1]}_\"\n",
    "    if (x[1] != \"\" and x[2] == \"\")\n",
    "    else f\"{x[0]}__{x[2]}\"\n",
    "    if (x[2] != \"\")\n",
    "    else f\"{x[0]}__\"\n",
    "    for x in cols\n",
    "]\n",
    "dfs_complete_episodes_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_complete_episodes_flat\n",
    "dfs_complete_episodes_flat.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_complete_episodes_flat[\"vehicle__\"].iloc[0:2].values.dtype\n",
    "dfs_complete_episodes_flat[\"vehicle__\"].dtype\n",
    "dfs_complete_episodes_flat[\"state_velocity_0\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ddf_episodes = dd.from_pandas(dfs_complete_episodes_flat, npartitions=1)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    ddf_episodes.index\n",
    "    ddf_episodes.columns\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "ddf_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_metadata = {\n",
    "    b\"eos\": b'{\"length\":[3, 600, 5],\"timezeone\":\"sh\",\"units\":[\"kph\",\"pct\",\"pct\"],\"data_propensity\":[50, 1, 4],\"Dataset Description\": \"MP vehicle from TBox\"}'\n",
    "}\n",
    "custom_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with ProgressBar():\n",
    "        ddf_episodes.to_parquet(\n",
    "            \"eos_new_episodes\",\n",
    "            engine=\"pyarrow\",\n",
    "            compression=\"snappy\",\n",
    "            partition_on=[\"vehicle__\", \"driver__\", \"episodestart__\"],\n",
    "            write_metadata_file=True,\n",
    "            custom_metadata=custom_metadata,\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read saved parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(\n",
    "    \"eos_new_episodes\",\n",
    "    engine=\"pyarrow\",\n",
    "    compression=\"snappy\",\n",
    "    ignore_metadata_file=False,\n",
    "    infer_divisions=True,\n",
    ")\n",
    "ddf.head(5)\n",
    "ddf.dtypes\n",
    "ddf.npartitions\n",
    "ddf.divisions\n",
    "ddf._meta\n",
    "ddf.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get customized metadata with pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "\n",
    "# print('pyarrow: ', pa.__version__)\n",
    "\n",
    "# table = pq.read_table('ddf_episodes')\n",
    "# table_meta = table.schema.metadata[b'eos']\n",
    "# print(table_meta)\n",
    "# # custom_metadata_info = json.loads(table.schema.metadata[b'eos'])\n",
    "# # print(custom_metadata_info)\n",
    "# try:\n",
    "#     custom_meta_info = json.loads(table_meta)\n",
    "# except Exception as e:\n",
    "#     print(f'Exception: {e}')\n",
    "\n",
    "# print(custom_meta_info)\n",
    "\n",
    "# # Print formatted output of the dichtionary custom_meta_info:\n",
    "# for key, val in custom_meta_info.items():\n",
    "#     print('{:15}: {}'.format(key, val))\n",
    "\n",
    "# ALTERNATE:\n",
    "\n",
    "table_meta = pq.read_metadata(\"eos_complete_episodes/_common_metadata\")\n",
    "# print(table_meta)\n",
    "# print(table_meta.metadata[b'eos'])\n",
    "custom_meta_info = table_meta.metadata[b\"eos\"]\n",
    "custom_meta_info = json.loads(custom_meta_info)\n",
    "# custom_meta_info\n",
    "\n",
    "for key, val in custom_meta_info.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "\n",
    "# table_meta = pq.read_metadata('ddf_episodes/_metadata')\n",
    "# # print(table_meta)\n",
    "# # print(table_meta.metadata[b'eos'])\n",
    "# custom_meta_info = table_meta.metadata[b'eos']\n",
    "# custom_meta_info = json.loads(custom_meta_info)\n",
    "# custom_meta_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoding flat_index of dask dataframe to multiindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes_read = ddf.compute()\n",
    "df_episodes_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding\n",
    "multi_cols = [tuple(col.split(\"_\")) for col in df_episodes_read.columns]\n",
    "multi_cols\n",
    "# multi_cols = [(col[0], int(col[1])) if len(col)==2 else col for col in multi_cols]\n",
    "# multi_cols\n",
    "df_episodes_read_multicol = df_episodes_read.copy()\n",
    "multi_idx = pd.MultiIndex.from_tuples(multi_cols)\n",
    "# multi_idx\n",
    "multi_idx.names = [\"qtuple\", \"rows\", \"idx\"]\n",
    "driver_vehicle = [\n",
    "    idx\n",
    "    for idx in multi_idx\n",
    "    if idx[0] == \"driver\" or idx[0] == \"vehicle\" or idx[0] == \"episodestart\"\n",
    "]\n",
    "driver_vehicle\n",
    "# multi_idx = multi_idx.set_levels([multi_idx.levels[0], multi_idx.levels[1].astype(int)])\n",
    "# multi_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series([1, np.nan,2,None])\n",
    "# pd.Series([1,np.nan,2,None,pd.NA])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding columns\n",
    "from datetime import datetime\n",
    "\n",
    "multi_tpl = [tuple(col.split(\"_\")) for col in df_episodes_read.columns]\n",
    "# multi_tpl\n",
    "# multi_cols = [(col[0], int(col[1])) if len(col)==2 else col for col in multi_cols]\n",
    "# multi_cols\n",
    "df_episodes_read_multicol = df_episodes_read.copy()\n",
    "multi_col = pd.MultiIndex.from_tuples(multi_tpl)\n",
    "# multi_idx\n",
    "# multi_col\n",
    "i1 = multi_col.get_level_values(0)\n",
    "i1 = [\n",
    "    \"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else idx for idx in i1\n",
    "]  # convert index of level 2 type to int and '' if NA\n",
    "i1\n",
    "# i2 = multi_col.get_level_values(1).fillna('') # must be null string instead of the default pd.NA or np.nan\n",
    "# i2 = [idx if isinstance(idx, int) else '' for idx in i2]\n",
    "i2 = multi_col.get_level_values(\n",
    "    1\n",
    ")  # must be null string instead of the default pd.NA or np.nan\n",
    "i2 = [\n",
    "    \"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else idx for idx in i2\n",
    "]  # convert index of level 2 type to int and '' if NA\n",
    "i2\n",
    "# i2 = [idx if isinstance(idx, str) else '' for idx in i2]\n",
    "# i2.astype('int')\n",
    "i3 = multi_col.get_level_values(\n",
    "    2\n",
    ")  # must be null string instead of the default pd.NA or np.nan\n",
    "i3 = [\n",
    "    \"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else int(idx) for idx in i3\n",
    "]  # convert index of level 2 type to int and '' if NA\n",
    "i3\n",
    "\n",
    "\n",
    "multi_col = pd.MultiIndex.from_arrays([i1, i2, i3])\n",
    "multi_col.names = [\"qtuple\", \"rows\", \"idx\"]\n",
    "multi_col\n",
    "\n",
    "\n",
    "# i2[-1]\n",
    "# i2.dropna()\n",
    "# i2\n",
    "# l2 = ['' if i==nan else i  for i in i2]\n",
    "# l2\n",
    "# i2= i2[:-2].astype('int')\n",
    "# i2 = i2.append(pd.Index(['','']))\n",
    "# l2 = [''] + list(l2.values[:-1])\n",
    "# print(l2)\n",
    "# arrays = [ for i2 in l2 for i1 in l1]\n",
    "# l2 = l2[:-2].astype('int') + ['', '']\n",
    "# multi_col.set_levels(ll, level=['qtuple','step'])\n",
    "# driver_vehicle = [idx for idx in multi_idx if idx[0]=='driver' or idx[0]=='vehicle']\n",
    "# multi_tpl =  [(col[0],)  if col[0]=='driver' or col[0]=='vehicle' else (col[0],int(col[1])) for col in multi_col]\n",
    "# multi_idx1 = pd.MultiIndex.from_tuples(multi_idx1)\n",
    "# multi_idx = multi_idx.set_levels([multi_idx.levels[0], multi_idx.levels[1].astype(int)])\n",
    "# multi_col = pd.MultiIndex.from_tuples(multi_tpl)\n",
    "# multi_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # decoding index\n",
    "# multi_tuple = [tuple(idx.split('_')) for idx in df_episodes_read.index]\n",
    "# multi_idx = pd.MultiIndex.from_tuples(multi_tuple)\n",
    "# # multi_idx\n",
    "# i0 = multi_idx.get_level_values(\n",
    "#     0\n",
    "# )  # must be null string instead of the default pd.NA or np.nan\n",
    "# i0 = [\n",
    "#     ''\n",
    "#     if str(idx) in (str(pd.NA), 'nan', '')\n",
    "#     else datetime.strptime(idx, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "#     for idx in i0\n",
    "# ]  # convert index of level 2 type to int and '' if NA\n",
    "# i1 = multi_idx.get_level_values(\n",
    "#     1\n",
    "# )  # must be null string instead of the default pd.NA or np.nan\n",
    "# i1 = [\n",
    "#     ''\n",
    "#     if str(idx) in (str(pd.NA), 'nan', '')\n",
    "#     else datetime.strptime(idx, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "#     for idx in i1\n",
    "# ]  # convert index of level 2 type to int and '' if NA\n",
    "# df_episodes_read_multiidx = df_episodes_read.copy()\n",
    "# multi_idx = pd.MultiIndex.from_arrays([i0, i1], names=('episodestart', 'timestamp'))\n",
    "#\n",
    "# multi_idx.dtypes\n",
    "# multi_idx\n",
    "# len(multi_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes_read_multi = df_episodes_read.copy()\n",
    "df_episodes_read_multi.columns = multi_col\n",
    "# df_episodes_read_multi.index = multi_idx\n",
    "df_episodes_read_multi\n",
    "df_episodes_read_multi.columns\n",
    "df_episodes_read_multi.index\n",
    "df_episodes_read_multi.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recover deep multiindex from columns ['vehicle','driver','episodestart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes_read_multi_setindex = df_episodes_read_multi.set_index(\n",
    "    [\"vehicle\", \"driver\", \"episodestart\", df_episodes_read_multi.index]\n",
    ")\n",
    "# df_episodes_read_multi_setindex\n",
    "# df_episodes_read_multi_setindex.sort_index(inplace=True)\n",
    "# df_episodes_read_multi_setindex = df_episodes_read_multi_setindex.swaplevel(\n",
    "#     1, 2, axis=0\n",
    "# ).swaplevel(0, 1, axis=0)\n",
    "# df_episodes_read_multi_setindex.index\n",
    "# df_episodes_read_multi_setindex.columns\n",
    "df_episodes_read_multi_setindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## append new episode to the specific parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode4 = dfs_epi.copy()\n",
    "dfs_episode4.index = dfs_episode4.index.set_levels(\n",
    "    [episodestart - pd.Timedelta(8, \"d\")], level=\"episodestart\"\n",
    ")\n",
    "dfs_episode4.index = dfs_episode4.index.set_levels(\n",
    "    [[trucks_by_id[\"VB4\"].vid], [drivers_by_id[\"zheng-longfei\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "# dfs_episode4\n",
    "# dfs_episode4 = dfs_episode4.swaplevel(1, 2, axis=0).swaplevel(0, 1, axis=0)\n",
    "dfs_episode4\n",
    "\n",
    "# dfs_episode5 = dfs_episode.copy()\n",
    "# dfs_episode5.index = dfs_episode3.index.set_levels([episodestart-pd.Timedelta(9,'d')], level='episode')\n",
    "# df_episodes_read_multi_setindex.columns\n",
    "# df_episodes_read_multi_setindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episodes_new = pd.concat([df_episodes_read_multi_setindex, dfs_episode4], axis=0)\n",
    "dfs_episodes_new.index.names = [\"vehicle\", \"driver\", \"episodestart\", \"timestamp\"]\n",
    "dfs_episodes_new\n",
    "dfs_episodes_new.columns\n",
    "dfs_episodes_new.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save new dataframe back to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episodes_new_flat = dfs_episodes_new.copy()\n",
    "dfs_episodes_new_flat = dfs_episodes_new_flat.reset_index(\n",
    "    level=[\"vehicle\", \"driver\", \"episodestart\"]\n",
    ")\n",
    "dfs_episodes_new_flat.columns\n",
    "dfs_episodes_new_flat.index\n",
    "dfs_episodes_new_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding index\n",
    "# from datetime import datetime\n",
    "\n",
    "# flat_idx = dfs_episodes_new_flat.index.to_flat_index()\n",
    "# flat_idx = [\n",
    "#     f'{x[0].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}_{x[1].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}'\n",
    "#     if x[1] != ''\n",
    "#     else f'{x[0].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}'\n",
    "#     for x in flat_idx\n",
    "# ]\n",
    "# # flat_idx\n",
    "\n",
    "# encoding columns\n",
    "flat_cols = dfs_episodes_new_flat.columns.to_flat_index()\n",
    "# flat_cols\n",
    "# flat_cols = [\n",
    "#     f'{x[0]}_{x[1]}_{x[2].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}_{x[3]}'\n",
    "#     if (x[1] != '' and x[3] not in ('', pd.NaT, str(pd.NA)))\n",
    "#     else f'{x[0]}_{x[1]}_{x[2].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}_'\n",
    "#     if (x[1] != '' and x[3] in ('', pd.NaT, str(pd.NA)))\n",
    "#     else f'{x[0]}__{x[2].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}_'\n",
    "#     if (x[2] not in ('', pd.NaT, str(pd.NA)))\n",
    "#     else f'{x[0]}___'\n",
    "#     for x in flat_cols\n",
    "# ]\n",
    "flat_cols = [\n",
    "    f\"{x[0]}_{x[1]}_{x[2]}\"\n",
    "    if (x[1] != \"\" and x[2] not in (\"\", pd.NaT, str(pd.NA)))\n",
    "    else f\"{x[0]}_{x[1]}_\"\n",
    "    if (x[1] != \"\" and x[2] in (\"\", pd.NaT, str(pd.NA)))\n",
    "    else f\"{x[0]}__{x[2]}\"\n",
    "    if (x[2] not in (\"\", pd.NaT, str(pd.NA)))\n",
    "    else f\"{x[0]}__\"\n",
    "    for x in flat_cols\n",
    "]\n",
    "flat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episodes_new_flat.columns = flat_cols\n",
    "# dfs_episodes_new_flat.index = flat_idx\n",
    "dfs_episodes_new_flat[\"vehicle__\"] = dfs_episodes_new_flat[\"vehicle__\"].astype(\n",
    "    \"category\"\n",
    ")\n",
    "dfs_episodes_new_flat[\"driver__\"] = dfs_episodes_new_flat[\"driver__\"].astype(\"category\")\n",
    "dfs_episodes_new_flat[\"episodestart__\"] = dfs_episodes_new_flat[\n",
    "    \"episodestart__\"\n",
    "].astype(\"datetime64[ns]\")\n",
    "# dfs_episodes_new_flat.index.name = 'episodestart_timestamp'\n",
    "dfs_episodes_new_flat.index\n",
    "dfs_episodes_new_flat.dtypes\n",
    "# dfs_episodes_new_flat.columns.name = 'episodestart_timestamp'\n",
    "dfs_episodes_new_flat.columns\n",
    "dfs_episodes_new_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ddf_episodes_new = dd.from_pandas(dfs_episodes_new_flat, npartitions=1)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "ddf_episodes_new.dtypes\n",
    "# dfs_episodes_new_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "custom_metadata = {\n",
    "    b\"eos\": b'{\"length\":[6, 600, 5],\"timezeone\":\"sh\",\"units\":[\"kph\",\"pct\",\"pct\"],\"data_propensity\":[50, 1, 4],\"Dataset Description\": \"MP vehicle from TBox\"}'\n",
    "}\n",
    "\n",
    "try:\n",
    "    with ProgressBar():\n",
    "        ddf_episodes_new.to_parquet(\n",
    "            \"eos_new_add_episodes\",\n",
    "            engine=\"pyarrow\",\n",
    "            compression=\"snappy\",\n",
    "            partition_on=[\"vehicle__\", \"driver__\", \"episodestart__\"],\n",
    "            write_metadata_file=True,\n",
    "            custom_metadata=custom_metadata,\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(\n",
    "    \"eos_new_add_episodes\",\n",
    "    engine=\"pyarrow\",\n",
    "    compression=\"snappy\",\n",
    "    ignore_metadata_file=False,\n",
    "    infer_divisions=True,\n",
    ")\n",
    "pq_meta = pq.read_metadata(\"eos_episodes/_common_metadata\")\n",
    "custom_meta_info = pq_meta.metadata[b\"eos\"]\n",
    "custom_meta_info = json.loads(custom_meta_info)\n",
    "for key, val in custom_meta_info.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "ddf.dtypes\n",
    "df = ddf.compute()\n",
    "\n",
    "# decoding columns index\n",
    "multi_tpl = [tuple(col.split(\"_\")) for col in df.columns]\n",
    "multi_col = pd.MultiIndex.from_tuples(multi_tpl)\n",
    "i1 = multi_col.get_level_values(0)\n",
    "i1 = [\"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else idx for idx in i1]\n",
    "i2 = multi_col.get_level_values(\n",
    "    1\n",
    ")  # must be null string instead of the default pd.NA or np.nan\n",
    "i2 = [\n",
    "    \"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else idx for idx in i2\n",
    "]  # convert index of level 2 type to int and '' if NA\n",
    "i3 = multi_col.get_level_values(\n",
    "    2\n",
    ")  # must be null string instead of the default pd.NA or np.nan\n",
    "i3 = [\n",
    "    \"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else int(idx) for idx in i3\n",
    "]  # convert index of level 2 type to int and '' if NA\n",
    "\n",
    "\n",
    "multi_col = pd.MultiIndex.from_arrays([i1, i2, i3])\n",
    "multi_col.names = [\"qtuple\", \"rows\", \"idx\"]\n",
    "multi_col\n",
    "\n",
    "# # decoding index\n",
    "# multi_tuple = [tuple(idx.split('_')) for idx in df.index]\n",
    "# multi_idx = pd.MultiIndex.from_tuples(multi_tuple)\n",
    "\n",
    "# i0 = multi_idx.get_level_values(\n",
    "#     0\n",
    "# )  # must be null string instead of the default pd.NA or np.nan\n",
    "# i0 = [\n",
    "#     ''\n",
    "#     if str(idx) in (str(pd.NA), 'nan', '')\n",
    "#     else datetime.strptime(idx, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "#     for idx in i0\n",
    "# ]  # convert index of level 2 type to int and '' if NA\n",
    "# i1 = multi_idx.get_level_values(\n",
    "#     1\n",
    "# )  # must be null string instead of the default pd.NA or np.nan\n",
    "# i1 = [\n",
    "#     ''\n",
    "#     if str(idx) in (str(pd.NA), 'nan', '')\n",
    "#     else datetime.strptime(idx, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "#     for idx in i1\n",
    "# ]  # convert index of level 2 type to int and '' if NA\n",
    "# df_episodes_read_multiidx = df_episodes_read.copy()\n",
    "# multi_idx = pd.MultiIndex.from_arrays([i0, i1], names=('episodestart', 'timestamp'))\n",
    "#\n",
    "# multi_idx.names = ['episodestart', 'timestamp']\n",
    "\n",
    "df.columns = multi_col\n",
    "# df.index = multi_idx\n",
    "\n",
    "df = df.set_index([\"vehicle\", \"driver\", \"episodestart\", df.index])\n",
    "df.index.dtypes\n",
    "df.index\n",
    "df\n",
    "# len(multi_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add rows to a partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(\n",
    "    \"eos_new_add_episodes\",\n",
    "    engine=\"pyarrow\",\n",
    "    compression=\"snappy\",\n",
    "    ignore_metadata_file=False,\n",
    "    infer_divisions=True,\n",
    ")\n",
    "ddf.index\n",
    "ddf.npartitions\n",
    "ddf.divisions\n",
    "ddf.dtypes\n",
    "\n",
    "# ddf['episodestart'] = ddf['episodestart'].astype('datetime64[ns]')  # error,  not allowed for dask dataframe\n",
    "# ddf.assign(episodestart=ddf['episodestart'].astype('datetime64[ns]'))\n",
    "ddf[\"episodestart__\"] = ddf[\"episodestart__\"].astype(\"datetime64[ns]\")\n",
    "ddf.dtypes\n",
    "ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode4\n",
    "dfs_episode4.index\n",
    "dfs_episode4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodestart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode5 = dfs_episode4.copy()\n",
    "dfs_episode5.index = dfs_episode5.index.set_levels(\n",
    "    [episodestart - pd.Timedelta(10, \"D\")], level=\"episodestart\"\n",
    ")\n",
    "dfs_episode5.index = dfs_episode5.index.set_levels(\n",
    "    [[trucks_by_id[\"MP74\"].vid], [drivers_by_id[\"zheng-longfei\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "# dfs_episode5 = dfs_episode5.swaplevel(0,1, axis=0).swaplevel(1,2,axis=0)\n",
    "dfs_episode5 = dfs_episode5.reset_index(level=[\"vehicle\", \"driver\", \"episodestart\"])\n",
    "\n",
    "dfs_episode5[\"vehicle\"] = dfs_episode5[\"vehicle\"].astype(\"category\")\n",
    "dfs_episode5[\"driver\"] = dfs_episode5[\"driver\"].astype(\"category\")\n",
    "# dfs_episode5['episodestart'] = dfs_episode5['episodestart'].astype('category')\n",
    "dfs_episode5.dtypes\n",
    "\n",
    "\n",
    "# encoding columns index\n",
    "dfs_episode5.columns = [\n",
    "    f\"{x[0]}_{x[1]}_{x[2]}\"\n",
    "    if (x[1] != \"\" and x[2] != \"\")\n",
    "    else f\"{x[0]}_{x[1]}_\"\n",
    "    if (x[1] != \"\" and x[2] == \"\")\n",
    "    else f\"{x[0]}__{x[2]}\"  # !!! dunder!!!\n",
    "    if (x[2] != \"\")\n",
    "    else f\"{x[0]}__\"\n",
    "    for x in dfs_episode5.columns.to_flat_index()\n",
    "]\n",
    "# df_episode_new.index = [\n",
    "#     f'{x[0].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}_{x[1].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}'\n",
    "#     if x[1] != ''\n",
    "#     else f'{x[0].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}'\n",
    "#     for x in dfs_episode5.index.to_flat_index()\n",
    "# ]\n",
    "# df_episode_new.dtypes\n",
    "dfs_episode5\n",
    "dfs_episode5.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode6 = dfs_episode4.copy()\n",
    "dfs_episode6.index = dfs_episode6.index.set_levels(\n",
    "    [episodestart - pd.Timedelta(1, \"D\") - pd.Timedelta(4, \"H\")], level=\"episodestart\"\n",
    ")\n",
    "dfs_episode6.index = dfs_episode6.index.set_levels(\n",
    "    [[trucks_by_id[\"MP73\"].vid], [drivers_by_id[\"wang-cheng\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "# dfs_episode6 = dfs_episode5.swaplevel(0, 1, axis=1).swaplevel(1, 2, axis=0)\n",
    "dfs_episode6 = dfs_episode6.reset_index(level=[\"vehicle\", \"driver\", \"episodestart\"])\n",
    "\n",
    "dfs_episode6[\"vehicle\"] = dfs_episode6[\"vehicle\"].astype(\"category\")\n",
    "dfs_episode6[\"driver\"] = dfs_episode6[\"driver\"].astype(\"category\")\n",
    "# dfs_episode6['episodestart'] = dfs_episode6['episodestart'].astype('category')\n",
    "dfs_episode6.dtypes\n",
    "# encoding columns index\n",
    "dfs_episode6.columns = [\n",
    "    f\"{x[0]}_{x[1]}_{x[2]}\"\n",
    "    if (x[1] != \"\" and x[2] != \"\")\n",
    "    else f\"{x[0]}_{x[1]}_\"\n",
    "    if (x[1] != \"\" and x[2] == \"\")\n",
    "    else f\"{x[0]}__{x[2]}\"  # !!! dunder!!!\n",
    "    if (x[2] != \"\")\n",
    "    else f\"{x[0]}__\"\n",
    "    for x in dfs_episode6.columns.to_flat_index()\n",
    "]\n",
    "# dfs_episode6.index = [\n",
    "#     f'{x[0].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}_{x[1].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}'\n",
    "#     if x[1] != ''\n",
    "#     else f'{x[0].strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}'\n",
    "#     for x in dfs_episode6.index.to_flat_index()\n",
    "# ]\n",
    "# df_episode_new.dtypes\n",
    "dfs_episode6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### encoding single dataframe of multiindex into dask single index dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_read = dd.read_parquet(\n",
    "    \"eos_new_episodes\",\n",
    "    engine=\"pyarrow\",\n",
    "    compression=\"snappy\",\n",
    "    ignore_metadata_file=False,\n",
    "    infer_divisions=True,\n",
    ")\n",
    "# df_all = ddf_read.compute()\n",
    "# df_all\n",
    "# idx = pd.IndexSlice\n",
    "# col_epistart = ddf_read.loc[:, 'episodestart__'].astype('datetime64[ns]').compute()\n",
    "# col_epistart\n",
    "#\n",
    "# ddf_read.loc[:, 'episodestart__'] = ddf_read.loc[:, 'episodestart__'].astype(\n",
    "#     'datetime64[ns]'\n",
    "# )\n",
    "ddf_read[\"episodestart__\"] = ddf_read[\"episodestart__\"].astype(\"datetime64[ns]\")\n",
    "ddf_read.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf_episodes_new = dd.concat([dfs_episode5, dfs_episode6])\n",
    "# ddf_combine = dd.concat([ddf_read, ddf_episodes_new])\n",
    "ddf_read = dd.concat([ddf_read, dfs_episode5, dfs_episode6])\n",
    "ddf_read.compute()\n",
    "ddf_read.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf_list = []\n",
    "# for n in range(ddf_read.npartitions):\n",
    "#     ddf_list.append(ddf_read.partitions[n])\n",
    "#     # ddf_list[n].compute()\n",
    "# len(ddf_list)\n",
    "# type(ddf_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask\n",
    "# df_new_list = [dfs_episode5, dfs_episode6]\n",
    "# df_attach = []\n",
    "# for df in df_new_list:\n",
    "#     bHit = False\n",
    "#     for i, ddf in enumerate(ddf_read.to_delayed()):\n",
    "#         # if dfs_episode_new['vehicle']\n",
    "#         print(f\"{i}\")\n",
    "#         # df = ddf.compute()\n",
    "#         # ddf.columns\n",
    "#         # ddf.dtypes\n",
    "#         # type(ddf)\n",
    "#         vehicle = ddf['vehicle__'].compute()[0]\n",
    "#         driver = ddf['driver__'].compute()[0]\n",
    "#         print(f\"new {df['vehicle__'][0]} in {df['driver__'][0]} vs. pq {vehicle} {driver}\")\n",
    "#\n",
    "#         # df_episode_new = df_episode_new.append(ddf)\n",
    "#         if df['vehicle__'][0] == vehicle and df['driver__'][0] == driver:\n",
    "#             bHit = True\n",
    "#             print(\n",
    "#                 f'hit {i}{\"st\" if i==1 else \"nd\" if i==2 else \"rd\" if i==3 else \"th\"} partition'\n",
    "#             )\n",
    "#             # ddf_list[i] = dd.concat([ddf, df], axis=0)\n",
    "#             # df_attach.append(df)\n",
    "#             # ddf = dd.from_delayed(dask.delayed(pd.concat)([ddf, df], axis=0),meta=ddf_read._meta)  # !!! not working, cannot change the specific partition!\n",
    "#\n",
    "#\n",
    "#     if bHit == False:\n",
    "#         print('no hit')\n",
    "#         df_attach.append(df)\n",
    "#         # append ddf to dfs_episode_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf_read = dd.concat([ddf_read, *df_attach, dfs_episode6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with ProgressBar():\n",
    "        ddf_read.to_parquet(\n",
    "            \"eos_complete_episodes_added\",\n",
    "            engine=\"pyarrow\",\n",
    "            compression=\"snappy\",\n",
    "            partition_on=[\"vehicle__\", \"driver__\", \"episodestart__\"],\n",
    "            write_metadata_file=True,\n",
    "            custom_metadata=custom_metadata,\n",
    "            append=True,\n",
    "        )\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "# append ddf to dfs_episode_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_read.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = [ddf_read.columns == ddf_read_new.columns]\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf_list.append(df_attach[0])\n",
    "# ddf_list[0].dtypes\n",
    "# ddf_all = dd.concat(ddf_list, axis=0, join='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf_all.compute()os.getcwd()\n",
    "custom_metadata = {\n",
    "    b\"eos\": b'{\"length\":[8, 800, 8],\"timezeone\":\"sh\",\"units\":[\"kph\",\"pct\",\"pct\"],\"data_propensity\":[50, 1, 4],\"Dataset Description\": \"MP vehicle from TBox\"}'\n",
    "}\n",
    "\n",
    "try:\n",
    "    with ProgressBar():\n",
    "        ddf_read.to_parquet(\n",
    "            \"eos_combine_episodes_added\",\n",
    "            engine=\"pyarrow\",\n",
    "            compression=\"snappy\",\n",
    "            partition_on=[\"vehicle__\", \"driver__\", \"episodestart__\"],\n",
    "            write_metadata_file=True,\n",
    "            custom_metadata=custom_metadata,\n",
    "        )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(\n",
    "    \"eos_combine_episodes_added\",\n",
    "    engine=\"pyarrow\",\n",
    "    compression=\"snappy\",\n",
    "    ignore_metadata_file=False,\n",
    "    infer_divisions=True,\n",
    ")\n",
    "df = ddf.compute()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf_all.compute()\n",
    "ddf = dd.read_parquet(\n",
    "    \"eos_combine_episodes_added\",\n",
    "    engine=\"pyarrow\",\n",
    "    compression=\"snappy\",\n",
    "    ignore_metadata_file=False,\n",
    "    infer_divisions=True,\n",
    ")\n",
    "pq_meta = pq.read_metadata(\"eos_new_episodes/_common_metadata\")\n",
    "# print(table_meta)\n",
    "# print(table_meta.metadata[b'eos'])\n",
    "custom_meta_info = pq_meta.metadata[b\"eos\"]\n",
    "custom_meta_info = json.loads(custom_meta_info)\n",
    "# custom_meta_info\n",
    "\n",
    "for key, val in custom_meta_info.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "\n",
    "df = ddf.compute()\n",
    "\n",
    "multi_tpl = [tuple(col.split(\"_\")) for col in ddf.columns]\n",
    "multi_col = pd.MultiIndex.from_tuples(multi_tpl)\n",
    "i1 = multi_col.get_level_values(0)\n",
    "i1 = [\n",
    "    \"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else idx for idx in i1\n",
    "]  # convert index of level 2 type to int and '' if NA\n",
    "i2 = multi_col.get_level_values(\n",
    "    1\n",
    ")  # must be null string instead of the default pd.NA or np.nan\n",
    "i2 = [\n",
    "    \"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else idx for idx in i2\n",
    "]  # convert index of level 2 type to int and '' if NA\n",
    "i3 = multi_col.get_level_values(\n",
    "    2\n",
    ")  # must be null string instead of the default pd.NA or np.nan\n",
    "i3 = [\n",
    "    \"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else int(idx) for idx in i3\n",
    "]  # convert index of level 2 type to int and '' if NA\n",
    "\n",
    "multi_col = pd.MultiIndex.from_arrays([i1, i2, i3])\n",
    "multi_col.names = [\"qtuple\", \"rows\", \"idx\"]\n",
    "df.columns = multi_col\n",
    "\n",
    "# multi_idx = [tuple(idx.split('_')) for idx in ddf.index]\n",
    "# multi_idx = pd.MultiIndex.from_tuples(multi_idx)\n",
    "# i0 = multi_idx.get_level_values(\n",
    "#     0\n",
    "# )  # must be null string instead of the default pd.NA or np.nan\n",
    "# i0 = [\n",
    "#     ''\n",
    "#     if str(idx) in (str(pd.NA), 'nan', '')\n",
    "#     else datetime.strptime(idx, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "#     for idx in i0\n",
    "# ]  # convert index of level 2 type to int and '' if NA\n",
    "# i1 = multi_idx.get_level_values(\n",
    "#     1\n",
    "# )  # must be null string instead of the default pd.NA or np.nan\n",
    "# i1 = [\n",
    "#     ''\n",
    "#     if str(idx) in (str(pd.NA), 'nan', '')\n",
    "#     else datetime.strptime(idx, '%Y-%m-%dT%H:%M:%S.%f')\n",
    "#     for idx in i1\n",
    "# ]  # convert index of level 2 type to int and '' if NA\n",
    "# multi_idx = pd.MultiIndex.from_arrays([i0, i1], names=('episodestart', 'timestamp'))\n",
    "# df.index = multi_idx\n",
    "\n",
    "\n",
    "df = df.set_index([\"vehicle\", \"driver\", \"episodestart\", df.index])\n",
    "df\n",
    "df.columns\n",
    "df.index\n",
    "# len(multi_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Quadratuple from parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(\n",
    "    \"eos_new_episodes\",\n",
    "    engine=\"pyarrow\",\n",
    "    compression=\"snappy\",\n",
    "    ignore_metadata_file=False,\n",
    "    infer_divisions=True,\n",
    ")\n",
    "df = ddf.compute()\n",
    "df = df.sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_list = []\n",
    "for n in range(ddf.npartitions):\n",
    "    ddf_list.append(ddf.partitions[n])\n",
    "len(ddf_list)\n",
    "type(ddf_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[[\"vehicle__\", \"driver__\", \"episodestart__\"]].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## slicing from dask dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_slice = pd.date_range(ts, periods=3, freq=\"H\") + pd.Timedelta(10, \"min\")\n",
    "timestamp_slice\n",
    "ep_start = ts - pd.Timedelta(6, \"D\")\n",
    "ep_start\n",
    "# time_slice\n",
    "episode_slice = pd.date_range(ep_start, periods=6, freq=\"D\")\n",
    "episode_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[\"episodestart__\"] = ddf[\"episodestart__\"].astype(\"datetime64[ns]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_selected_episodes = ddf.loc[\n",
    "    (ddf[\"driver__\"] == drivers_by_id[\"wang-cheng\"].pid)\n",
    "    & (ddf[\"vehicle__\"] == trucks_by_id[\"VB7\"].vid)\n",
    "]\n",
    "dfs_selected_episodes.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_selected_episodes = ddf.loc[\n",
    "    (ddf[\"driver__\"] == drivers_by_id[\"zheng-longfei\"].pid)\n",
    "    & (ddf[\"episodestart__\"] < episode_slice[5])\n",
    "    & (ddf[\"episodestart__\"] > episode_slice[0])\n",
    "]\n",
    "episode_slice[5]\n",
    "dfs_selected_episodes.compute()\n",
    "len(dfs_selected_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_selected_episodes = ddf.loc[\n",
    "    (ddf[\"driver__\"] == drivers_by_id[\"zheng-longfei\"].pid)\n",
    "    & (ddf[\"episodestart__\"] < episode_slice[5])\n",
    "    & (ddf[\"episodestart__\"] > episode_slice[0])\n",
    "].loc[timestamp_slice[0] : timestamp_slice[2]]\n",
    "dfs_selected_episodes.compute()\n",
    "len(dfs_selected_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "dfs_selected_episodes = ddf.loc[\n",
    "    (ddf[\"driver__\"] == drivers_by_id[\"wang-cheng\"].pid)\n",
    "    & (ddf[\"vehicle__\"] == trucks_by_id[\"VB7\"].vid)\n",
    "]\n",
    "dfs_selected_episodes.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dfs_selected_episodes.sample(frac=0.4)\n",
    "batch.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_dataframe_from_parquet(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    decode the dataframe from parquet with flat column indices to MultiIndexed DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    multi_tpl = [tuple(col.split(\"_\")) for col in df.columns]\n",
    "    multi_col = pd.MultiIndex.from_tuples(multi_tpl)\n",
    "    i1 = multi_col.get_level_values(0)\n",
    "    i1 = [\n",
    "        \"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else idx for idx in i1\n",
    "    ]  # convert index of level 2 type to int and '' if NA\n",
    "    i2 = multi_col.get_level_values(\n",
    "        1\n",
    "    )  # must be null string instead of the default pd.NA or np.nan\n",
    "    i2 = [\n",
    "        \"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else idx for idx in i2\n",
    "    ]  # convert index of level 2 type to int and '' if NA\n",
    "    i3 = multi_col.get_level_values(\n",
    "        2\n",
    "    )  # must be null string instead of the default pd.NA or np.nan\n",
    "    i3 = [\n",
    "        \"\" if str(idx) in (str(pd.NA), \"nan\", \"\") else int(idx) for idx in i3\n",
    "    ]  # convert index of level 2 type to int and '' if NA\n",
    "\n",
    "    multi_col = pd.MultiIndex.from_arrays([i1, i2, i3])\n",
    "    multi_col.names = [\"qtuple\", \"rows\", \"idx\"]\n",
    "    df.columns = multi_col\n",
    "\n",
    "    df = df.set_index([\"vehicle\", \"driver\", \"episodestart\", df.index])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = batch.compute()\n",
    "batch = encode_dataframe_from_parquet(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch\n",
    "batch.index\n",
    "batch.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.loc[:, idx[\"state\", [\"velocity\", \"thrust\", \"brake\"]]].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"action\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torque_table_row_names = [\"r0\", \"r1\", \"r2\"]\n",
    "batch.loc[:, idx[\"action\", torque_table_row_names]].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.loc[:, idx[\"nstate\", [\"velocity\", \"thrust\", \"brake\"]]].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.loc[:, idx[\"reward\", \"work\"]].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = ddf.drop(dfs_selected_episodes.index)\n",
    "ddf.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Quadratuple from parquet files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vehicle = \"VB7\"\n",
    "target_driver = \"wang-cheng\"\n",
    "partition = []\n",
    "for i, ddf in enumerate(ddf_list):\n",
    "    # if dfs_episode_new['vehicle']\n",
    "    print(f\"{i}\")\n",
    "    # df = ddf.compute()\n",
    "    # ddf.columns\n",
    "    # ddf.dtypes\n",
    "    vehicle = ddf[\"vehicle__\"].compute()[0]\n",
    "    driver = ddf[\"driver__\"].compute()[0]\n",
    "    # df_episode_new = df_episode_new.append(ddf)\n",
    "    if target_vehicle == vehicle and target_driver == driver:\n",
    "        bHit = True\n",
    "        print(\n",
    "            f'hit {i}{\"st\" if i==1 else \"nd\" if i==2 else \"rd\" if i==3 else \"th\"} partition'\n",
    "        )\n",
    "        # k = i\n",
    "        partition.append(i)\n",
    "\n",
    "        # ddf_list[i] = dd.concat([ddf, df], axis=0)\n",
    "        # append ddf to dfs_episode_new\n",
    "\n",
    "if bHit == False:\n",
    "    print(\"no hit\")\n",
    "    # ddf_list.append(df_episode_new)\n",
    "    # append ddf to dfs_episode_new\n",
    "\n",
    "partition\n",
    "ddf_target = dd.concat([ddf_list[i] for i in partition], axis=0)\n",
    "# ddf_target\n",
    "# # ddf_target.head()\n",
    "df_target = ddf_target.compute()\n",
    "df_target\n",
    "# # df_target\n",
    "\n",
    "# #     c_ep = df_episode_new['ep\n",
    "# #     dfs_episode_new = dfs_episode_newappend(ddf)\n",
    "# # print(f\"splicing dfs_episode_new {c_ep} with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple_sample = ddf_target.sample(frac=0.2).compute()\n",
    "tuple_sample[\"episodestart__\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_target.sample(frac=0.2).compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
