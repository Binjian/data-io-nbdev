{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "\n",
    "class display(object):\n",
    "    \"\"\"Display HTML representation of multiple objects\"\"\"\n",
    "\n",
    "    template = \"\"\"<div style=\"float: left; padding: 10px;\">\n",
    "    <p style='font-family:\"Courier New\", Courier, monospace'>{0}</p>{1}\n",
    "    </div>\"\"\"\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "\n",
    "    def _repr_html_(self):\n",
    "        return \"\\n\".join(\n",
    "            self.template.format(a, eval(a)._repr_html_()) for a in self.args\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"\\n\\n\".join(a + \"\\n\" + repr(eval(a)) for a in self.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "a\n",
    "\n",
    "b = a.flatten()\n",
    "b\n",
    "c = a.flatten(\"F\")\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eos.data_io.pool.parquet_pool import ParquetPool\n",
    "from eos.data_io.pool.avro_pool import AvroPool\n",
    "from eos.data_io.pool.dask_pool import DaskPool\n",
    "from eos.data_io.pool.mongo_pool import MongoPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eos.data_io.buffer.mongo_buffer import MongoBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eos.data_io.buffer.dask_buffer import DaskBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import dataclass, field\n",
    "# from typing import Optional\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from ordered_set import OrderedSet\n",
    "from eos.data_io.config import drivers, trucks_by_id, drivers_by_id\n",
    "\n",
    "# DriverSet = OrderedSet(\n",
    "#     [\n",
    "#         'wang-cheng',\n",
    "#         'li-changlong',\n",
    "#         'chen-hongmei',\n",
    "#         'zheng-longfei',\n",
    "#     ]\n",
    "# )\n",
    "# DriverSet[0]\n",
    "# DriverSet.get_loc('wang-cheng')\n",
    "# DriverSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eos.data_io.eos_struct import StateSpecsCloud, StateSpecsECU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eos.data_io.eos_struct.eos_data import StateSpecs\n",
    "\n",
    "state_specs = StateSpecs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "from eos.data_io.eos_struct.eos_data import (\n",
    "    StateSpecsCloud,\n",
    "    StateSpecsECU,\n",
    "    CloudMixin,\n",
    "    ECUMixin,\n",
    ")\n",
    "from eos.data_io.eos_struct.eos_data import ActionSpecs, RewardSpecs\n",
    "\n",
    "state_specs = StateSpecsCloud(cloud_interface=CloudMixin())\n",
    "print(asdict(state_specs))\n",
    "\n",
    "action_specs = ActionSpecs()\n",
    "reward_specs = RewardSpecs()\n",
    "# state_specs = StateSpecsECU(ecu_interface=ECUMixin())\n",
    "# print(asdict(state_specs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eos.data_io.eos_struct.eos_data import ObservationMetaCloud, ObservationMetaECU\n",
    "\n",
    "# observation_meta = ObservationMetaCloud(state_specs=StateSpecsCloud(cloud_interface=CloudMixin()))\n",
    "# print(asdict(observation_meta))\n",
    "observation_meta = ObservationMetaCloud(\n",
    "    state_specs=state_specs,\n",
    "    action_specs=action_specs,\n",
    "    reward_specs=reward_specs,\n",
    "    site=\"Anting\",\n",
    ")\n",
    "type(observation_meta)\n",
    "observation_meta.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "ts = pd.to_datetime(datetime.now())\n",
    "ts_ind = ts + pd.to_timedelta(np.arange(2), \"H\")\n",
    "ts_ind\n",
    "df = pd.DataFrame(a, index=ts_ind, columns=[\"c1\", \"c2\", \"c3\"])\n",
    "df\n",
    "df.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts\n",
    "ts_end = pd.to_datetime(datetime.now())\n",
    "ts_end\n",
    "span = ts_end - ts\n",
    "span\n",
    "span_each_row = span / 4\n",
    "span_each_row\n",
    "np.linspace(0, 4, 5)\n",
    "ts_ser = ts + pd.to_timedelta(np.linspace(1, 4, 4) * span_each_row, unit=\"s\")\n",
    "ts_ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts = pd.DataFrame(np.array([ts]), columns=[\"timestamp\"])\n",
    "df_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.arange(10)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = np.arange(12)\n",
    "a1 = ss[:4].tolist()\n",
    "a2 = ss[4:8].tolist()\n",
    "a3 = ss[8:].tolist()\n",
    "ss = [a1, a2, a3]\n",
    "ss\n",
    "df_s = pd.DataFrame(ss, columns=[\"t\", \"v\", \"p\", \"b\"])  # .set_index('t')\n",
    "df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(0, 4 * 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_ind = ts + pd.to_timedelta(np.arange(0, 4 * 20, 20), \"ms\")\n",
    "ts_ind\n",
    "ss = np.arange(12)\n",
    "a1 = ss[:4]\n",
    "a2 = ss[4:8]\n",
    "a3 = ss[8:]\n",
    "df_ss = pd.DataFrame(\n",
    "    {\"timestep\": ts_ind, \"velocity\": a1, \"thrust\": a2, \"brake\": a3}\n",
    ")  # .set_index('timestep')\n",
    "df_ss.columns.name = \"qtuple\"\n",
    "df_ss\n",
    "df_ss[[\"velocity\", \"thrust\", \"brake\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa_ss_flatten = df_ss[[\"velocity\", \"thrust\", \"brake\"]].to_numpy().flatten()\n",
    "npa_ss_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui_t = df_ss.loc[:, [\"velocity\", \"thrust\"]]\n",
    "ui_t\n",
    "power_t = ui_t.prod(axis=1)\n",
    "power_t\n",
    "power_t.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = df_ss.stack().swaplevel(0, 1)\n",
    "state.name = \"state\"\n",
    "state.index.names = [\"rows\", \"idx\"]\n",
    "state.sort_index(inplace=True)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state[[\"velocity\", \"thrust\", \"brake\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import reduce\n",
    "#\n",
    "#\n",
    "# velocity = pd.Series(ss[:4], name='velocity')\n",
    "# thrust = pd.Series(ss[4:8], name='thrust')\n",
    "# brake = pd.Series(ss[8:], name='brake')\n",
    "# series = [velocity, thrust, brake]\n",
    "# state = reduce(\n",
    "#     lambda x, y: pd.merge(x, y, how='outer', left_index=True, right_index=True), series\n",
    "# )\n",
    "# state = state.stack().swaplevel(0, 1).sort_index()\n",
    "# state.name = 'state'\n",
    "# state\n",
    "# len(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import reduce\n",
    "#\n",
    "#\n",
    "# velocity = pd.Series(s[:4], name='velocity')\n",
    "# thrust = pd.Series(s[4:7], name='thrust')\n",
    "# brake = pd.Series(s[7:], name='brake')\n",
    "# series = [velocity, thrust, brake]\n",
    "# state = reduce(\n",
    "#     lambda x, y: pd.merge(x, y, how='outer', left_index=True, right_index=True), series\n",
    "# )\n",
    "# state = state.stack().swaplevel(0, 1).sort_index()\n",
    "# state.name = 'state'\n",
    "# state\n",
    "# len(state)\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "a = len(ss) + np.arange(15)\n",
    "speed_ser = pd.Series(np.linspace(40, 60, 3), name=\"speed\")\n",
    "# row_ser\n",
    "row_dict = {f\"r{i}\": a[i * 5 : i * 5 + 5] for i in np.arange(3)}\n",
    "row_dict\n",
    "row_array = a.reshape(3, 5).transpose()\n",
    "row_array\n",
    "rows_df = pd.DataFrame(row_array)\n",
    "rows_df.columns = [f\"r{i}\" for i in np.arange(3)]\n",
    "rows_df\n",
    "\n",
    "action_row_names = [f\"r{i}\" for i in np.arange(3)]\n",
    "action_row_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_ind = ts + pd.to_timedelta(np.arange(5 * 20, 8 * 20, 20), \"ms\")\n",
    "ts_ser = pd.Series(ts_ind, name=\"timestep\")\n",
    "throttle_ser = pd.Series(np.linspace(0, 1.0, 5), name=\"throttle\")\n",
    "# throttle_ser\n",
    "dfs = [rows_df, ts_ser, speed_ser, throttle_ser]\n",
    "action = (\n",
    "    reduce(\n",
    "        lambda left, right: pd.merge(\n",
    "            left, right, how=\"outer\", left_index=True, right_index=True\n",
    "        ),\n",
    "        dfs,\n",
    "    )\n",
    "    .stack()\n",
    "    .swaplevel(0, 1)\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "action.name = \"action\"\n",
    "action.index.names = [\"rows\", \"idx\"]\n",
    "action.index\n",
    "action\n",
    "# action = pd.concat([df.DataFrame(row_dict),df.Series(row_ind),df.Series(ts_ind),df.Series(pedal_ind)],axis=1)\n",
    "# action\n",
    "# pedal_ind\n",
    "# action_ind = pd.MultiIndex.from_product([row_ind, ts_ind], names=['row', 'timestamp'])\n",
    "# a\n",
    "# speed_row, row_name = *row_dict\n",
    "# action = (\n",
    "#     pd.DataFrame(\n",
    "#         {\n",
    "#             **row_dict,\n",
    "#             'throttle': pedal_ind,\n",
    "#             'speed': [r[1] for r in row_ind],\n",
    "#         }\n",
    "#     )  # .set_index('pedal')\n",
    "#     # .stack()\n",
    "#     # .swaplevel(0, 1)\n",
    "#     .sort_index()\n",
    "# )\n",
    "# action\n",
    "# action.columns\n",
    "# action.columns = pd.MultiIndex.from_arrays(\n",
    "#     [action.columns, ts_ind], names=['nominalspeed', 'timestep']\n",
    "# )\n",
    "# action = action.stack([0, 1]).swaplevel(0, 1).swaplevel(1, 2).sort_index()\n",
    "# action.name = 'action'\n",
    "# # action.index.names = ['nominalspeed', 'timestamp']\n",
    "# # action.index\n",
    "# action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action[[\"r0\", \"r1\", \"r2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = len(state) + np.arange(6)\n",
    "# ar0 = pd.Series(a[:2], name='row0')\n",
    "# ar1 = pd.Series(a[2:4], name='row1')\n",
    "# ar2 = pd.Series(a[4:], name='row2')\n",
    "# series = [ar0, ar1, ar2]\n",
    "# action = reduce(\n",
    "#     lambda x, y: pd.merge(x, y, how='outer', left_index=True, right_index=True), series\n",
    "# )\n",
    "# action = action.stack().swaplevel(0, 1).sort_index()\n",
    "# action.name = 'action'\n",
    "# action\n",
    "# len(action) + len(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = (\n",
    "    pd.DataFrame({\"work\": len(ss) + len(a), \"timestep\": ts_ind[0]}, index=[0])\n",
    "    .stack()\n",
    "    .swaplevel(0, 1)\n",
    "    .sort_index()\n",
    ")\n",
    "# reward_index = (reward.name,  ts_ind[0], 0)\n",
    "reward.index.names = [\"rows\", \"idx\"]\n",
    "reward.name = \"reward\"\n",
    "reward.index\n",
    "reward\n",
    "# reward.name = 'reward'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward[\"work\"][0]\n",
    "reward.loc[(\"work\", 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# velocity = pd.Series(len(state) + len(action) + len(reward) + s[:4], name='velocity')\n",
    "# thrust = pd.Series(len(state) + len(action) + len(reward) + s[4:7], name='thrust')\n",
    "# brake = pd.Series(len(state) + len(action) + len(reward) + s[7:], name='brake')\n",
    "# series = [velocity, thrust, brake]\n",
    "# nstate = reduce(\n",
    "#     lambda x, y: pd.merge(x, y, how='outer', left_index=True, right_index=True), series\n",
    "# )\n",
    "# nstate = nstate.stack().swaplevel(0, 1).sort_index()\n",
    "# nstate.name = 'nstate'\n",
    "# nstate\n",
    "# len(nstate)\n",
    "ts_ind = ts + pd.to_timedelta(5, \"s\") + pd.to_timedelta(np.arange(0, 4 * 20, 20), \"ms\")\n",
    "ts_ind\n",
    "ss = np.arange(12) + len(ss) + len(a) + len(reward)\n",
    "a1 = ss[:4]\n",
    "a2 = ss[4:8]\n",
    "a3 = ss[8:]\n",
    "nstate = (\n",
    "    pd.DataFrame({\"timestep\": ts_ind, \"velocity\": a1, \"thrust\": a2, \"brake\": a3})\n",
    "    # .set_index('timestamp')\n",
    "    .stack()\n",
    "    .swaplevel(0, 1)\n",
    "    .sort_index()\n",
    ")\n",
    "nstate.name = \"nstate\"\n",
    "nstate.index.names = [\"rows\", \"idx\"]\n",
    "len(nstate)\n",
    "nstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = pd.Series([ts], name=\"timestamp\")\n",
    "timestamp.index = pd.MultiIndex.from_product(\n",
    "    [timestamp.index, [0]], names=[\"rows\", \"idx\"]\n",
    ")\n",
    "timestamp.index\n",
    "state.index\n",
    "action.index\n",
    "reward.index\n",
    "nstate.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_index = (timestamp.name, \"\", 0)\n",
    "# timestamp_index\n",
    "state_index = [(state.name, *i) for i in state.index]\n",
    "# state_index\n",
    "reward_index = [(reward.name, *i) for i in reward.index]\n",
    "# reward_index\n",
    "action_index = [(action.name, *i) for i in action.index]\n",
    "# action_index\n",
    "nstate_index = [(nstate.name, *i) for i in nstate.index]\n",
    "# nstate_index\n",
    "\n",
    "multiindex = pd.MultiIndex.from_tuples(\n",
    "    [timestamp_index, *state_index, *action_index, *reward_index, *nstate_index]\n",
    ")\n",
    "multiindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp.index\n",
    "timestamp.values\n",
    "timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_list = [timestamp, state, action, reward, nstate]\n",
    "observation = pd.concat(observation_list)\n",
    "observation.index = multiindex\n",
    "observation\n",
    "# observation.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "# observation_sorted = observation.sort_index()\n",
    "actions = (\n",
    "    observation.sort_index()\n",
    "    .loc[idx[\"action\", action_row_names, :]]\n",
    "    .values.astype(np.float32)\n",
    ")\n",
    "actions\n",
    "actions.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "actions\n",
    "last_actions = tf.expand_dims(tf.expand_dims(actions, axis=0), axis=0)\n",
    "last_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = (\n",
    "    observation.sort_index()\n",
    "    .loc[idx[\"state\", [\"velocity\", \"thrust\", \"brake\"], :]]\n",
    "    .values.astype(np.float32)\n",
    ")\n",
    "states = tf.expand_dims(\n",
    "    tf.expand_dims(tf.convert_to_tensor(states, dtype=tf.float32), axis=0), axis=0\n",
    ")\n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation0 = observation.copy()\n",
    "observation0.loc[\"timestamp\", \"\", 0] = ts + pd.Timedelta(1, \"h\")\n",
    "observation1 = observation.copy()\n",
    "observation1.loc[\"timestamp\", \"\", 0] = ts + pd.Timedelta(2, \"h\")\n",
    "observation2 = observation.copy()\n",
    "observation2.loc[\"timestamp\", \"\", 0] = ts + pd.Timedelta(3, \"h\")\n",
    "observation3 = observation.copy()\n",
    "observation3.loc[\"timestamp\", \"\", 0] = ts + pd.Timedelta(4, \"h\")\n",
    "observation4 = observation.copy()\n",
    "observation4.loc[\"timestamp\", \"\", 0] = ts + pd.Timedelta(5, \"h\")\n",
    "observation_list = [\n",
    "    observation0,\n",
    "    observation1,\n",
    "    observation2,\n",
    "    observation3,\n",
    "    observation4,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_epi = pd.concat(observation_list, axis=1).transpose()\n",
    "dfs_epi.columns.names = [\"qtuple\", \"rows\", \"idx\"]\n",
    "dfs_epi.columns\n",
    "dfs_epi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode1 = dfs_epi.copy().sort_index(axis=1)\n",
    "dfs_episode1.set_index((\"timestamp\", \"\", 0), inplace=True)\n",
    "dfs_episode1.index.name = \"timestamp\"\n",
    "idx = pd.IndexSlice\n",
    "dfs_episode1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame with MultiIndex columns\n",
    "data = {\"col1\": [\"1\", \"2\", \"3\"], \"col2\": [\"4.5\", \"5.6\", \"6.7\"]}\n",
    "df = pd.DataFrame(data)\n",
    "df\n",
    "df.index\n",
    "df.columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'col1' to integer and 'col2' to float\n",
    "df.columns = pd.MultiIndex.from_tuples([(\"A\", \"col1\"), (\"B\", \"col2\")])\n",
    "df[(\"A\", \"col1\")] = df[(\"A\", \"col1\")].astype(int)\n",
    "df[(\"B\", \"col2\")] = df[(\"B\", \"col2\")].astype(float)\n",
    "\n",
    "# Check the updated data types\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"user1\": [2, 4, 21, 21],\n",
    "        \"user2\": [6, 13, 76, 76],\n",
    "        \"param1\": [0, 2, 0, 1],\n",
    "        \"param2\": [\"x\", \"a\", \"a\", \"d\"],\n",
    "        \"count\": [1, 3, 2, 1],\n",
    "    },\n",
    "    columns=[\"user1\", \"user2\", \"param1\", \"param2\", \"count\"],\n",
    ")\n",
    "df\n",
    "df = df.set_index([\"user1\", \"user2\", \"param1\", \"param2\"])\n",
    "df\n",
    "df = df.unstack([2, 3]).sort_index(axis=1)\n",
    "df\n",
    "df2 = pd.DataFrame({\"user1\": [2, 5, 21], \"user2\": [6, 18, 76]})\n",
    "df2.columns = pd.MultiIndex.from_product([df2.columns, [\"\"], [\"\"]])\n",
    "df2\n",
    "final_df = df2.merge(df, on=[\"user1\", \"user2\"], how=\"outer\").fillna(0)\n",
    "final_df\n",
    "final_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[:, idx[\"count\", [0, 1, 2], [\"a\", \"x\", \"d\", \"a\"]]].astype(int)\n",
    "final_df.loc[:, idx[\"count\", [0, 1, 2], [\"a\", \"x\", \"d\", \"a\"]]] = final_df.loc[\n",
    "    :, idx[\"count\", [0, 1, 2], [\"a\", \"x\", \"d\", \"a\"]]\n",
    "].astype(int)\n",
    "final_df\n",
    "final_df.dtypes\n",
    "final_df1 = final_df.copy()\n",
    "final_df1.loc[:, idx[\"count\", [0, 1, 2], [\"a\", \"x\", \"d\", \"a\"]]] = \"a\"\n",
    "final_df1\n",
    "final_df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_actions = dfs_episode1.loc[:, idx[\"action\", :, :]]\n",
    "dfs_actions.dtypes\n",
    "dfs_actions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_actions.loc[:, idx['action', ['r0','r1','r2'],0:5]] = dfs_actions.loc[\n",
    "#                 :, idx['action', ['r0','r1','r2'],0:5]].astype(int)\n",
    "# dfs_actions.loc[:, idx['action', ['r0','r1','r2'],0:5]].astype(int, copy=False)\n",
    "# dfs_actions.dtypes\n",
    "\n",
    "dfs_actions[(\"action\", \"r0\", 0)].astype(int)\n",
    "# dfs_actions.loc[:, ('action', 'r0', 0)]\n",
    "dfs_actions.loc[:, (\"action\", \"r0\", 0)] = dfs_actions[(\"action\", \"r0\", 0)].astype(int)\n",
    "dfs_actions[(\"action\", \"r0\", 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode2 = dfs_episode1.copy()\n",
    "dfs_episode2\n",
    "dfs_episode2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_actions = dfs_episode2.loc[\n",
    "    :, idx[\"action\", [\"r0\", \"r1\", \"r2\", \"speed\", \"throttle\"], :]\n",
    "].astype(float)\n",
    "dfs_actions\n",
    "dfs_actions.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode2.loc[\n",
    "    :, idx[\"action\", [\"r0\", \"r1\", \"r2\", \"speed\", \"throttle\"], :]\n",
    "] = dfs_actions\n",
    "dfs_episode2\n",
    "dfs_episode2.dtypes\n",
    "dfs_actions1 = dfs_episode2.loc[\n",
    "    :, idx[\"action\", [\"r0\", \"r1\", \"r2\", \"speed\", \"throttle\"], :]\n",
    "]\n",
    "dfs_actions1\n",
    "dfs_actions1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_actions\n",
    "dfs_actions.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode2.loc[:, idx[\"action\", \"r0\", :]] = dfs_episode2.loc[\n",
    "    :, idx[\"action\", \"r0\", :]\n",
    "].astype(int)\n",
    "dfs_episode2.dtypes\n",
    "dfs_episode2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode1.loc[\n",
    "    :, idx[\"action\", [\"r0\", \"r1\", \"r2\", \"speed\", \"throttle\"], :]\n",
    "] = dfs_episode1.loc[\n",
    "    :, idx[\"action\", [\"r0\", \"r1\", \"r2\", \"speed\", \"throttle\"], :]\n",
    "].astype(\n",
    "    \"float\"\n",
    ")\n",
    "dfs_episode1.loc[\n",
    "    :, idx[\"state\", [\"brake\", \"thrust\", \"velocity\"], :]\n",
    "] = dfs_episode1.loc[:, idx[\"state\", [\"brake\", \"thrust\", \"velocity\"], :]].astype(\n",
    "    \"float\"\n",
    ")\n",
    "dfs_episode1.loc[\n",
    "    :, idx[\"nstate\", [\"brake\", \"thrust\", \"velocity\"], :]\n",
    "] = dfs_episode1.loc[:, idx[\"nstate\", [\"brake\", \"thrust\", \"velocity\"], :]].astype(\n",
    "    \"float\"\n",
    ")\n",
    "dfs_episode1\n",
    "dfs_episode1.dtypes\n",
    "dfs_episode1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert columns types to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_epi\n",
    "dfs_episode = dfs_epi.copy()\n",
    "dfs_episode.index\n",
    "dfs_episode.set_index((\"timestamp\", \"\", 0), inplace=True)\n",
    "dfs_episode.sort_index(axis=1, inplace=True)\n",
    "dfs_episode.index\n",
    "dfs_episode\n",
    "dfs_episode.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode.index.name = \"timestamp\"\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# # dfs_episode = dfs_episode.astype({idx['state', 'velocity', :]: 'int'})  # Error unhashable type: 'slice' for dictionary\n",
    "# # sorted_idx = **(idx['state', :, :]:'int')\n",
    "# # sorted_idx\n",
    "# dfs_episode.loc[:, idx['state', ['brake', 'thrust', 'velocity'], :]] = dfs_episode.loc[\n",
    "#     :, idx['state', ['brake', 'thrust', 'velocity'], :]\n",
    "# ].astype(\n",
    "#     'float'\n",
    "# )  # float16 not allowed in parquet\n",
    "#\n",
    "# dfs_episode.loc[:, idx['action', ['r0', 'r1', 'r2'], :]] = dfs_episode.loc[\n",
    "#     :, idx['action', ['r0', 'r1', 'r2'], :]\n",
    "# ].astype(\n",
    "#     'float'\n",
    "# )  # float16 not allowed in parquet\n",
    "# # dfs_episode['action'] = dfs_episode['action'].astype('float')\n",
    "# # dfs_episode['reward'] = dfs_episode['reward'].astype('float')\n",
    "# dfs_episode.loc[:, idx['reward', 'work', 0]] = dfs_episode.loc[\n",
    "#     :, idx['reward', 'work', 0]\n",
    "# ].astype(\n",
    "#     'float'\n",
    "# )  # float16 not allowed in parquet\n",
    "#\n",
    "# # dfs_episode['nstate'] = dfs_episode['nstate'].astype('float')\n",
    "# dfs_episode.loc[:, idx['nstate', ['brake', 'thrust', 'velocity'], :]] = dfs_episode.loc[\n",
    "#     :, idx['nstate', ['brake', 'thrust', 'velocity'], :]\n",
    "# ].astype(\n",
    "#     'float'\n",
    "# )  # float16 not allowed in parquet\n",
    "state_cols_float = [(\"state\", col) for col in [\"brake\", \"thrust\", \"velocity\"]]\n",
    "action_cols_float = [(\"action\", col) for col in [\"r0\", \"r1\", \"r2\", \"speed\", \"throttle\"]]\n",
    "reward_cols_float = [(\"reward\", \"work\")]\n",
    "nstate_cols_float = [(\"nstate\", col) for col in [\"brake\", \"thrust\", \"velocity\"]]\n",
    "for col in action_cols_float + state_cols_float + reward_cols_float + nstate_cols_float:\n",
    "    dfs_episode[col[0], col[1]] = dfs_episode[col[0], col[1]].astype(\n",
    "        \"float\"\n",
    "    )  # float16 not allowed in parquet\n",
    "# dfs_episode['action','timestep'] = dfs_episode['action','timestep'].astype(\n",
    "#     'datetime64[ns]'\n",
    "# )  # float16 not allowed in parquet\n",
    "# dfs_episode['state','timestep'] = dfs_episode['state', 'timestep'].astype(\n",
    "#     'datetime64[ns]'\n",
    "# )  # float16 not allowed in parquet\n",
    "# dfs_episode['nstate','timestep'] = dfs_episode['nstate', 'timestep'].astype(\n",
    "#     'datetime64[ns]'\n",
    "# )  # float16 not allowed in parquet\n",
    "dfs_episode\n",
    "dfs_episode.dtypes\n",
    "# dfs_episode.columns\n",
    "# dfs_epi\n",
    "# dfs_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepend two levels of index \"vehicle\" and \"driver\" to the DataFrame object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode = pd.concat(\n",
    "    [dfs_episode], keys=[drivers_by_id[\"wang-cheng\"].pid], names=[\"driver\"]\n",
    ")\n",
    "dfs_episode = pd.concat(\n",
    "    [dfs_episode], keys=[trucks_by_id[\"VB7\"].vid], names=[\"vehicle\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode.index\n",
    "dfs_episode.index.dtypes\n",
    "dfs_episode.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_episode.columns.set_names(\n",
    "#     [\n",
    "#         'qtuple',\n",
    "#         'rows',\n",
    "#         'idx',\n",
    "#     ],\n",
    "#     level=[0, 1, 2],\n",
    "#     inplace=True,\n",
    "# )\n",
    "dfs_episode\n",
    "dfs_episode.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs_episode['state', 'thrust'] = dfs_episode['state', 'thrust'].astype('float')\n",
    "# # dfs_episode.loc[:, idx['state', ['brake', 'thrust', 'velocity'], :]] = dfs_episode.loc[\n",
    "# #     :, idx['state', ['brake', 'thrust', 'velocity'], :]\n",
    "# # ].astype(\n",
    "# #     'float'\n",
    "# # )  # float16 not allowed in parquet\n",
    "# dfs_episode.loc[:, ('state', ['brake', 'thrust', 'velocity'])] = dfs_episode.loc[\n",
    "#     :, ('state', ['brake', 'thrust', 'velocity'])\n",
    "# ].astype(\n",
    "#     'float'\n",
    "# )  # float16 not allowed in parquet\n",
    "# dfs_episode['action', 'r0'] = dfs_episode['action', 'r0'].astype(\n",
    "#     'float'\n",
    "# )  # float16 not allowed in parquet\n",
    "#\n",
    "# dfs_episode.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = pd.IndexSlice\n",
    "# dfs_episode.loc[:, idx['state', :, :]] = dfs_episode.loc[:, idx['state', :, :]].astype(\n",
    "#     'int'\n",
    "# )\n",
    "# dfs_episode.loc[:, idx['action', :, :]] = dfs_episode.loc[\n",
    "#     :, idx['action', :, :]\n",
    "# ].astype('float16')\n",
    "# dfs_episode.loc[:, idx['reward', :, :]] = dfs_episode.loc[\n",
    "#     :, idx['reward', :, :]\n",
    "# ].astype('float16')\n",
    "# dfs_episode.loc[:, idx['nstate', :, :]] = dfs_episode.loc[\n",
    "#     :, idx['nstate', :, :]\n",
    "# ].astype('float16')\n",
    "#\n",
    "# vel_1 = dfs_episode[[('state', 'velocity', 1)]]\n",
    "# vel_1.dtypes\n",
    "# vel_1.index\n",
    "# vel_1.values\n",
    "# # type(vel_1)\n",
    "# vel_1.iloc[0]\n",
    "# type(vel_1.iloc[0])\n",
    "# type(vel_1.iloc[0].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a level of index for episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodestart = ts - pd.Timedelta(1, \"h\")\n",
    "dfs_episode = pd.concat([dfs_episode], keys=[ts], names=[\"episodestart\"])\n",
    "dfs_episode = dfs_episode.swaplevel(1, 0, axis=0)\n",
    "dfs_episode = dfs_episode.swaplevel(1, 2, axis=0)\n",
    "dfs_episode.sort_index(inplace=True)\n",
    "dfs_episode.index\n",
    "dfs_episode.columns\n",
    "dfs_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vb7 = trucks_by_id[\"VB7\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding MultiIndexed DataFrame to documents array of MongoDB record type (nested dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eos.data_io.eos_struct import (\n",
    "    ObservationMetaCloud,\n",
    "    StateSpecs,\n",
    "    StateSpecsCloud,\n",
    "    StateUnitCodes,\n",
    "    ActionSpecs,\n",
    "    RewardSpecs,\n",
    ")\n",
    "\n",
    "state_specs_cloud = StateSpecsCloud()\n",
    "observation_meta = ObservationMetaCloud(\n",
    "    state_specs=state_specs_cloud,\n",
    "    action_specs=action_specs,\n",
    "    reward_specs=reward_specs,\n",
    "    site=\"Anting\",\n",
    ")\n",
    "#     state_specs=StateSpecs(\n",
    "#         interface='cloud',\n",
    "#         state_unit_codes=StateUnitCodes(\n",
    "#             state_unit_code='kph',\n",
    "#             thrust_unit_code='pct',\n",
    "#             brake_unit_code='pct',\n",
    "#         ),\n",
    "#         state_number =3,\n",
    "#         unit_number_per_state = vb7.state_number_per_state,\n",
    "#     ),\n",
    "#     action_specs=ActionSpecs(\n",
    "#         action_unit_code='nm',\n",
    "#         action_row_number=trucks_by_id['VB7'].torque_table_row_num_flash,\n",
    "#         action_column_number=len(trucks_by_id['VB7'].pedal_scale),\n",
    "#     ),\n",
    "#     reward_specs=RewardSpecs(\n",
    "#         reward_unit_code='wh',\n",
    "#         reward_number=1\n",
    "#     ),\n",
    "#     site=trucks_by_id['VB7'].site,\n",
    "# )\n",
    "observation_meta\n",
    "observation_meta.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode\n",
    "#\n",
    "# records = dfs_episode.to_dict('records')\n",
    "# # records\n",
    "# record_nested = [[{key: value} for key, value in record.items()] for record in records]\n",
    "# record_nested\n",
    "# ser = dfs_episode.to_dict('split')\n",
    "# ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nest(d: dict) -> dict:\n",
    "    result = {}\n",
    "    for key, value in d.items():\n",
    "        target = result\n",
    "        for k in key[:-1]:\n",
    "            target = target.setdefault(k, {})\n",
    "        target[key[-1]] = value\n",
    "    return result\n",
    "\n",
    "\n",
    "def df_to_nested_dict(df_multi_indexed_col: pd.DataFrame) -> dict:\n",
    "    d = df_multi_indexed_col.to_dict(\"index\")\n",
    "    return {k: nest(v) for k, v in d.items()}\n",
    "\n",
    "\n",
    "def eos_df_to_nested_dict(episode: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a eos dataframe with multi-indexed columns to a nested dictionary\n",
    "    Remove all the levels of the multi-indexed columns except for 'timestamp'\n",
    "    Keep only the timestamp as the single key for the nested dictionary\n",
    "    \"\"\"\n",
    "    dict_nested = df_to_nested_dict(\n",
    "        episode\n",
    "    )  # for multi-indexed dataframe, the index in the first level of the dictionary is still a tuple!\n",
    "    indices_dict = [\n",
    "        {episode.index.names[i]: level for i, level in enumerate(levels)}\n",
    "        for levels in episode.index\n",
    "    ]  # all elements in the array should have the same vehicle, driver, episodestart\n",
    "    single_key_dict = {\n",
    "        idx[\"timestamp\"]: dict_nested[key]\n",
    "        for idx, key in zip(indices_dict, dict_nested)\n",
    "    }\n",
    "\n",
    "    return single_key_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_nested = df_to_nested_dict(dfs_episode)\n",
    "dict_nested\n",
    "indices_dict = [\n",
    "    {dfs_episode.index.names[i]: level for i, level in enumerate(levels)}\n",
    "    for levels in dfs_episode.index\n",
    "]  # all elements in the array should have the same vehicle, driver, episodestart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_nested = [{i: dfs_episode.xs(i).to_dict('index') for i in level} for level in dfs_episode.index.levels]\n",
    "# dfs_index = dfs_episode.to_dict('index')\n",
    "# dfs_index\n",
    "\n",
    "\n",
    "# dfs_episode\n",
    "# dfs_episode.columns\n",
    "\n",
    "# dict1 = {key: nest(value) for key, value in dfs_episode.to_dict('index').items()}\n",
    "# dict1\n",
    "\n",
    "dict_nested = eos_df_to_nested_dict(dfs_episode)\n",
    "dict_nested\n",
    "# dict_nested = {level:\n",
    "#                    {i: dfs_episode.xs(i).to_dict('index') for i in level }\n",
    "#                for level in dfs_episode.index.levels}\n",
    "# dict_nested = {level: dfs_episode.xs(level).to_dict('index')\n",
    "#                for level in dfs_episode.columns.levels[0]}\n",
    "# dict_nested\n",
    "len(dict_nested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_df_xs = {level: dfs_episode.xs(level) for level in dfs_episode.index.levels[0]}\n",
    "# dict_df_xs['VB7']\n",
    "len(dict_nested)\n",
    "dfs_episode\n",
    "dict_nested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = dfs_episode.index\n",
    "indices.name\n",
    "indices.names\n",
    "indices\n",
    "indices_dict = [\n",
    "    {indices.names[i]: level for i, level in enumerate(levels)} for levels in indices\n",
    "]\n",
    "indices_dict\n",
    "len(indices_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_key_dict = [\n",
    "    (idx[\"timestamp\"], dict_nested[key]) for idx, key in zip(indices_dict, dict_nested)\n",
    "]\n",
    "single_key_dict[3]\n",
    "len(single_key_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode episode into one document of mongo episode type (nested dictionaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "episode_meta = indices_dict[0].copy()\n",
    "try:\n",
    "    episode_meta.pop(\"timestamp\")\n",
    "except KeyError:\n",
    "    print(f\"Key 'timestamp' not found\")\n",
    "\n",
    "episode_meta[\"seq_len\"] = len(single_key_dict)\n",
    "print(episode_meta)\n",
    "meta = {**(observation_meta.dict()), **episode_meta}\n",
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_key_dict = {\n",
    "    idx[\"timestamp\"]: dict_nested[key] for idx, key in zip(indices_dict, dict_nested)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eos.data_io.eos_struct import DataFrameDoc\n",
    "\n",
    "doc = DataFrameDoc(\n",
    "    timestamp=meta[\"episodestart\"],\n",
    "    meta=meta,\n",
    "    observation=single_key_dict,\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import asdict\n",
    "#\n",
    "# rows_multi = [\n",
    "#     {\n",
    "#         'timestamp': idx['timestamp'],\n",
    "#         'meta': {**idx, **(asdict(meta))},\n",
    "#         'observation': {key: value for key, value in record.items()},\n",
    "#     }\n",
    "#     for (idx, record) in zip(indices_dict, records)\n",
    "# ]\n",
    "# rows_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the records of an episode into rows of nested dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "rows = [\n",
    "    {\n",
    "        \"timestamp\": idx[\"timestamp\"],\n",
    "        \"meta\": {**idx, **(observation_meta.dict())},\n",
    "        \"observation\": dict_nested[key],\n",
    "    }\n",
    "    for (idx, key) in zip(indices_dict, dict_nested)\n",
    "]\n",
    "rows\n",
    "len(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding MongoDB dict to MultiIndexed DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = pd.DataFrame(rows)\n",
    "df_rows\n",
    "df_rows[\"observation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_observations = [\n",
    "    {key: value for key, value in row.items()} for row in df_rows[\"observation\"]\n",
    "]\n",
    "dict_observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding MongoDB dict to MultiIndexed DataFrame\n",
    "first get the multiindex as tuple from nested keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_observations = {\n",
    "    (\n",
    "        meta[\"episodestart\"],\n",
    "        meta[\"vehicle\"],\n",
    "        meta[\"driver\"],\n",
    "        meta[\"timestamp\"],\n",
    "        key1,\n",
    "        key2,\n",
    "        key3,\n",
    "    ): value\n",
    "    for meta, obs in zip(df_rows[\"meta\"], df_rows[\"observation\"])\n",
    "    for key1, obs1 in obs.items()\n",
    "    for key2, obs2 in obs1.items()\n",
    "    for key3, value in obs2.items()\n",
    "}\n",
    "\n",
    "dict_observations\n",
    "len(dict_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_decoded = pd.Series(dict_observations)\n",
    "ser_decoded.index.names = [\n",
    "    \"episodestart\",\n",
    "    \"vehicle\",\n",
    "    \"driver\",\n",
    "    \"timestamp\",\n",
    "    \"qtuple\",\n",
    "    \"rows\",\n",
    "    \"idx\",\n",
    "]\n",
    "ser_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_decoded = ser_decoded.unstack(level=[\"qtuple\", \"rows\", \"idx\"])\n",
    "df_decoded\n",
    "df_decoded.columns\n",
    "# df_decoded.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_decoded.reset_index()\n",
    "# df_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ser_action = ser_decoded.loc[('action', ['r0', 'r1', 'r2', 'throttle'])]\n",
    "# df_action = ser_action.unstack(level=[4, 5])\n",
    "idx = pd.IndexSlice\n",
    "df_action = df_decoded.loc[:, idx[\"action\", [\"r0\", \"r1\", \"r2\", \"throttle\"]]]\n",
    "df_action\n",
    "# df_action.set_index(('action', 'throttle'))5369@"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding MongoDB dict to MultiIndexed fully described DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_observations_list = [\n",
    "    {\n",
    "        (\n",
    "            meta[\"episodestart\"],\n",
    "            meta[\"vehicle\"],\n",
    "            meta[\"driver\"],\n",
    "            meta[\"timestamp\"],\n",
    "            key1,\n",
    "            key2,\n",
    "            key3,\n",
    "        ): value\n",
    "        for key1, obs1 in obs.items()\n",
    "        for key2, obs2 in obs1.items()\n",
    "        for key3, value in obs2.items()\n",
    "    }\n",
    "    for meta, obs in zip(df_rows[\"meta\"], df_rows[\"observation\"])\n",
    "]\n",
    "\n",
    "dict_observations_list[0]\n",
    "len(dict_observations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "ser_decoded = pd.Series(dict_observations_list[0])\n",
    "ser_decoded.index.names = [\n",
    "    \"episodestart\",\n",
    "    \"vehicle\",\n",
    "    \"driver\",\n",
    "    \"timestamp\",\n",
    "    \"qtuple\",\n",
    "    \"rows\",\n",
    "    \"idx\",\n",
    "]\n",
    "# ser_decoded\n",
    "# ser_decoded.index\n",
    "ser_action = ser_decoded.loc[idx[:, :, :, :, \"action\", [\"r0\", \"r1\", \"r2\", \"throttle\"]]]\n",
    "# ser_action.index\n",
    "df_action = ser_action.unstack([0, 1, 2, 3, 4, 5])\n",
    "# df_action.index\n",
    "# df_action.columns\n",
    "multiindex = df_action.columns\n",
    "# multiindex[0]\n",
    "multiindex[-1]\n",
    "# df_action.index.names\n",
    "# multiindex = (*df_action.index.names,('action', 'throttle'))\n",
    "# multiindex\n",
    "df_action.set_index(multiindex[-1], inplace=True)\n",
    "df_action\n",
    "df_action.columns\n",
    "# df_action.set_index(idx[:, :, :, :, 'action', 'throttle'], inplace=True)\n",
    "# df_action = pd.concat([df_action], keys=[('action','throttle')], names=['qtuple', 'rows']).set\n",
    "# multiindex = (df_action.index,'action', 'throttle'))\n",
    "# df_action.set_index(multiindex, inplace=True)\n",
    "# df_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_timestep = ser_decoded.loc[idx[:, :, :, :, \"action\", \"timestep\"]]\n",
    "action_timestep\n",
    "action_speed = ser_decoded.loc[idx[:, :, :, :, \"action\", \"speed\"]]\n",
    "action_speed\n",
    "action_multicol = [\n",
    "    (*column, speed, timestep)\n",
    "    for column, timestep, speed in zip(df_action.columns, action_timestep, action_speed)\n",
    "]\n",
    "action_multicol\n",
    "\n",
    "df_action.columns = pd.MultiIndex.from_tuples(\n",
    "    action_multicol,\n",
    "    names=[\n",
    "        \"episodestart\",\n",
    "        \"vehicle\",\n",
    "        \"driver\",\n",
    "        \"timestamp\",\n",
    "        \"qtuple\",\n",
    "        \"rows\",\n",
    "        \"speed\",\n",
    "        \"timestep\",\n",
    "    ],\n",
    ")\n",
    "df_action\n",
    "df_action.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_state = ser_decoded.loc[\n",
    "    idx[:, :, :, :, \"state\", [\"brake\", \"thrust\", \"velocity\", \"timestep\"]]\n",
    "]\n",
    "# ser_state\n",
    "df_state = ser_state.unstack([0, 1, 2, 3, 4, 5])\n",
    "# df_state = ser_state.unstack(level=[0, 1]).set_index(('state', 'timestep'))\n",
    "# df_state\n",
    "multiindex = df_state.columns\n",
    "multiindex[-1]\n",
    "df_state.set_index(multiindex[-1], inplace=True)\n",
    "# multiindex[idx[:,:,:,:,:,'timestep']]\n",
    "# df_state.set_index(multiindex[idx[:,:,:,:,:,'timestep']], inplace=True)\n",
    "df_state\n",
    "# df_state.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "ser_decoded = pd.Series(dict_observations_list[0])\n",
    "ser_decoded.index.names = [\n",
    "    \"episodestart\",\n",
    "    \"vehicle\",\n",
    "    \"driver\",\n",
    "    \"timestamp\",\n",
    "    \"qtuple\",\n",
    "    \"rows\",\n",
    "    \"idx\",\n",
    "]\n",
    "ser_action = ser_decoded.loc[idx[:, :, :, :, \"action\", [\"r0\", \"r1\", \"r2\", \"throttle\"]]]\n",
    "df_action = ser_action.unstack([0, 1, 2, 3, 4, 5])\n",
    "multiindex = df_action.columns\n",
    "df_action.set_index(multiindex[-1], inplace=True)\n",
    "\n",
    "action_timestep = ser_decoded.loc[idx[:, :, :, :, \"action\", \"timestep\"]]\n",
    "action_speed = ser_decoded.loc[idx[:, :, :, :, \"action\", \"speed\"]]\n",
    "action_multicol = [\n",
    "    (*column, speed, timestep)\n",
    "    for column, timestep, speed in zip(df_action.columns, action_timestep, action_speed)\n",
    "]\n",
    "df_action.columns = pd.MultiIndex.from_tuples(\n",
    "    action_multicol,\n",
    "    names=[\n",
    "        \"episodestart\",\n",
    "        \"vehicle\",\n",
    "        \"driver\",\n",
    "        \"timestamp\",\n",
    "        \"qtuple\",\n",
    "        \"rows\",\n",
    "        \"speed\",\n",
    "        \"timestep\",\n",
    "    ],\n",
    ")\n",
    "df_action\n",
    "\n",
    "\n",
    "ser_state = ser_decoded.loc[\n",
    "    idx[:, :, :, :, \"state\", [\"brake\", \"thrust\", \"velocity\", \"timestep\"]]\n",
    "]\n",
    "df_state = ser_state.unstack([0, 1, 2, 3, 4, 5])\n",
    "multiindex = df_state.columns\n",
    "df_state.set_index(multiindex[-1], inplace=True)\n",
    "df_state\n",
    "\n",
    "\n",
    "ser_nstate = ser_decoded.loc[\n",
    "    idx[:, :, :, :, \"nstate\", [\"brake\", \"thrust\", \"velocity\", \"timestep\"]]\n",
    "]\n",
    "df_nstate = ser_nstate.unstack([0, 1, 2, 3, 4, 5])\n",
    "multiindex = df_nstate.columns\n",
    "df_nstate.set_index(multiindex[-1], inplace=True)\n",
    "df_nstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_reward = ser_decoded.loc[idx[:, :, :, :, \"reward\", [\"work\", \"timestep\"]]]\n",
    "# ser_reward = pd.Series(\n",
    "#     data=[ser_reward[('reward', 'work', 0)]],\n",
    "#     index=[ser_reward[('reward', 'timestep', 0)]],\n",
    "#     name='work',\n",
    "# )\n",
    "ser_reward = ser_reward.unstack([0, 1, 2, 3, 4, 5])\n",
    "multiindex = ser_reward.columns\n",
    "ser_reward.set_index(multiindex[-1], inplace=True)\n",
    "ser_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ser_reward = ser_decoded.loc[idx[:,:,:,:,'reward', ['work', 'timestep']]]\n",
    "# ser_reward = pd.Series(\n",
    "#     data=[ser_reward[multiindex[0]]],\n",
    "#     index=[ser_reward[multiindex[1]]],\n",
    "#     name='work',\n",
    "# )\n",
    "# ser_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actions = []\n",
    "df_states = []\n",
    "df_nstates = []\n",
    "ser_rewards = []\n",
    "\n",
    "for dict_observations in dict_observations_list:\n",
    "    idx = pd.IndexSlice\n",
    "    ser_decoded = pd.Series(dict_observations)\n",
    "    ser_decoded.index.names = [\n",
    "        \"episodestart\",\n",
    "        \"vehicle\",\n",
    "        \"driver\",\n",
    "        \"timestamp\",\n",
    "        \"qtuple\",\n",
    "        \"rows\",\n",
    "        \"idx\",\n",
    "    ]\n",
    "\n",
    "    ser_action = ser_decoded.loc[\n",
    "        idx[:, :, :, :, \"action\", [\"r0\", \"r1\", \"r2\", \"throttle\"]]\n",
    "    ]\n",
    "    df_action = ser_action.unstack([0, 1, 2, 3, 4, 5])\n",
    "    multiindex = df_action.columns\n",
    "    df_action.set_index(multiindex[-1], inplace=True)\n",
    "\n",
    "    action_timestep = ser_decoded.loc[idx[:, :, :, :, \"action\", \"timestep\"]]\n",
    "    action_speed = ser_decoded.loc[idx[:, :, :, :, \"action\", \"speed\"]]\n",
    "    action_multicol = [\n",
    "        (*column, speed, timestep)\n",
    "        for column, timestep, speed in zip(\n",
    "            df_action.columns, action_timestep, action_speed\n",
    "        )\n",
    "    ]\n",
    "    df_action.columns = pd.MultiIndex.from_tuples(\n",
    "        action_multicol,\n",
    "        names=[\n",
    "            \"episodestart\",\n",
    "            \"vehicle\",\n",
    "            \"driver\",\n",
    "            \"timestamp\",\n",
    "            \"qtuple\",\n",
    "            \"rows\",\n",
    "            \"speed\",\n",
    "            \"timestep\",\n",
    "        ],\n",
    "    )\n",
    "    # df_action\n",
    "    df_actions.append(df_action)\n",
    "\n",
    "    ser_state = ser_decoded.loc[\n",
    "        idx[:, :, :, :, \"state\", [\"brake\", \"thrust\", \"velocity\", \"timestep\"]]\n",
    "    ]\n",
    "    df_state = ser_state.unstack([0, 1, 2, 3, 4, 5])\n",
    "    multiindex = df_state.columns\n",
    "    df_state.set_index(multiindex[-1], inplace=True)\n",
    "    # df_state\n",
    "    df_states.append(df_state)\n",
    "\n",
    "    ser_nstate = ser_decoded.loc[\n",
    "        idx[:, :, :, :, \"nstate\", [\"brake\", \"thrust\", \"velocity\", \"timestep\"]]\n",
    "    ]\n",
    "    df_nstate = ser_nstate.unstack([0, 1, 2, 3, 4, 5])\n",
    "    multiindex = df_nstate.columns\n",
    "    df_nstate.set_index(multiindex[-1], inplace=True)\n",
    "    # df_nstate\n",
    "    df_nstates.append(df_nstate)\n",
    "\n",
    "    ser_reward = ser_decoded.loc[idx[:, :, :, :, \"reward\", [\"work\", \"timestep\"]]]\n",
    "    ser_reward = ser_reward.unstack([0, 1, 2, 3, 4, 5])\n",
    "    multiindex = ser_reward.columns\n",
    "    ser_reward.set_index(multiindex[-1], inplace=True)\n",
    "    # ser_reward\n",
    "    ser_rewards.append(ser_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simply concatenate the pandas dataframes leads to confusion\n",
    "please see the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actions_all = pd.concat(df_actions)\n",
    "df_actions_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the following is the correct way to extract columns, index and values from the pandas dataframes\n",
    "\n",
    "and stack them into numpy arrays for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa_action_columns_list = [df_action.columns for df_action in df_actions]\n",
    "# npa_action_columns_list\n",
    "npa_action_indices_list = [df_action.index for df_action in df_actions]\n",
    "# npa_action_indices_list\n",
    "npa_state_columns_list = [df_state.columns for df_state in df_states]\n",
    "npa_state_indices_list = [df_state.index for df_state in df_states]\n",
    "npa_nstate_columns_list = [df_nstate.columns for df_nstate in df_nstates]\n",
    "npa_nstate_indices_list = [df_nstate.index for df_nstate in df_nstates]\n",
    "npa_reward_columns_list = [ser_reward.columns for ser_reward in ser_rewards]\n",
    "npa_reward_indices_list = [ser_reward.index for ser_reward in ser_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa_action_columns_list[0]\n",
    "npa_action_indices_list[0]\n",
    "ts_array = []\n",
    "for columns in npa_action_columns_list:\n",
    "    timestep = columns.get_level_values(\"timestep\")\n",
    "    ts_array.append(timestep)\n",
    "    # print(timestep)\n",
    "    # print(npa_action_columns_list[0].loc[idx[:,:,:,timestep,:,:], :])\n",
    "    # print(npa_action_indices_list[0].loc[idx[:,:,:,timestep,:,:], :])\n",
    "\n",
    "ts_array = pd.DataFrame(np.stack(ts_array).T)\n",
    "ts_array\n",
    "ts_array.describe()\n",
    "# ts_array.average()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa_reward_columns_list[0]\n",
    "npa_reward_indices_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_states[0]\n",
    "df_states[0].values.flatten()\n",
    "df_states[0].loc[\n",
    "    :, idx[:, :, :, :, \"state\", [\"velocity\", \"thrust\", \"brake\"]]\n",
    "]  ## keep the order of input\n",
    "df_states[0].loc[\n",
    "    :, idx[:, :, :, :, \"state\", [\"velocity\", \"thrust\", \"brake\"]]\n",
    "].values.flatten()\n",
    "# df_actions[0]\n",
    "# df_actions[0].values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa_states_list = [\n",
    "    df_state.loc[\n",
    "        :, idx[:, :, :, :, \"state\", [\"velocity\", \"thrust\", \"brake\"]]\n",
    "    ].values.T.flatten()\n",
    "    for df_state in df_states\n",
    "]  # transpose T is necessary for column-wise stacking and flatten\n",
    "npa_states = np.stack(npa_states_list).astype(\"float\")\n",
    "npa_states\n",
    "\n",
    "\n",
    "npa_actions_list = [\n",
    "    df_action.loc[:, idx[:, :, :, :, \"action\", [\"r0\", \"r1\", \"r2\"]]].values.T.flatten()\n",
    "    for df_action in df_actions\n",
    "]\n",
    "npa_actions = np.stack(npa_actions_list).astype(\"float\")\n",
    "npa_actions\n",
    "\n",
    "npa_rewards_list = [ser_reward.values.T.flatten() for ser_reward in ser_rewards]\n",
    "npa_rewards = np.stack(npa_rewards_list).astype(\"float\")\n",
    "npa_rewards\n",
    "\n",
    "npa_nstates_list = [\n",
    "    df_nstate.loc[\n",
    "        :, idx[:, :, :, :, \"nstate\", [\"velocity\", \"thrust\", \"brake\"]]\n",
    "    ].values.T.flatten()\n",
    "    for df_nstate in df_nstates\n",
    "]\n",
    "npa_nstates = np.stack(npa_nstates_list).astype(\"float\")\n",
    "npa_nstates\n",
    "\n",
    "# np.concatenate(df_actions.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(npa_states)\n",
    "npa_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa_nstates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npa_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_action = ser_action.unstack(level=[0, 1]).set_index(('action', 'throttle'))\n",
    "# action_timestep = ser_decoded.loc[('action', 'timestep')]\n",
    "# action_speed = ser_decoded.loc[('action', 'speed')]\n",
    "#\n",
    "# action_multicol = [\n",
    "#     (*column, speed, timestep)\n",
    "#     for column, timestep, speed in zip(df_action.columns, action_timestep, action_speed)\n",
    "# ]\n",
    "# df_action.columns = pd.MultiIndex.from_tuples(\n",
    "#     action_multicol, names=['qtuple', 'rows', 'speed', 'timestep']\n",
    "# )\n",
    "# df_action\n",
    "# df_action.values.flatten()\n",
    "#\n",
    "# ser_state = ser_decoded.loc[:, idx['state', ['brake', 'thrust', 'velocity', 'timestep']]]\n",
    "# # ser_state\n",
    "# df_state = ser_state.unstack(level=[0, 1]).set_index(('state', 'timestep'))\n",
    "# df_state\n",
    "#\n",
    "# df_state.values\n",
    "#\n",
    "# ser_nstate = ser_decoded.loc[:, idx['nstate', ['brake', 'thrust', 'velocity', 'timestep']]]\n",
    "# # ser_state\n",
    "# df_nstate = ser_nstate.unstack(level=[0, 1]).set_index(('nstate', 'timestep'))\n",
    "# df_nstate\n",
    "#\n",
    "# ser_reward = ser_decoded.loc[:, idx['reward', ['work', 'timestep']]]\n",
    "# ser_reward = pd.Series(\n",
    "#     data=[ser_reward[('reward', 'work', 0)]],\n",
    "#     index=[ser_reward[('reward', 'timestep', 0)]],\n",
    "#     name='work',\n",
    "# )\n",
    "# ser_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ser_decoded = pd.Series(dict_observations)\n",
    "# ser_action = ser_decoded.loc[('action', ['r0', 'r1', 'r2', 'throttle'])]\n",
    "# df_action = ser_action.unstack(level=[0, 1]).set_index(('action', 'throttle'))\n",
    "# action_timestep = ser_decoded.loc[('action', 'timestep')]\n",
    "# action_speed = ser_decoded.loc[('action', 'speed')]\n",
    "#\n",
    "# action_multicol = [\n",
    "#     (*column, speed, timestep)\n",
    "#     for column, timestep, speed in zip(df_action.columns, action_timestep, action_speed)\n",
    "# ]\n",
    "# df_action.columns = pd.MultiIndex.from_tuples(\n",
    "#     action_multicol, names=['qtuple', 'rows', 'speed', 'timestep']\n",
    "# )\n",
    "# df_action\n",
    "# df_action.values.flatten()\n",
    "#\n",
    "# ser_state = ser_decoded.loc[('state', ['brake', 'thrust', 'velocity', 'timestep'])]\n",
    "# # ser_state\n",
    "# df_state = ser_state.unstack(level=[0, 1]).set_index(('state', 'timestep'))\n",
    "# df_state\n",
    "#\n",
    "# df_state.values\n",
    "#\n",
    "# ser_nstate = ser_decoded.loc[('nstate', ['brake', 'thrust', 'velocity', 'timestep'])]\n",
    "# # ser_state\n",
    "# df_nstate = ser_nstate.unstack(level=[0, 1]).set_index(('nstate', 'timestep'))\n",
    "# df_nstate\n",
    "#\n",
    "# ser_reward = ser_decoded.loc[('reward', ['work', 'timestep'])]\n",
    "# ser_reward = pd.Series(\n",
    "#     data=[ser_reward[('reward', 'work', 0)]],\n",
    "#     index=[ser_reward[('reward', 'timestep', 0)]],\n",
    "#     name='work',\n",
    "# )\n",
    "# ser_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding sampled multiple episodes from Mongo episode documents to multiindexed DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate mutliple episodes of mongo episode docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create epsiode with different lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_episodex = dfs_episode.copy()\n",
    "dfs_episodex.index = dfs_episodex.index.set_levels([episodestart], level=\"episodestart\")\n",
    "dfs_episodex.index = dfs_episodex.index.set_levels(\n",
    "    [[trucks_by_id[\"VB4\"].vid], [drivers_by_id[\"li-changlong\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "dfs_episodex\n",
    "ts_index = dfs_episodex.index.unique(level=\"timestamp\")\n",
    "\n",
    "idx_num = len(ts_index)\n",
    "drop_num = np.random.randint(low=1, high=idx_num - 1)\n",
    "ts_index_to_drop = np.random.choice(ts_index, drop_num, replace=False)\n",
    "ts_index_to_drop\n",
    "dfs_zigzag = dfs_episodex.drop(index=ts_index_to_drop, level=\"timestamp\")\n",
    "dfs_zigzag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_new = pd.to_datetime((datetime.now()))\n",
    "ts_new\n",
    "episodestart = ts_new - pd.Timedelta(2, \"d\")\n",
    "episodestart\n",
    "\n",
    "dfs_episode0 = dfs_episode.copy()\n",
    "dfs_episode0.index = dfs_episode0.index.set_levels([episodestart], level=\"episodestart\")\n",
    "dfs_episode0.index = dfs_episode0.index.set_levels(\n",
    "    [[trucks_by_id[\"VB7\"].vid], [drivers_by_id[\"zheng-longfei\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "ts_index = dfs_episode0.index.unique(level=\"timestamp\")\n",
    "idx_num = len(ts_index)\n",
    "drop_num = np.random.randint(low=1, high=idx_num - 1)\n",
    "ts_index_to_drop = np.random.choice(ts_index, drop_num, replace=False)\n",
    "dfs_episode0 = dfs_episode0.drop(index=ts_index_to_drop, level=\"timestamp\")\n",
    "\n",
    "dfs_episode1 = dfs_episode.copy()\n",
    "dfs_episode1.index = dfs_episode1.index.set_levels(\n",
    "    [episodestart - pd.Timedelta(3, \"d\")], level=\"episodestart\"\n",
    ")\n",
    "dfs_episode1.index = dfs_episode1.index.set_levels(\n",
    "    [[trucks_by_id[\"MP73\"].vid], [drivers_by_id[\"wang-cheng\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "ts_index = dfs_episode1.index.unique(level=\"timestamp\")\n",
    "idx_num = len(ts_index)\n",
    "drop_num = np.random.randint(low=1, high=idx_num - 1)\n",
    "ts_index_to_drop = np.random.choice(ts_index, drop_num, replace=False)\n",
    "dfs_episode1 = dfs_episode1.drop(index=ts_index_to_drop, level=\"timestamp\")\n",
    "\n",
    "dfs_episode2 = dfs_episode.copy()\n",
    "dfs_episode2.index = dfs_episode2.index.set_levels(\n",
    "    [episodestart - pd.Timedelta(4, \"d\")], level=\"episodestart\"\n",
    ")\n",
    "dfs_episode2.index = dfs_episode2.index.set_levels(\n",
    "    [[trucks_by_id[\"VB7\"].vid], [drivers_by_id[\"wang-cheng\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "ts_index = dfs_episode2.index.unique(level=\"timestamp\")\n",
    "idx_num = len(ts_index)\n",
    "drop_num = np.random.randint(low=1, high=idx_num - 1)\n",
    "ts_index_to_drop = np.random.choice(ts_index, drop_num, replace=False)\n",
    "dfs_episode2 = dfs_episode2.drop(index=ts_index_to_drop, level=\"timestamp\")\n",
    "\n",
    "dfs_episode3 = dfs_episode.copy()\n",
    "dfs_episode3.index = dfs_episode3.index.set_levels(\n",
    "    [episodestart - pd.Timedelta(5, \"d\")], level=\"episodestart\"\n",
    ")\n",
    "dfs_episode3.index = dfs_episode3.index.set_levels(\n",
    "    [[trucks_by_id[\"MP73\"].vid], [drivers_by_id[\"zheng-longfei\"].pid]],\n",
    "    level=[\"vehicle\", \"driver\"],\n",
    "    verify_integrity=False,\n",
    ")\n",
    "\n",
    "ts_index = dfs_episode3.index.unique(level=\"timestamp\")\n",
    "idx_num = len(ts_index)\n",
    "drop_num = np.random.randint(low=1, high=idx_num - 1)\n",
    "ts_index_to_drop = np.random.choice(ts_index, drop_num, replace=False)\n",
    "dfs_episode3 = dfs_episode3.drop(index=ts_index_to_drop, level=\"timestamp\")\n",
    "dfs_episode3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "episodes = [dfs_episode, dfs_episode0, dfs_episode1, dfs_episode2, dfs_episode3]\n",
    "try:\n",
    "    dfs_episode_all = reduce(\n",
    "        lambda left, right,: pd.concat([left, right], axis=0), episodes\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "# dfs_episode_all.sort_index(inplace=True)\n",
    "# dfs_episode_all = dfs_episode_all.swaplevel(1, 0, axis=0)\n",
    "# dfs_episode_all = dfs_episode_all.swaplevel(1, 2, axis=0)\n",
    "dfs_episode_all.sort_index(inplace=True)\n",
    "dfs_episode_all = dfs_episode_all[[\"state\", \"action\", \"reward\", \"nstate\"]]\n",
    "display(\"dfs_episode_all\")\n",
    "dfs_episode_all.index\n",
    "dfs_episode_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for episode in episodes:\n",
    "    dict_nested = eos_df_to_nested_dict(episode)\n",
    "    indices_dict = [\n",
    "        {indices.names[i]: level for i, level in enumerate(levels)}\n",
    "        for levels in episode.index\n",
    "    ]\n",
    "    single_key_dict = {\n",
    "        idx[\"timestamp\"]: dict_nested[key]\n",
    "        for idx, key in zip(indices_dict, dict_nested)\n",
    "    }\n",
    "\n",
    "    episode_meta = indices_dict[0].copy()\n",
    "    try:\n",
    "        episode_meta.pop(\"timestamp\")\n",
    "    except KeyError:\n",
    "        print(f\"Key 'timestamp' not found\")\n",
    "\n",
    "    episode_meta[\"seq_len\"] = len(single_key_dict)\n",
    "\n",
    "    meta = {**(observation_meta.dict()), **episode_meta}\n",
    "\n",
    "    doc = DataFrameDoc(\n",
    "        timestamp=meta[\"episodestart\"],\n",
    "        meta=meta,\n",
    "        observation=single_key_dict,\n",
    "    )\n",
    "    docs.append(doc)\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encode multiple episodes of mongo documents to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs = pd.DataFrame(docs)\n",
    "df_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_observations = [\n",
    "    {key: value for key, value in row.items()} for row in df_docs[\"observation\"]\n",
    "]\n",
    "dict_observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decoding observations (episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_observations = {\n",
    "    (\n",
    "        meta[\"vehicle\"],\n",
    "        meta[\"driver\"],\n",
    "        meta[\"episodestart\"],\n",
    "        key1,\n",
    "        key2,\n",
    "        key3,\n",
    "        key4,\n",
    "    ): value\n",
    "    for meta, obs in zip(df_docs[\"meta\"], df_docs[\"observation\"])\n",
    "    for key1, obs1 in obs.items()\n",
    "    for key2, obs2 in obs1.items()\n",
    "    for key3, obs3 in obs2.items()\n",
    "    for key4, value in obs3.items()\n",
    "}\n",
    "\n",
    "dict_observations\n",
    "len(dict_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_decoded = pd.Series(dict_observations)\n",
    "ser_decoded.index.names = [\n",
    "    \"vehicle\",\n",
    "    \"driver\",\n",
    "    \"episodestart\",\n",
    "    \"timestamp\",\n",
    "    \"qtuple\",\n",
    "    \"rows\",\n",
    "    \"idx\",\n",
    "]\n",
    "ser_decoded\n",
    "df_decoded = ser_decoded.unstack(level=[\"qtuple\", \"rows\", \"idx\"])\n",
    "df_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodestart_index = df_decoded.index.unique(level=\"episodestart\")\n",
    "# rewards.values\n",
    "episodestart_index\n",
    "\n",
    "df_rewards = df_decoded[\"reward\", \"work\"]\n",
    "# rewards.index.get_level_values('episodestart')\n",
    "idx = pd.IndexSlice\n",
    "np_r_n_t = [\n",
    "    df_rewards.loc[idx[:, :, ep_start, :]].values.tolist()\n",
    "    for ep_start in episodestart_index\n",
    "]\n",
    "# np_r_n_t = [ rewards.loc[:,:,ep_start].values.tolist()   for ep_start in episodestart_index]\n",
    "np_r_n_t\n",
    "np_r_n_t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rewards.loc[:, :, episodestart_index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import pad_sequences\n",
    "# from tensorflow import keras\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "r_n_t = pad_sequences(np_r_n_t, padding=\"post\", dtype=\"float32\", value=-10000.0)\n",
    "r_n_t\n",
    "r_n_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "states = df_decoded.loc[:, idx[\"state\", [\"velocity\", \"thrust\", \"brake\"]]]\n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_s_n_t = [\n",
    "    states.loc[idx[:, :, ep_start, :]].values.tolist()\n",
    "    for ep_start in episodestart_index\n",
    "]\n",
    "s_n_t = pad_sequences(np_s_n_t, padding=\"post\", dtype=\"float32\", value=-10000.0)\n",
    "print(s_n_t)\n",
    "s_n_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torque_table_row_names = [\"r0\", \"r1\", \"r2\"]\n",
    "actions = df_decoded.loc[:, idx[\"action\", torque_table_row_names]]\n",
    "np_a_n_t = [\n",
    "    actions.loc[idx[:, :, ep_start, :]].values.tolist()\n",
    "    for ep_start in episodestart_index\n",
    "]\n",
    "a_n_t = pad_sequences(np_a_n_t, padding=\"post\", dtype=\"float32\", value=-10000.0)\n",
    "print(a_n_t)\n",
    "a_n_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_n_t.shape\n",
    "a_n_t.shape\n",
    "s_n_t\n",
    "a_n_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "nstates = df_decoded.loc[:, idx[\"nstate\", [\"velocity\", \"thrust\", \"brake\"]]]\n",
    "print(nstates)\n",
    "np_ns_n_t = [\n",
    "    nstates.loc[idx[:, :, ep_start, :]].values.tolist()\n",
    "    for ep_start in episodestart_index\n",
    "]\n",
    "ns_n_t = pad_sequences(np_ns_n_t, padding=\"post\", dtype=\"float32\", value=-10000.0)\n",
    "print(ns_n_t)\n",
    "ns_n_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_n_t_ = s_n_t[:, 1:, :]\n",
    "s_n_t_.shape\n",
    "s_n_t_\n",
    "a_n_t_ = a_n_t[:, :-1, :]\n",
    "a_n_t_.shape\n",
    "a_n_t_\n",
    "\n",
    "h_n_t = np.concatenate((s_n_t_, a_n_t_), axis=2)\n",
    "h_n_t.shape\n",
    "h_n_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split batches into trucated sequences for TBPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_n_t = zip(s_n_t, a_n_t, r_n_t, ns_n_t)\n",
    "type(obs_n_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_n_t\n",
    "s_n_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_num = s_n_t.shape[1] // 2 + 1\n",
    "print(f\"split_num: {split_num}\")\n",
    "s_n_t_splits = np.array_split(s_n_t, split_num, axis=1)\n",
    "len(s_n_t_splits)\n",
    "s_n_t_splits[0]\n",
    "s_n_t_splits[1]\n",
    "s_n_t_splits[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_num = s_n_t.shape[1] // 2 + 1\n",
    "print(f\"split_num: {split_num}\")\n",
    "\n",
    "for i, batch_t in enumerate(\n",
    "    zip(\n",
    "        np.array_split(s_n_t, split_num, axis=1),\n",
    "        np.array_split(a_n_t, split_num, axis=1),\n",
    "        np.array_split(r_n_t, split_num, axis=1),\n",
    "        np.array_split(ns_n_t, split_num, axis=1),\n",
    "    )\n",
    "):\n",
    "    s_n_t_b, a_n_t_b, r_n_t_b, ns_n_t_b = batch_t\n",
    "    print(f\"batch number: {i}\")\n",
    "    print(f\"s_n_t_b: {s_n_t_b}\")\n",
    "    print(f\"a_n_t_b: {a_n_t_b}\")\n",
    "    print(f\"r_n_t_b: {r_n_t_b}\")\n",
    "    print(f\"ns_n_t_b: {ns_n_t_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoding in batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rows = pd.DataFrame(docs)\n",
    "df_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_observations = [\n",
    "    {\n",
    "        (\n",
    "            meta[\"vehicle\"],\n",
    "            meta[\"driver\"],\n",
    "            meta[\"episodestart\"],\n",
    "            timestamp,\n",
    "            qtuple,\n",
    "            rows,\n",
    "            idx,\n",
    "        ): value\n",
    "        for timestamp, obs1 in obs.items()\n",
    "        for qtuple, obs2 in obs1.items()\n",
    "        for rows, obs3 in obs2.items()\n",
    "        for idx, value in obs3.items()\n",
    "    }\n",
    "    for meta, obs in zip(df_rows[\"meta\"], df_rows[\"observation\"])\n",
    "]\n",
    "\n",
    "# dict_observations\n",
    "len(dict_observations)\n",
    "\n",
    "batch = []\n",
    "for dict_obs in dict_observations:\n",
    "    ser_decoded = pd.Series(dict_obs)\n",
    "    ser_decoded.index.names = [\n",
    "        \"vehicle\",\n",
    "        \"driver\",\n",
    "        \"episodestart\",\n",
    "        \"timestamp\",\n",
    "        \"qtuple\",\n",
    "        \"rows\",\n",
    "        \"idx\",\n",
    "    ]\n",
    "    df_decoded = ser_decoded.unstack(level=[\"qtuple\", \"rows\", \"idx\"])  # type: ignore\n",
    "    df_decoded.sort_index(inplace=True, axis=1)\n",
    "    batch.append(df_decoded)  # qtuple, rows, index\n",
    "\n",
    "# batch.sort_index(inplace=True, axis=0)\n",
    "# must not sort_index, otherwise the order of the columns will be changed, if there were duplicated episodes\n",
    "index_names = batch[0].index.names\n",
    "df_episodes = pd.concat(batch, keys=range(len(batch)), names=[\"batch\"])\n",
    "df_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_episodes.loc[1, :, :, :, :][\"reward\", \"work\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "padding_value = -10000\n",
    "rewards_list = [\n",
    "    df_episodes.loc[idx[i, :, :, :, :], idx[\"reward\", \"work\"]].values.tolist()\n",
    "    for i in df_episodes.index.levels[0]\n",
    "]\n",
    "r_n_t = tf.keras.utils.pad_sequences(\n",
    "    rewards_list, padding=\"post\", dtype=np.float32, value=padding_value\n",
    ")\n",
    "states_list = [\n",
    "    df_episodes.loc[idx[i, :, :, :, :], idx[\"state\", [\"velocity\", \"thrust\", \"brake\"]]].values.tolist()  # type: ignore\n",
    "    for i in df_episodes.index.levels[0]\n",
    "]  # type: ignore\n",
    "s_n_t = tf.keras.utils.pad_sequences(\n",
    "    states_list, padding=\"post\", dtype=np.float32, value=padding_value\n",
    ")\n",
    "actions_list = [\n",
    "    df_episodes.loc[idx[i, :, :, :, :], idx[\"action\", torque_table_row_names]].values.tolist()  # type: ignore\n",
    "    for i in df_episodes.index.levels[0]\n",
    "]  # type: ignore\n",
    "a_n_t = tf.keras.utils.pad_sequences(\n",
    "    actions_list, padding=\"post\", dtype=np.float32, value=padding_value\n",
    ")\n",
    "nstates_list = [\n",
    "    df_episodes.loc[idx[i, :, :, :, :], idx[\"nstate\", [\"velocity\", \"thrust\", \"brake\"]]].values.tolist()  # type: ignore\n",
    "    for i in df_episodes.index.levels[0]\n",
    "]  # type: ignore\n",
    "ns_n_t = tf.keras.utils.pad_sequences(\n",
    "    nstates_list, padding=\"post\", dtype=np.float32, value=padding_value\n",
    ")\n",
    "\n",
    "r_n_t\n",
    "s_n_t\n",
    "a_n_t\n",
    "ns_n_t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
