[
  {
    "objectID": "05.storage.pool.avro.schema.html",
    "href": "05.storage.pool.avro.schema.html",
    "title": "Schema",
    "section": "",
    "text": "source\n\ngen_torque_table_schema\n\n gen_torque_table_schema (obs_meta:tspace.data.core.ObservationMeta)\n\nGenerate torque table schema from observation meta data\n\n\n\n\nType\nDetails\n\n\n\n\nobs_meta\nObservationMeta\nAn ObservationMeta object\n\n\n\n\nsource\n\n\ngen_episode_array_fields_schema\n\n gen_episode_array_fields_schema\n                                  (obs_meta:tspace.data.core.ObservationMe\n                                  ta)\n\nGenerate episode array fields schema from observation meta data\n\n\n\n\nType\nDetails\n\n\n\n\nobs_meta\nObservationMeta\nAn ObservationMeta object\n\n\n\n\nsource\n\n\ngen_episode_schema\n\n gen_episode_schema (obs_meta:tspace.data.core.ObservationMeta)\n\nGenerate episode schema from observation meta data\n\n\n\n\nType\nDetails\n\n\n\n\nobs_meta\nObservationMeta\nAn ObservationMeta object\n\n\nReturns\ndict\n\n\n\n\nparsed_schema_episode = fastavro.schema.parse_schema(schema_episode)",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Storage</b>",
      "<b>Pool</b>",
      "<b>Avro</b>",
      "Schema"
    ]
  },
  {
    "objectID": "05.storage.pool.dask.html",
    "href": "05.storage.pool.dask.html",
    "title": "Dask",
    "section": "",
    "text": "source\n\nDaskPool\n\n DaskPool (_cnt:int=0, recipe:configparser.ConfigParser,\n           query:tspace.data.core.PoolQuery,\n           meta:tspace.data.core.ObservationMeta,\n           pl_path:Optional[pathlib.Path]=None,\n           logger:Optional[logging.Logger]=None,\n           dict_logger:Optional[dict]=None)\n\n*The pool Class to be derived from as shared common interfaces and attributes for ParquetPool and AvroPool\nIt has with the following features:\n- use Dask dataframe for lazy data processing\n- using dask delayed to parallelize the data processing like sampling,\nAttributes:\n- recipe: a config file for the pool\n- pl_path: the pool path, a Path object to the parquet file for RECORD, to avro file for EPISODE\n- query: a PoolQuery object\n- meta: the meta information for the data collection\n- logger: a logger object\n- dict_logger: a dictionary logger object*\n\nsource\n\n\nDaskPool.get_query\n\n DaskPool.get_query (query:Optional[tspace.data.core.PoolQuery]=None)\n\n*Get records by PoolQuery\nArgs:\nquery: a [`PoolQuery`](https://Binjian.github.io/tspace/01.data.core.html#poolquery) object\nreturn:\na DataFrame with all records in the query time range*\n\nsource\n\n\nDaskPool.sample\n\n DaskPool.sample (size:int,\n                  query:Optional[tspace.data.core.PoolQuery]=None)\n\n*Sample a batch of data from the pool\nAn abstract method to be implemented by the derived class ParquetPool and AvroPool\nArgs:\nsize: the number of records to be sampled\nquery: a [`PoolQuery`](https://Binjian.github.io/tspace/01.data.core.html#poolquery) object\nReturn: a Pandas DataFrame*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsize\nint\n\nrequired size of samples\n\n\nquery\nOptional[PoolQuery]\nNone\n\n\n\nReturns\npd.DataFrame\n\nPoolQuery object, query specification\n\n\n\n\nsource\n\n\nDaskPool._count\n\n DaskPool._count (query:Optional[tspace.data.core.PoolQuery]=None)\n\n*Count the number of records in the db.\nArgs:\nquery: a [`PoolQuery`](https://Binjian.github.io/tspace/01.data.core.html#poolquery) object\nReturn:\n    the number of records in the db*\n\nsource\n\n\nDaskPool.__post_init__\n\n DaskPool.__post_init__ ()\n\nParsing the recipe and set the pool path\n\nsource\n\n\nDaskPool.find\n\n DaskPool.find (query:tspace.data.core.PoolQuery)\n\n*Find records by PoolQuery with\nArgs:\nquery: a [`PoolQuery`](https://Binjian.github.io/tspace/01.data.core.html#poolquery) object\nreturn: a DataFrame with all records matching query specification*",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Storage</b>",
      "<b>Pool</b>",
      "Dask"
    ]
  },
  {
    "objectID": "07.agent.idql.html",
    "href": "07.agent.idql.html",
    "title": "IDQL",
    "section": "",
    "text": "source\n\nIDQL\n\n IDQL (idql_net:Optional[jaxrl5.agents.ddpm_iql_simple.ddpm_iql_learner.DD\n       PMIQLLearner]=None,\n       action_space:Optional[gymnasium.spaces.space.Space]=None,\n       observation_space:Optional[gymnasium.spaces.space.Space]=None,\n       _ckpt_idql_dir:Optional[pathlib.Path]=None,\n       _truck:tspace.config.vehicles.Truck,\n       _driver:tspace.config.drivers.Driver, _resume:bool, _coll_type:str,\n       _hyper_param:Union[tspace.agent.utils.hyperparams.HyperParamDDPG,ts\n       pace.agent.utils.hyperparams.HyperParamRDPG,tspace.agent.utils.hype\n       rparams.HyperParamIDQL], _pool_key:str, _data_folder:str,\n       _infer_mode:bool, _buffer:Union[tspace.storage.buffer.mongo.MongoBu\n       ffer,tspace.storage.buffer.dask.DaskBuffer,NoneType]=None, _episode\n       _start_dt:Optional[pandas._libs.tslibs.timestamps.Timestamp]=None, \n       _observation_meta:Union[tspace.data.core.ObservationMetaCloud,tspac\n       e.data.core.ObservationMetaECU,NoneType]=None,\n       _torque_table_row_names:Optional[list[str]]=None,\n       _observations:Optional[list[pandas.core.series.Series]]=None,\n       _epi_no:Optional[int]=None, logger:Optional[logging.Logger]=None,\n       dict_logger:Optional[dict]=None)\n\n*IDQL agent for VEOS.\nAbstracts:\ndata interface:\n    - pool in mongodb\n    - buffer in memory (numpy array)\nmodel interface:\n    - idql_net: the implicit diffusion q-learning networks, which contains\n        - actor_net: the behavior actor network (from the data)\n        - critic_net: the critic network (Q-value function)\n        - value_net: the value network (V-value function)\n        The immplicit policy is re-weighting the sample from the behavior actor network with the importance weights \n        recommending the expectile loss by the paper \n    _ckpt_idql_dir: checkpoint directory for critic*\n\nsource\n\n\nIDQL.__post_init__\n\n IDQL.__post_init__ ()\n\n*initialize the rdpg agent.\nargs:\n- truck.ObservationNumber (int): dimension of the state space.\n- padding_value (float): value to pad the state with, impossible value for observation, action or re*\n\nsource\n\n\nIDQL.__repr__\n\n IDQL.__repr__ ()\n\nReturn repr(self).\n\nsource\n\n\nIDQL.__str__\n\n IDQL.__str__ ()\n\nReturn str(self).\n\nsource\n\n\nIDQL.__hash__\n\n IDQL.__hash__ ()\n\nReturn hash(self).\n\nsource\n\n\nIDQL.touch_gpu\n\n IDQL.touch_gpu ()\n\ntouch the gpu to avoid the first time delay\n\nsource\n\n\nIDQL.init_checkpoint\n\n IDQL.init_checkpoint ()\n\ncreate or restore from checkpoint\n\nsource\n\n\nIDQL.actor_predict\n\n IDQL.actor_predict (state:pandas.core.series.Series)\n\n*sample actions with additive ou noise\ninput: state is a pd.Series of length 3103/4503 (r*c), output numpy array\nAction outputs and noise object are all row vectors of length 2117 (rc), output numpy array*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nstate\nSeries\nstate sequence of the current episode\n\n\nReturns\nndarray\naction sequence of the current episode\n\n\n\n\nsource\n\n\nIDQL.sample_minibatch\n\n IDQL.sample_minibatch ()\n\nConvert batch type from DataFrames to flattened tensors.\n\nsource\n\n\nIDQL.train\n\n IDQL.train ()\n\nTrain the networks on the batch sampled from the pool.\n\nsource\n\n\nIDQL.soft_update_target\n\n IDQL.soft_update_target ()\n\nupdate the target networks with Polyak averaging\n\nsource\n\n\nIDQL.save_ckpt\n\n IDQL.save_ckpt ()\n\nTODO Save the checkpoint of the actor, critic and value network in Flax.\n\nsource\n\n\nIDQL.get_losses\n\n IDQL.get_losses ()\n\nGet the losses of the networks on the batch sampled from the pool.",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Agent</b>",
      "IDQL"
    ]
  },
  {
    "objectID": "07.agent.dpg.html",
    "href": "07.agent.dpg.html",
    "title": "DPG",
    "section": "",
    "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n\nsource\n\nDPG\n\n DPG (_truck:tspace.config.vehicles.Truck,\n      _driver:tspace.config.drivers.Driver, _resume:bool, _coll_type:str, \n      _hyper_param:Union[tspace.agent.utils.hyperparams.HyperParamDDPG,tsp\n      ace.agent.utils.hyperparams.HyperParamRDPG,tspace.agent.utils.hyperp\n      arams.HyperParamIDQL], _pool_key:str, _data_folder:str,\n      _infer_mode:bool, _buffer:Union[tspace.storage.buffer.mongo.MongoBuf\n      fer,tspace.storage.buffer.dask.DaskBuffer,NoneType]=None, _episode_s\n      tart_dt:Optional[pandas._libs.tslibs.timestamps.Timestamp]=None, _ob\n      servation_meta:Union[tspace.data.core.ObservationMetaCloud,tspace.da\n      ta.core.ObservationMetaECU,NoneType]=None,\n      _torque_table_row_names:Optional[list[str]]=None,\n      _observations:Optional[list[pandas.core.series.Series]]=None,\n      _epi_no:Optional[int]=None, logger:Optional[logging.Logger]=None,\n      dict_logger:Optional[dict]=None)\n\n*Base class for differentiable policy gradient methods\nAttributes:\ntruck_type: class variable [`Truck`](https://Binjian.github.io/tspace/03.config.vehicles.html#truck) Type,\nrdpg_hyper_type: class variable [`HyperParamRDPG`](https://Binjian.github.io/tspace/07.agent.utils.hyperparams.html#hyperparamrdpg)\n_truck: Truck object\n_driver: Driver object\n_resume: bool type, whether to resume training or start from scratch\n_coll_type: str, either \"RECORD\" or \"EPISODE\"\n_hyper_param: either HyperParamDDPG or HyperParamRDPG object\n_pool_key: str, database account, password, host and port specs\n_data_folder: str, root for data folder\n_infer_mode: bool, either pure inferring and no training or both inferring and training\n_buffer: Buffer object, either [`MongoBuffer`](https://Binjian.github.io/tspace/05.storage.buffer.mongo.html#mongobuffer) or [`DaskBuffer`](https://Binjian.github.io/tspace/05.storage.buffer.dask.html#daskbuffer)\n_episdoe_start_dt: Timestamp, starting time of the current episode\n-observation_meta: metadata of the observation, either from Cloud or from Kvaser\n_torque_table_row_name: list of str, ['r0', 'r1', 'r2', ...]\n_observations: list of pd.Series, the observation quadruple (s, a, r, s')\n_epi_no: int, sequence number of the episode\nlogger: logging.Logger, logging object\ndict_logger: dict, logging format specs*\n=======\n=======\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; 8d57352 ([fix] remove itikz in sandbox)\n\nsource\n\n\nDPG\n\n DPG (_truck:tspace.config.vehicles.Truck,\n      _driver:tspace.config.drivers.Driver, _resume:bool, _coll_type:str, \n      _hyper_param:Union[tspace.agent.utils.hyperparams.HyperParamDDPG,tsp\n      ace.agent.utils.hyperparams.HyperParamRDPG,tspace.agent.utils.hyperp\n      arams.HyperParamIDQL], _pool_key:str, _data_folder:str,\n      _infer_mode:bool, _buffer:Union[tspace.storage.buffer.mongo.MongoBuf\n      fer,tspace.storage.buffer.dask.DaskBuffer,NoneType]=None, _episode_s\n      tart_dt:Optional[pandas._libs.tslibs.timestamps.Timestamp]=None, _ob\n      servation_meta:Union[tspace.data.core.ObservationMetaCloud,tspace.da\n      ta.core.ObservationMetaECU,NoneType]=None,\n      _torque_table_row_names:Optional[list[str]]=None,\n      _observations:Optional[list[pandas.core.series.Series]]=None,\n      _epi_no:Optional[int]=None, logger:Optional[logging.Logger]=None,\n      dict_logger:Optional[dict]=None)\n\n*Base class for differentiable policy gradient methods\nAttributes:\ntruck_type: class variable [`Truck`](https://Binjian.github.io/tspace/03.config.vehicles.html#truck) Type,\nrdpg_hyper_type: class variable [`HyperParamRDPG`](https://Binjian.github.io/tspace/07.agent.utils.hyperparams.html#hyperparamrdpg)\n_truck: Truck object\n_driver: Driver object\n_resume: bool type, whether to resume training or start from scratch\n_coll_type: str, either \"RECORD\" or \"EPISODE\"\n_hyper_param: either HyperParamDDPG or HyperParamRDPG object\n_pool_key: str, database account, password, host and port specs\n_data_folder: str, root for data folder\n_infer_mode: bool, either pure inferring and no training or both inferring and training\n_buffer: Buffer object, either [`MongoBuffer`](https://Binjian.github.io/tspace/05.storage.buffer.mongo.html#mongobuffer) or [`DaskBuffer`](https://Binjian.github.io/tspace/05.storage.buffer.dask.html#daskbuffer)\n_episdoe_start_dt: Timestamp, starting time of the current episode\n-observation_meta: metadata of the observation, either from Cloud or from Kvaser\n_torque_table_row_name: list of str, ['r0', 'r1', 'r2', ...]\n_observations: list of pd.Series, the observation quadruple (s, a, r, s')\n_epi_no: int, sequence number of the episode\nlogger: logging.Logger, logging object\ndict_logger: dict, logging format specs*\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; 8d57352 ([fix] remove itikz in sandbox)\n\nsource\n\n\nDPG.__post_init__\n\n DPG.__post_init__ ()\n\n*Initialize the DPG object.\nHeavy lifting of data interface (buffer, pool) for both DDPG and RDPG*\n\nsource\n\n\nDPG.touch_gpu\n\n DPG.touch_gpu ()\n\nwarm up gpu for computing\n\nsource\n\n\nDPG.init_checkpoint\n\n DPG.init_checkpoint ()\n\n*Actor create or restore from checkpoint\nadd checkpoints manager*\n\nsource\n\n\nDPG.actor_predict\n\n DPG.actor_predict (state:pandas.core.series.Series)\n\n*Evaluate the actors given a single observations.\nbatch_size is 1.*\n\nsource\n\n\nDPG.start_episode\n\n DPG.start_episode (ts:pandas._libs.tslibs.timestamps.Timestamp)\n\ninitialize observation list\n\nsource\n\n\nDPG.deposit\n\n DPG.deposit (timestamp:pandas._libs.tslibs.timestamps.Timestamp,\n              state:pandas.core.series.Series,\n              action:pandas.core.series.Series,\n              reward:pandas.core.series.Series,\n              nstate:pandas.core.series.Series)\n\nDeposit the experience quadruple into the replay buffer.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntimestamp\npd.Timestamp\ntimestamp of the quadruple\n\n\nstate\npd.Series\nstate, pd.Series [brake row -&gt; thrust row -&gt; timestep row -&gt; velocity row ]\n\n\naction\npd.Series\naction, pd.Series [r0, r1, r2, … rows -&gt; speed row -&gt; throttle row-&gt; (flash) timestep row ]\n\n\nreward\npd.Series\nreward, pd.Series [timestep row -&gt; work row]\n\n\nnstate\npd.Series\nnext state, like state\n\n\n\n\nsource\n\n\nDPG.end_episode\n\n DPG.end_episode ()\n\nDeposit the whole episode of experience into the replay buffer for DPG.\n\nsource\n\n\nDPG.deposit_episode\n\n DPG.deposit_episode ()\n\nDeposit the whole episode of experience into the replay buffer for DPG.\n\nsource\n\n\nDPG.train\n\n DPG.train ()\n\n*Train the actor and critic moving network.\nReturn:\ntuple: (actor_loss, critic_loss)*\n\nsource\n\n\nDPG.get_losses\n\n DPG.get_losses ()\n\nGet the actor and critic losses without calculating the gradients.\n\nsource\n\n\nDPG.soft_update_target\n\n DPG.soft_update_target ()\n\n*update target networks with tiny tau value, typical value 0.001.\nIt’ll be done once after each batch, slowly update target by Polyak averaging.*\n\nsource\n\n\nDPG.save_ckpt\n\n DPG.save_ckpt ()\n\nsave checkpoints of actor and critic",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Agent</b>",
      "DPG"
    ]
  },
  {
    "objectID": "07.agent.utils.ou_action_noise.html",
    "href": "07.agent.utils.ou_action_noise.html",
    "title": "OUActionNoise",
    "section": "",
    "text": "source\n\nOUActionNoise\n\n OUActionNoise (mean, std_deviation, theta=0.15, dt=0.01, x_initial=None)\n\nOrnstein-Uhlenbeck process.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmean\n\n\nmean of the noise\n\n\nstd_deviation\n\n\nstandard deviation of the noise\n\n\ntheta\nfloat\n0.15\n\\(\\theta\\) is the rate of mean reversion\n\n\ndt\nfloat\n0.01\ndt is the time step\n\n\nx_initial\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nOUActionNoise.__init__\n\n OUActionNoise.__init__ (mean, std_deviation, theta=0.15, dt=0.01,\n                         x_initial=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmean\n\n\nmean of the noise\n\n\nstd_deviation\n\n\nstandard deviation of the noise\n\n\ntheta\nfloat\n0.15\n\\(\\theta\\) is the rate of mean reversion\n\n\ndt\nfloat\n0.01\ndt is the time step\n\n\nx_initial\nNoneType\nNone\n\n\n\n\n\nsource\n\n\nOUActionNoise.__call__\n\n OUActionNoise.__call__ ()\n\n*Call the noise.\nFormula taken from Ornstein-Uhlenbeck.*\n\nsource\n\n\nOUActionNoise.reset\n\n OUActionNoise.reset ()\n\nReset the Ornstein-Uhlenbeck process.",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Agent</b>",
      "<b>utils</b>",
      "OUActionNoise"
    ]
  },
  {
    "objectID": "05.storage.pool.parquet.html",
    "href": "05.storage.pool.parquet.html",
    "title": "Parquet",
    "section": "",
    "text": "source\n\nParquetPool\n\n ParquetPool (_cnt:int=0, recipe:configparser.ConfigParser,\n              query:tspace.data.core.PoolQuery,\n              meta:tspace.data.core.ObservationMeta,\n              pl_path:Optional[pathlib.Path]=None,\n              logger:Optional[logging.Logger]=None,\n              dict_logger:Optional[dict]=None,\n              ddf:Optional[dask.dataframe.core.DataFrame]=None)\n\n*The pool class for storing and retrieving records in Apache Arrow parquet files.\nIt uses Pandas backend for Parquet, PyArrow Parquet interface for meta data storage, and Dask DataFrame for data processing. meta information is stored in parquet metadata (in footer of parquet file).\nSample random observation quadruples will need some care to reassure the randomness. Here we apply dask DataFrame sample method. We use Dask Delayed to parallelize the data processing like sampling\nAttributes:\npl_path: `Path` to the parquet file folder\nmeta: meta information of the pool\nquery: [`PoolQuery`](https://Binjian.github.io/tspace/01.data.core.html#poolquery) object to the pool\ncnt: number of records in the pool\nddf: dask DataFrame object*\n\n# show_doc(ParquetPool.__post_init__)\n\n\nsource\n\n\nDaskPool.find\n\n DaskPool.find (query:tspace.data.core.PoolQuery)\n\n*Find records by PoolQuery with\nArgs:\nquery: a [`PoolQuery`](https://Binjian.github.io/tspace/01.data.core.html#poolquery) object\nreturn: a DataFrame with all records matching query specification*\n\nsource\n\n\nParquetPool.get_query\n\n ParquetPool.get_query (query:Optional[tspace.data.core.PoolQuery]=None)\n\n*get query from dask dataframe parquet storage\nArg: query: PoolQuery object to the pool\nReturn:\nA Dask DataFrame with all records in the query time range*\n\nsource\n\n\nParquetPool.sample\n\n ParquetPool.sample (size:int=4, query:tspace.data.core.PoolQuery)\n\n*Sample a batch of records from arrow parquet pool with fractional sampling.\nArgs: size: number of records in the batch query: PoolQuery object to the pool\nReturn: A Pandas DataFrame with all records in the query range*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsize\nint\n4\n\n\n\nquery\nPoolQuery\n\n\n\n\nReturns\npd.DataFrame\n\ntype: ignore\n\n\n\n\nsource\n\n\nParquetPool.store\n\n ParquetPool.store (episode:pandas.core.frame.DataFrame)\n\nDeposit an episode with all records in every time step into arrow parquet.\n\nsource\n\n\nParquetPool.close\n\n ParquetPool.close ()\n\nclose the pool\n\nsource\n\n\nParquetPool.load\n\n ParquetPool.load ()\n\nload RECORD arrays from parquet files in folder specified by the recipe",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Storage</b>",
      "<b>Pool</b>",
      "Parquet"
    ]
  },
  {
    "objectID": "06.dataflow.cruncher.html",
    "href": "06.dataflow.cruncher.html",
    "title": "Cruncher",
    "section": "",
    "text": "from tensorflow.summary import SummaryWriter, create_file_write, scalar # type: ignore\n\nsource\n\nCruncher\n\n Cruncher (agent:tspace.agent.dpg.DPG, truck:tspace.config.vehicles.Truck,\n           driver:tspace.config.drivers.Driver, resume:bool=False,\n           infer_mode:bool=False, data_dir:Optional[pathlib.Path]=None, tr\n           ain_summary_writer:Optional[tensorflow.python.ops.summary_ops_v\n           2.SummaryWriter]=None, logger:Optional[logging.Logger]=None,\n           dict_logger:Optional[dict]=None)\n\n*Cruncher is the processing unit of the data flow.\nIt consumes data (pd.DataFrame) from the observe pipeline and produces data (pd.DataFrame) into the flash pipeline.\nAttributes:\n- agent: abstract base class DPG, available DDPG, RDPG\n- truck: Truck object\n- driver: Driver object\n- resume: bool, whether to resume training\n- infer_mode: bool, whether only inferring or with training and inferring\n- data_dir: Path, the local path to save all generated data\n- train_summary_writer: SummaryWriter, Tensorflow training writer\n- logger: Logger\n- dict_logger: logger format specs*\n\nsource\n\n\nCruncher.__post_init__\n\n Cruncher.__post_init__ ()\n\nSet logger, Tensorflow data path and running mode\n\nsource\n\n\nCruncher.filter\n\n Cruncher.filter (in_pipeline:tspace.dataflow.pipeline.queue.Pipeline[pand\n                  as.core.frame.DataFrame], out_pipeline:tspace.dataflow.p\n                  ipeline.queue.Pipeline[pandas.core.frame.DataFrame],\n                  start_event:Optional[threading.Event],\n                  stop_event:Optional[threading.Event],\n                  interrupt_event:Optional[threading.Event],\n                  flash_event:Optional[threading.Event],\n                  exit_event:Optional[threading.Event])\n\n*Consume data from the pipeline\nArgs: in_pipeline: Pipeline, the input pipeline out_pipeline: Pipeline, the output pipeline start_event: Event, start event stop_event: Event, stop event interrupt_event: Event, interrupt event flash_event: Event, flash event exit_event: Event, exit event*\n\n\n\n\nType\nDetails\n\n\n\n\nin_pipeline\nPipeline\ninput pipeline\n\n\nout_pipeline\nPipeline\noutput pipeline\n\n\nstart_event\nOptional\ninput event start\n\n\nstop_event\nOptional\ninput event stop\n\n\ninterrupt_event\nOptional\ninput event interrupt\n\n\nflash_event\nOptional\ninput event flash\n\n\nexit_event\nOptional\ninput event exit\n\n\nReturns\nNone",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Dataflow</b>",
      "Cruncher"
    ]
  },
  {
    "objectID": "06.dataflow.filter.homo.html",
    "href": "06.dataflow.filter.homo.html",
    "title": "Homofilter",
    "section": "",
    "text": "source\n\nHomoFilter\n\n HomoFilter ()\n\nHomoegeneous filter class with the same type for input and output.\n\nsource\n\n\nHomoFilter.filter\n\n HomoFilter.filter\n                    (in_pipeline:tspace.dataflow.pipeline.queue.Pipeline[~\n                    T], out_pipeline:tspace.dataflow.pipeline.queue.Pipeli\n                    ne[~T], start_event:Optional[threading.Event],\n                    stop_event:Optional[threading.Event],\n                    interrupt_event:Optional[threading.Event],\n                    flash_event:Optional[threading.Event],\n                    exit_event:Optional[threading.Event])\n\nconsume data into the pipeline\n\n\n\n\nType\nDetails\n\n\n\n\nin_pipeline\nPipeline\n\n\n\nout_pipeline\nPipeline\noutput pipeline\n\n\nstart_event\nOptional\n\n\n\nstop_event\nOptional\n\n\n\ninterrupt_event\nOptional\ninput event\n\n\nflash_event\nOptional\n\n\n\nexit_event\nOptional\n\n\n\nReturns\nNone",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Dataflow</b>",
      "<b>Filter</b>",
      "Homofilter"
    ]
  },
  {
    "objectID": "sandbox.html",
    "href": "sandbox.html",
    "title": "sandbox",
    "section": "",
    "text": "proj_root\n\nPath('/Users/x/devel/tspace')\nsource",
    "crumbs": [
      "sandbox"
    ]
  },
  {
    "objectID": "sandbox.html#show-a-graph-with-mermaid",
    "href": "sandbox.html#show-a-graph-with-mermaid",
    "title": "sandbox",
    "section": "show a graph with mermaid",
    "text": "show a graph with mermaid\n\n\n\n\n\nflowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D[Result one]\n  C --&gt; E[Result two]\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nrun\n\nrun\n\n\n\nintr\n\nintr\n\n\n\nrun--intr\n\n\n\n\nkernel\n\nkernel\n\n\n\nrun--kernel\n\n\n\n\nrunbl\n\nrunbl\n\n\n\nintr--runbl\n\n\n\n\nrunbl--run\n\n\n\n\nzombie\n\nzombie\n\n\n\nkernel--zombie\n\n\n\n\nsleep\n\nsleep\n\n\n\nkernel--sleep\n\n\n\n\nrunmem\n\nrunmem\n\n\n\nkernel--runmem\n\n\n\n\nsleep--runmem\n\n\n\n\nswap\n\nswap\n\n\n\nsleep--swap\n\n\n\n\nrunswap\n\nrunswap\n\n\n\nswap--runswap\n\n\n\n\nrunswap--runmem\n\n\n\n\nnew\n\nnew\n\n\n\nrunswap--new\n\n\n\n\nnew--runmem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsequenceDiagram\n  participant Alice\n  participant Bob\n  Alice-&gt;&gt;John: Hello John, how are you?\n  loop Healthcheck\n    John-&gt;&gt;John: Fight against hypochondria\n  end\n  Note right of John: Rational thoughts &lt;br/&gt;prevail!\n  John--&gt;&gt;Alice: Great!\n  John-&gt;&gt;Bob: How about you?\n  Bob--&gt;&gt;John: Jolly good!\n\n\n\n\n\n\n\nsource\n\nsay_hi\n\n say_hi (to:str)\n\n*Say hello to somebody\nArgs: to: somebody to say hello to*\n\n\nExported source\ndef say_hi(to: str):\n    \"\"\"\n    Say hello to somebody\n\n\n    Args:\n        to: somebody to say hello to\n    \"\"\"\n\n    return f\"Hi {to}!\"\n\n\n\nsource\n\n\nsay_hello\n\n say_hello (to:str)\n\nSay hello to somebody\n\nassert say_hello(\"Jeremy\")==\"Hello Jeremy!\"\n\n\nfrom fastcore.test import *\n\n\ntest_eq(say_hello(\"Jeremy\"), \"Hello Jeremy!\")\n\n\nfrom IPython.display import display, SVG\n\n\ndisplay(SVG('&lt;svg height=\"100\" xmlns=\"http://www.w3.org/2000/svg\"&gt;&lt;circle cx=\"50\" cy=\"50\" r=\"40\"/&gt;&lt;/svg&gt; '))",
    "crumbs": [
      "sandbox"
    ]
  },
  {
    "objectID": "sandbox.html#show-a-graph-with-mermaid-1",
    "href": "sandbox.html#show-a-graph-with-mermaid-1",
    "title": "sandbox",
    "section": "show a graph with mermaid",
    "text": "show a graph with mermaid\n\n\n\n\n\nflowchart LR\n  A[Hard edge] --&gt; B(Round edge)\n  B --&gt; C{Decision}\n  C --&gt; D[Result one]\n  C --&gt; E[Result two]\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nrun\n\nrun\n\n\n\nintr\n\nintr\n\n\n\nrun--intr\n\n\n\n\nkernel\n\nkernel\n\n\n\nrun--kernel\n\n\n\n\nrunbl\n\nrunbl\n\n\n\nintr--runbl\n\n\n\n\nrunbl--run\n\n\n\n\nzombie\n\nzombie\n\n\n\nkernel--zombie\n\n\n\n\nsleep\n\nsleep\n\n\n\nkernel--sleep\n\n\n\n\nrunmem\n\nrunmem\n\n\n\nkernel--runmem\n\n\n\n\nsleep--runmem\n\n\n\n\nswap\n\nswap\n\n\n\nsleep--swap\n\n\n\n\nrunswap\n\nrunswap\n\n\n\nswap--runswap\n\n\n\n\nrunswap--runmem\n\n\n\n\nnew\n\nnew\n\n\n\nrunswap--new\n\n\n\n\nnew--runmem\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsequenceDiagram\n  participant Alice\n  participant Bob\n  Alice-&gt;&gt;John: Hello John, how are you?\n  loop Healthcheck\n    John-&gt;&gt;John: Fight against hypochondria\n  end\n  Note right of John: Rational thoughts &lt;br/&gt;prevail!\n  John--&gt;&gt;Alice: Great!\n  John-&gt;&gt;Bob: How about you?\n  Bob--&gt;&gt;John: Jolly good!",
    "crumbs": [
      "sandbox"
    ]
  },
  {
    "objectID": "sandbox.html#add-a-class",
    "href": "sandbox.html#add-a-class",
    "title": "sandbox",
    "section": "add a class",
    "text": "add a class\n\nsource\n\nHelloSayer\n\n HelloSayer (to)\n\nSay hello to to using say_hello\n\nsource\n\n\nHelloSayer.say\n\n HelloSayer.say ()\n\nDo the saying\n\no = HelloSayer(\"Alexis\")\no.say()\n\n'Hello Alexis!'\n\n\n\ndraw_n(3)\n\n[1, 2, 3]\n\n\n\nsource\n\n\ndraw_n\n\n draw_n (n:int, replace:bool=True)\n\nDraw n cards.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn\nint\n\n\n\n\nreplace\nbool\nTrue\nNumber of cards to draw # Draw with replacement?\n\n\nReturns\nlist\n\nList of cards\n\n\n\n\nsource\n\n\ndraw_np\n\n draw_np (n:int, replace:bool=True)\n\nDraw n cards.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn\nint\n\nNumber of cards to draw\n\n\nreplace\nbool\nTrue\nDraw with replacement?\n\n\nReturns\nlist\n\nList of cards\n\n\n\n\nsource\n\n\nMP\n\n MP (timestep:pandas._libs.tslibs.timestamps.Timestamp, velocity:float,\n     thrust:float, brake:float, current:float, voltage:float)\n\nMotion power tuple",
    "crumbs": [
      "sandbox"
    ]
  },
  {
    "objectID": "sandbox.html#add-math",
    "href": "sandbox.html#add-math",
    "title": "sandbox",
    "section": "add math",
    "text": "add math\n\\[\\sum_{i=1}^{k+1}i\\]",
    "crumbs": [
      "sandbox"
    ]
  },
  {
    "objectID": "05.storage.pool.pool.html",
    "href": "05.storage.pool.pool.html",
    "title": "Pool",
    "section": "",
    "text": "source\n\nPool\n\n Pool (_cnt:int=0)\n\n*The abstract storage for pooling the real-time data from the cloud.\nPool provides the following features:\n- load(): initialize the pool interface\n- close(): destructor\n- store(): Store the data in the pool\n- delete(id): remove the data with id in the pool\n- find(id): Get the data from the pool\n- sample(size, query: Optional[dict]=None): Sample a batch of data from the pool\n- count(query: Optional[dict] = None): Count the number of data in the pool\nAttributes: - _type_T: the type of the data stored in the pool - _cnt: the number of records in the pool*\n\nsource\n\n\nPool.load\n\n Pool.load ()\n\n*Initialize the pool interface\nThis function should: - connect to db - init*\n\nsource\n\n\nPool.close\n\n Pool.close ()\n\nclose the pool, for destructor\n\nsource\n\n\nPool.store\n\n Pool.store (item:~ItemT)\n\nDeposit an item (record) into the pool\n\nsource\n\n\nPool.delete\n\n Pool.delete (idx)\n\ndelete an itme by id or name.\n\nsource\n\n\nPool._count\n\n Pool._count (query:Optional[tspace.data.core.PoolQuery]=None)\n\n*Count the number of records in the db.\nquery = { vehicle_id: str = “VB7”, driver_id: str = “longfei-zheng”, dt_start: datetime = None, dt_end: datetime = None, }*\n\nsource\n\n\nPool.find\n\n Pool.find (query:tspace.data.core.PoolQuery)\n\nFind an item by id or name.\n\nsource\n\n\nPool.sample\n\n Pool.sample (size:int, query:Optional[tspace.data.core.PoolQuery]=None)\n\n*Sample a size of records from the pool.\nArgs:\nsize: desired size of the samples\nrule: an optional dictionary specifying a rule or a pipeline in mongodb\nquery: query to filter the records\nvehicle_id: str = \"VB7\",\ndriver_id: str = \"longfei-zheng\",\ndt_start: datetime = None,\ndt_end: datetime = None,\n}*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsize\nint\n\nnumber of required samples\n\n\nquery\nOptional[PoolQuery]\nNone\n\n\n\nReturns\nOptional[Union[pd.DataFrame, list[ItemT]]]\n\nquery to filter the records\n\n\n\n\nsource\n\n\nPool.__iter__\n\n Pool.__iter__ ()\n\nIterate over the pool.\n\nsource\n\n\nPool.__getitem__\n\n Pool.__getitem__ (query:tspace.data.core.PoolQuery)\n\nGet an item by id or name.\n\nsource\n\n\nPool.__len__\n\n Pool.__len__ ()\n\n\nsource\n\n\nPool.__repr__\n\n Pool.__repr__ ()\n\nReturn repr(self).",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Storage</b>",
      "<b>Pool</b>",
      "Pool"
    ]
  },
  {
    "objectID": "03.config.db.html",
    "href": "03.config.db.html",
    "title": "database",
    "section": "",
    "text": "pprint(db_config_list)\n\n\npprint(db_config_servers_by_name)\n\n\npprint(db_config_servers_by_host)\n\n\nsource\n\nget_db_config\n\n get_db_config (db_key:str)\n\n*Get the db config.\nArgs: db_key (str): string for db server name or format “usr:password@host:port”\nReturns: dict: db_config*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndb_key\nstr\nstring for db server name or format “usr:password@host:port”\n\n\nReturns\nDBConfig\nDBConfig object",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Config</b>",
      "database"
    ]
  },
  {
    "objectID": "04.conn.tbox.html",
    "href": "04.conn.tbox.html",
    "title": "tbox interface",
    "section": "",
    "text": "source\n\nTBoxCanException\n\n TBoxCanException (err_code:Optional[int]=0, extra_msg:Optional[str]=None,\n                   codes:collections.UserDict=&lt;factory&gt;)\n\n*Base class for all TBox CAN exceptions (Kvaser exceptions).\nArgs:\nerr_code (int): error code\nextra_msg (str): extra message\ncodes (UserDict): error code and message mapping*\n\nsource\n\n\nfloat_to_hex\n\n float_to_hex (value)\n\n\nsource\n\n\nhex_to_float\n\n hex_to_float (value)\n\n\nsource\n\n\nfloat_array_to_buffer\n\n float_array_to_buffer (float_array)\n\n\nsource\n\n\nparse_arg\n\n parse_arg ()\n\n\nsource\n\n\nwrite_json\n\n write_json (output_json_path, example_json_path, data)\n\nTest can only be run in the vehicle with real hardware\nvalue = [99.0] * 21 * 17 send_float_array(‘TQD_trqTrqSetECO_MAP_v’, value)\n\nsource\n\n\nsend_float_array\n\n send_float_array (name:str, float_df:pandas.core.frame.DataFrame,\n                   sw_diff:bool=False)\n\n*send float array to tbox simulator\nthe decorator prepend_string_arg is to set the default CAN ID for flashing torque table send_float_array(name, float_array, sw_diff) –&gt; send_float_array(float_array, sw_diff)*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\n\nstring for the CAN message name\n\n\nfloat_df\nDataFrame\n\nthe torque table to be flashed onto VBU\n\n\nsw_diff\nbool\nFalse\nwhether to use diff mode to accelerate flashing\n\n\nReturns\nNone",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Connection</b>",
      "tbox interface"
    ]
  },
  {
    "objectID": "06.dataflow.consumer.html",
    "href": "06.dataflow.consumer.html",
    "title": "Consumer",
    "section": "",
    "text": "source\n\nConsumer\n\n Consumer ()\n\nConsumer consumes data into the pipeline. It provides the unified interface for data consumption consume()\n\nsource\n\n\nConsumer.consume\n\n Consumer.consume (pipeline:tspace.dataflow.pipeline.queue.Pipeline[~T],\n                   start_event:Optional[threading.Event]=None,\n                   stop_event:Optional[threading.Event]=None,\n                   interrupt_event:Optional[threading.Event]=None,\n                   exit_event:Optional[threading.Event]=None,\n                   flash_event:Optional[threading.Event]=None)\n\nconsume data into the pipeline\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npipeline\nPipeline\n\nT: pd.DataFrame\n\n\nstart_event\nOptional\nNone\ninput event: start\n\n\nstop_event\nOptional\nNone\ninput event: stop\n\n\ninterrupt_event\nOptional\nNone\ninput event: interrupt\n\n\nexit_event\nOptional\nNone\ninput event: exit\n\n\nflash_event\nOptional\nNone\noutput event: flash",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Dataflow</b>",
      "Consumer"
    ]
  },
  {
    "objectID": "05.storage.buffer.mongo.html",
    "href": "05.storage.buffer.mongo.html",
    "title": "Mongo",
    "section": "",
    "text": "source\n\nMongoBuffer\n\n MongoBuffer (pool:Optional[tspace.storage.pool.mongo.MongoPool]=None,\n              batch_size:int, driver:tspace.config.drivers.Driver,\n              truck:tspace.config.vehicles.Truck,\n              meta:tspace.data.core.ObservationMeta,\n              db_config:tspace.config.db.DBConfig,\n              torque_table_row_names:list[str],\n              query:Optional[tspace.data.core.PoolQuery]=None, logger:Opti\n              onal[&lt;module'logging'from'/opt/hostedtoolcache/Python/3.11.9\n              /x64/lib/python3.11/logging/__init__.py'&gt;]=None,\n              dict_logger:Optional[dict]=None)\n\n*A Buffer connected with a MongoDB database pool\nArgs:\nbatch_size: number of documents for batch sampling\ndriver: driver of the vehicle\ntruck: subject vehicle\nmeta: metadata of the observation\ndb_config: config for mongodb (str) or DBConfig (dict)\nthe key leads to a config with db_name and collection name with a switch for record or episode:\n    - string for db server name\n    - or string of the format \"usr:password@host:port\"\n        for mongo_cluster:\n            Host=\"10.10.0.4\",  # url for the database server\n            port=\"23000\",  # port for the database server\n            user_name=\"admin\",  # username for the database server\n            password=\"ty02ydhVqDj3QFjT\",  # password for the database server\n            ==&gt; mongo_key = \"admin:ty02ydhVqDj3QFjT@10.10.0.4:23000\"\ntorque_table_row_names: list of torque table row names, e.g. ['r0, r1, ..., r9']\nquery: query for the pool\npool: pool of the database, a [`MongoPool`](https://Binjian.github.io/tspace/05.storage.pool.mongo.html#mongopool) instance\nlogger: logger for the buffer\ndict_logger: dict logger for the buffer*\n\nsource\n\n\nMongoBuffer.__post_init__\n\n MongoBuffer.__post_init__ ()\n\nset logger and load pool\n\nsource\n\n\nMongoBuffer.load\n\n MongoBuffer.load ()\n\nload pool from database\n\nsource\n\n\nMongoBuffer.find\n\n MongoBuffer.find (idx)\n\nfind a record by index\n\nsource\n\n\nMongoBuffer.close\n\n MongoBuffer.close ()\n\nclose the pool and the database\n\nsource\n\n\nMongoBuffer.sample\n\n MongoBuffer.sample ()\n\n*Sampling a batch of records or episodes from the pool.\nDecoding the batch observations from mongodb nested dicts to pandas dataframe flatten is used to adapt to model interface\nReturn:\nA quadruple of numpy arrays (states, actions, rewards, next_states)*\n\nsource\n\n\nMongoBuffer.decode_batch_records\n\n MongoBuffer.decode_batch_records (df:pandas.core.frame.DataFrame)\n\n*Sample a batch of records from the pool.\nArgs:\ndf: dataframe of sampled from the record collection of the pool\nReturn:\nA quadruple of numpy arrays (states, actions, rewards, next_states)*",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Storage</b>",
      "<b>Buffer</b>",
      "Mongo"
    ]
  },
  {
    "objectID": "02.system.plot.html",
    "href": "02.system.plot.html",
    "title": "plot",
    "section": "",
    "text": "source\n\nplot_3d_figure\n\n plot_3d_figure (table:pandas.core.frame.DataFrame)\n\n*Create a matplotlib 3d figure\nexport and save in log\nParameter:\ntable: pd.DataFrame with 3 columns: throttle, speed, torque\nReturn:\n    fig: matplotlib figure*\n\nsource\n\n\nplot_to_image\n\n plot_to_image (figure)\n\n*Converts the matplotlib plot specified by ‘figure’ to a PNG image and returns it.\nThe supplied figure is closed and inaccessible after this call.\nParameter:\n    figure: matplotlib figure\nReturn:\n        image: tf.Tensor*",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">System</b>",
      "plot"
    ]
  },
  {
    "objectID": "00.avatar.html",
    "href": "00.avatar.html",
    "title": "Avatar",
    "section": "",
    "text": "short_sha = repo.git.rev_parse(repo.head.commit.hexsha, short=7)\nrepo.head.commit.author.name\nrepo.head.commit.message\n\n'[add] generated code back\\n\\nfor CI\\n'\n\n\ngpus = tf.config.experimental.list_physical_devices(‘GPU’) tf.config.experimental.set_memory_growth(gpus[0], True)\nos.environ[“CUDA_VISIBLE_DEVICES”] = “-1” os.environ[“TF_FORCE_GPU_ALLOW_GROWTH”] = “true”\nsystem warnings and numpy warnings handling np.warnings.filterwarnings(‘ignore’, category=DeprecationWarning)\n\nsource\n\nAvatar\n\n Avatar (_truck:Union[tspace.config.vehicles.TruckInField,tspace.config.ve\n         hicles.TruckInCloud], _driver:tspace.config.drivers.Driver,\n         _can_server:tspace.config.messengers.CANMessenger,\n         _trip_server:Optional[tspace.config.messengers.TripMessenger],\n         _agent:tspace.agent.dpg.DPG, logger:logging.Logger,\n         dict_logger:dict, vehicle_interface:Union[tspace.dataflow.kvaser.\n         Kvaser,tspace.dataflow.cloud.Cloud]=None, _resume:bool=True,\n         _infer_mode:bool=False,\n         cruncher:Optional[tspace.dataflow.cruncher.Cruncher]=None,\n         data_root:pathlib.Path=Path('data'),\n         log_root:Optional[pathlib.Path]=None)\n\n*Description: Implement realtime reinforcement learning algorithm for training and inference convergence of ddpg and rdpg agent\nArgs:\n_truck: TruckInField or TruckInCloud\n_driver: Driver\n_agent: DDPG or RDPG\n_can_server: CANMessenger\n_trip_server: TripMessenger\n_resume: bool\n_infer_mode: bool\nlogger: logging.Logger\ndict_logger: dict\ndata_root: Path\nlog_root: Optional[Path]*\n\nsource\n\n\nAvatar.__post_init__\n\n Avatar.__post_init__ ()\n\n*Initialize logging, vehicle interface (Kvaser or Cloud), cruncher.\nAgent should have been initialized and passed in as argument.*\n\nclass C:\n    pass\n\n\nc = C()\n# parser.parse_args(args=['-v', 'VB7_FIELD'], namespace=c)\n# parser.parse_args(args=['-d', 'wang-kai'], namespace=c)\n# parser.parse_args(args=['-i', 'can_udp_svc'], namespace=c)\n# parser.parse_args(args=['-t', 'local_udp'], namespace=c)\n# parser.parse_args(args=['-c', 'UDP'], namespace=c)\n# parser.parse_args(args=['-a', 'ddpg'], namespace=c)\n# parser.parse_args(args=['-r'], namespace=c)\n# parser.parse_args(args=['-l'], namespace=c)\n# parser.parse_args(args=['-p', '.'], namespace=c)\n# parser.parse_args(args=['-o', 'mongo_local'], namespace=c)\nargs = parser.parse_args(\n    args=['-v', 'VB7_FIELD', \n    '-d', 'wang-kai', \n    '-i', 'can_udp_svc', \n    '-t', 'local_udp', \n    '-c', 'UDP', \n    '-a', 'ddpg', \n    '-l', \n    '-p', '.', \n    '-o', 'mongo_local', \n    '--watchdog_nap_time', '10', # test with 10 seconds nap time\n    '--watchdog_capture_error_upper_bound', '1',  # test with 1 failuire, since no hardware is connected\n    '--watchdog_flash_error_upper_bound', '1' ], namespace=c)    # test with 1 failuire, since no hardware is connected\n#| output: true\nargs.__dict__\n\n{'vehicle': 'VB7_FIELD',\n 'driver': 'wang-kai',\n 'interface': 'can_udp_svc',\n 'trip': 'local_udp',\n 'control': 'UDP',\n 'agent': 'ddpg',\n 'resume': False,\n 'learning': True,\n 'path': '.',\n 'output': 'mongo_local',\n 'watchdog_nap_time': '10',\n 'watchdog_capture_error_upper_bound': '1',\n 'watchdog_flash_error_upper_bound': '1'}\n\n\n\nsource\n\n\nmain\n\n main (args:argparse.Namespace)\n\n*Description: main function to start the Avatar.\nInitialize the Avatar with truck, driver, can server, trip server, and agent for input arguments. Create the first tier of the cascaded threading pools for vehicle interface and crucher.*\n\n!pwd\n\n/Users/x/devel/tspace/nbs\n\n\n\n!cp ../tspace/res/tbox/xcp_driver/json/example.json /dev/shm/out.json\n\ncp: ../tspace/res/tbox/xcp_driver/json/example.json: No such file or directory\n\n\n\n#main(args)",
    "crumbs": [
      "Avatar"
    ]
  },
  {
    "objectID": "03.config.vehicles.html",
    "href": "03.config.vehicles.html",
    "title": "vehicles",
    "section": "",
    "text": "pprint(PEDAL_SCALES,compact=True)\n\n\npprint(SPEED_SCALES_MULE,compact=True)\n\n\npprint(SPEED_SCALES_VB,compact=True)\n\n\npprint(TRIANGLE_TEST_CASE_TARGET_VELOCITIES, compact=True)\n\n\npprint(TruckCat, compact=True)\n\n\nMaturity\n\n\nRE_VIN\n\n\nsource\n\nOperationHistory\n\n OperationHistory (site:Optional[tspace.data.location.EosLocation]=None, d\n                   ate_range:tuple[pandas._libs.tslibs.timestamps.Timestam\n                   p,pandas._libs.tslibs.timestamps.Timestamp]=(Timestamp(\n                   '2022-12-01 00:00:00+0800', tz='Asia/Shanghai'),\n                   Timestamp('2023-12-31 00:00:00+0800',\n                   tz='Asia/Shanghai')))\n\n*History of the vehicle operation\nAttributes:\nsite: location of the vehicle\ndate_range: date range of the vehicle operation*\n\nsource\n\n\nKvaserMixin\n\n KvaserMixin ()\n\n*Mixin class for Kvaser interface\nAttributes:\n    kvaser_observation_number: number of observation in one unit\n    kvaser_observation_frequency: frequency of observation\n    kvaser_countdown: countdown time before observation*\n\nsource\n\n\nTboxMixin\n\n TboxMixin ()\n\n*Mixin class for Tbox interface\nfixed by hardware setting of remotecan\nAttributes:\n    tbox_signal_frequency: frequency of signal\n    tbox_gear_frequency: frequency of gear\n    tbox_unit_duration: duration of one unit\n    tbox_unit_number: number of units*\n\nsource\n\n\nTruck\n\n Truck (vid:str, vin:str, plate:str, maturity:str,\n        site:tspace.data.location.EosLocation,\n        operation_history:list[__main__.OperationHistory]=&lt;factory&gt;,\n        interface:str='', tbox_id:Optional[str]=None,\n        pedal_scale:tuple=&lt;factory&gt;,\n        _torque_table_col_num:Optional[int]=None,\n        speed_scale:tuple=&lt;factory&gt;,\n        _torque_table_row_num:Optional[int]=None,\n        observation_number:int=3, torque_budget:int=250,\n        torque_lower_bound:float=0.8, torque_upper_bound:float=1.0,\n        torque_bias:float=0.0, torque_table_row_num_flash:int=4,\n        cat:ordered_set.OrderedSet=&lt;factory&gt;,\n        _torque_flash_numel:Optional[int]=None,\n        _torque_full_numel:Optional[int]=None,\n        _observation_numel:Optional[float]=None,\n        _observation_length:Optional[int]=None,\n        _observation_sampling_rate:Optional[float]=None,\n        _observation_duration:Optional[float]=None)\n\n*Truck class\nAttributes:\nvid: vehicle id (short name) of the truck: VB7, M2, MP2, etc.\nvin: Vehicle Identification Number\nplate: License plate number\nmaturity: \"VB\", \"MULE\", \"MP\"\nsite: current Location of the truck\noperation_history: list of Operation history of the truck\ninterface: interface of the truck, \"cloud\" or \"kvaser\"\ntbox_id: Tbox id of the truck\npedal_scale: percentage of pedal opening [0, 100]\n_torque_table_col_num: number of columns/pedal_scales in the torque map\nspeed_scale: range of velocity [0, 100] in km/h\n_torque_table_row_num: number of rows/speed_scales in the torque map\nobservation_number: number of observation, 3: velocity, throttle, brake\ntorque_budget: maximal delta torque to be overlapped on the torque map 250 in Nm\ntorque_lower_bound: minimal percentage of delta torque to be overlapped on the torque map: 0.8\ntorque_upper_bound: maximal percentage of delta torque to be overlapped on the torque map: 1.0\ntorque_bias: bias of delta torque to be overlapped on the torque map: 0.0\ntorque_table_row_num_flash: number of rows to be flashed in the torque map: 4\ncat: category of the truck\n_torque_flash_numel: actually flashed number of torque items in the torque map\n_torque_full_numel: number of full torque items in the torque map\n_observation_numel: number of torque items in the torque map\n_observation_length: number of torque items in the torque map\n_observation_sampling_rate: sampling rate/frequency of the truck\n_observation_duration: sampling rate of the truck*\n\nsource\n\n\nTruckInCloud\n\n TruckInCloud (vid:str, vin:str, plate:str, maturity:str,\n               site:tspace.data.location.EosLocation, operation_history:li\n               st[__main__.OperationHistory]=&lt;factory&gt;, interface:str='',\n               tbox_id:Optional[str]=None, pedal_scale:tuple=&lt;factory&gt;,\n               _torque_table_col_num:Optional[int]=None,\n               speed_scale:tuple=&lt;factory&gt;,\n               _torque_table_row_num:Optional[int]=None,\n               observation_number:int=3, torque_budget:int=250,\n               torque_lower_bound:float=0.8, torque_upper_bound:float=1.0,\n               torque_bias:float=0.0, torque_table_row_num_flash:int=4,\n               cat:ordered_set.OrderedSet=&lt;factory&gt;,\n               _torque_flash_numel:Optional[int]=None,\n               _torque_full_numel:Optional[int]=None,\n               _observation_numel:Optional[float]=None,\n               _observation_length:Optional[int]=None,\n               _observation_sampling_rate:Optional[float]=None,\n               _observation_duration:Optional[float]=None)\n\n*Truck in cloud\nAttributes:\ninterface: interface of the truck, \"cloud\"\nobservation_length: length of observation\nobservation_numel: number of observation\nobservation_sampling_rate: sampling rate of observation\nobservation_duration: duration of observation\ntorque_table_row_num_flash: number of rows to be flashed in the torque map*\n\nsource\n\n\nTruckInField\n\n TruckInField (vid:str, vin:str, plate:str, maturity:str,\n               site:tspace.data.location.EosLocation, operation_history:li\n               st[__main__.OperationHistory]=&lt;factory&gt;, interface:str='',\n               tbox_id:Optional[str]=None, pedal_scale:tuple=&lt;factory&gt;,\n               _torque_table_col_num:Optional[int]=None,\n               speed_scale:tuple=&lt;factory&gt;,\n               _torque_table_row_num:Optional[int]=None,\n               observation_number:int=3, torque_budget:int=250,\n               torque_lower_bound:float=0.8, torque_upper_bound:float=1.0,\n               torque_bias:float=0.0, torque_table_row_num_flash:int=4,\n               cat:ordered_set.OrderedSet=&lt;factory&gt;,\n               _torque_flash_numel:Optional[int]=None,\n               _torque_full_numel:Optional[int]=None,\n               _observation_numel:Optional[float]=None,\n               _observation_length:Optional[int]=None,\n               _observation_sampling_rate:Optional[float]=None,\n               _observation_duration:Optional[float]=None)\n\n*Truck in field\nAttributes:\ninterface: interface of the truck, \"kvaser\"\nobservation_length: length of observation\nobservation_numel: number of observation\nobservation_sampling_rate: sampling rate of observation\nobservation_duration: duration of observation\ntorque_table_row_num_flash: number of rows to be flashed in the torque map*\n\npprint(trucks, compact=True)\n\n\npprint(trucks_all, compact=True)\n\n\npprint(trucks_by_id, compact=True)\n\n\ntrucks_by_vin",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Config</b>",
      "vehicles"
    ]
  },
  {
    "objectID": "03.config.drivers.html",
    "href": "03.config.drivers.html",
    "title": "drivers",
    "section": "",
    "text": "source\n\nDriver\n\n Driver (pid:str, name:str, site:tspace.data.location.EosLocation, contrac\n         t_range:tuple[pandas._libs.tslibs.timestamps.Timestamp,pandas._li\n         bs.tslibs.timestamps.Timestamp]=(Timestamp('2022-12-01\n         00:00:00+0800', tz='Asia/Shanghai'), Timestamp('2032-12-31\n         00:00:00+0800', tz='Asia/Shanghai')),\n         cat:ordered_set.OrderedSet=&lt;factory&gt;)\n\n*Driver configuration\nAttributes:\npid: driver id\nname: driver name\nsite: driver location\ncontract_range: contract range\ncat: driver category*\n\npprint(drivers_by_id)",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Config</b>",
      "drivers"
    ]
  },
  {
    "objectID": "07.agent.rdpg.actor.html",
    "href": "07.agent.rdpg.actor.html",
    "title": "SeqActor",
    "section": "",
    "text": "source\n\nSeqActor\n\n SeqActor (state_dim:int=0, action_dim:int=0, hidden_dim:int=0,\n           n_layers:int=0, batch_size:int=0, padding_value:float=0.0,\n           tau:float=0.0, lr:float=0.0, ckpt_dir:pathlib.Path=Path('.'),\n           ckpt_interval:int=0, logger:Optional[logging.Logger]=None,\n           dict_logger:Optional[dict]=None)\n\n*Sequential Actor network for the RDPG algorithm.\nAttributes:\n- state_dim (int): Dimension1 of the state space.\n- action_dim (int): Dimension of the action space.\n- hidden_dim (int): Dimension of the hidden layer.\n- lr (float): Learning rate for the network.\n- ckpt_dir (str): Directory to restore the checkpoint from.*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstate_dim\nint\n0\ndimension of the state space\n\n\naction_dim\nint\n0\ndimension of the action space\n\n\nhidden_dim\nint\n0\ndimension of the hidden layer\n\n\nn_layers\nint\n0\nnumber of lstm layers\n\n\nbatch_size\nint\n0\nbatch size\n\n\npadding_value\nfloat\n0.0\npadding value for masking\n\n\ntau\nfloat\n0.0\nsoft update parameter\n\n\nlr\nfloat\n0.0\nlearning rate\n\n\nckpt_dir\nPath\n.\ncheckpoint directory\n\n\nckpt_interval\nint\n0\ncheckpoint interval\n\n\nlogger\nOptional\nNone\nlogger\n\n\ndict_logger\nOptional\nNone\nlogger dict\n\n\n\n\nsource\n\n\nSeqActor.clone_weights\n\n SeqActor.clone_weights (moving_net)\n\nClone weights from a model to another model. only for target critic\n\nsource\n\n\nSeqActor.soft_update\n\n SeqActor.soft_update (moving_net)\n\nUpdate the target weights.\n\nsource\n\n\nSeqActor.save_ckpt\n\n SeqActor.save_ckpt ()\n\nSave the checkpoint.\n\nsource\n\n\nSeqActor.reset_noise\n\n SeqActor.reset_noise ()\n\nReset the ou_noise.\n\nsource\n\n\nSeqActor.predict\n\n SeqActor.predict (states:tensorflow.python.framework.tensor.Tensor,\n                   last_actions:tensorflow.python.framework.tensor.Tensor)\n\n*Predict the action given the state. Batch dimension needs to be one.\nArgs:\nstates: State, Batch dimension needs to be one.\nlast_actions: Last action, Batch dimension needs to be one.\nReturn: Action*\n\nsource\n\n\nSeqActor.predict_step\n\n SeqActor.predict_step (states, last_actions)\n\n*Predict the action given the state.\nFor Inferring\nArgs:\nstates (tf.Tensor): State, Batch dimension needs to be one.\nlast_actions (tf.Tensor): State, Batch dimension needs to be one.\nReturns:\nnp.array: Action, ditch the batch dimension*\n\nsource\n\n\nSeqActor.evaluate_actions\n\n SeqActor.evaluate_actions (states, last_actions)\n\n*Evaluate the action given the state.\nFor training\nArgs:\nstates (tf.Tensor): State, Batch dimension needs to be one.\nlast_actions (tf.Tensor): State, Batch dimension needs to be one.\nReturn: np.array: Action, keep the batch dimension*",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Agent</b>",
      "<b>rdpg</b>",
      "SeqActor"
    ]
  },
  {
    "objectID": "06.dataflow.producer.html",
    "href": "06.dataflow.producer.html",
    "title": "Producer",
    "section": "",
    "text": "source\n\nProducer\n\n Producer ()\n\nProducer produce data into the pipeline. It provides the unified interface for data capturing interface produce()\n\nsource\n\n\nProducer.produce\n\n Producer.produce\n                   (raw_pipeline:tspace.dataflow.pipeline.deque.PipelineDQ\n                   [~T_RAW], hmi_pipeline:Optional[tspace.dataflow.pipelin\n                   e.queue.Pipeline[~T_HMI]]=None,\n                   exit_event:Optional[threading.Event]=None)\n\nProduce data into the pipeline\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraw_pipeline\nPipelineDQ\n\n\n\n\nhmi_pipeline\nOptional\nNone\nRaw pipeline is deque to keep data fresh and ignore stale data, such as one with dict[str,str]\n\n\nexit_event\nOptional\nNone\nHMI pipeline is Queue, such as one with str",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Dataflow</b>",
      "Producer"
    ]
  },
  {
    "objectID": "06.dataflow.kvaser.html",
    "href": "06.dataflow.kvaser.html",
    "title": "Kvaser",
    "section": "",
    "text": "source\n\nKvaser\n\n Kvaser (truck:tspace.config.vehicles.TruckInField, can_server:tspace.conf\n         ig.messengers.CANMessenger=CANMessenger(server_name='can_udp_svc'\n         , host='127.0.0.1', port='8002', protocol='udp'),\n         driver:tspace.config.drivers.Driver, resume:bool=False,\n         data_dir:Optional[pathlib.Path]=None, flash_count:int=0,\n         episode_count:int=0, vcu_calib_table_row_start:int=0,\n         torque_table_default:Optional[pandas.core.frame.DataFrame]=None,\n         torque_table_live:Optional[pandas.core.frame.DataFrame]=None,\n         epi_countdown_time:float=3.0, lock_watchdog:&lt;built-\n         infunctionallocate_lock&gt;=&lt;unlocked _thread.lock object at\n         0x7f9a997f6f80&gt;, capture_failure_count:int=0,\n         flash_failure_count:int=0, logger:Optional[logging.Logger]=None,\n         dict_logger:Optional[dict]=None)\n\n*Kvaser is local vehicle interface with Producer(get vehicle status) and Consumer(flasher)\nAttributes:\ntruck: TruckInField\n    truck object\ncan_server: CANMessenger\n    can server object*\n\nsource\n\n\nKvaser.flash_vehicle\n\n Kvaser.flash_vehicle (torque_table:pandas.core.frame.DataFrame)\n\nflash the torque table to the vehicle via kvaser\n\n\n\n\nType\nDetails\n\n\n\n\ntorque_table\nDataFrame\nthe torque table to be flashed\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nKvaser.init_internal_pipelines\n\n Kvaser.init_internal_pipelines ()\n\ninitialize the internal pipelines for kvaser\n\nsource\n\n\nKvaser.produce\n\n Kvaser.produce (raw_pipeline:tspace.dataflow.pipeline.deque.PipelineDQ[ty\n                 ping.Union[dict[str,str],dict[str,dict[str,list[typing.Un\n                 ion[str,list[list[str]]]]]]]], hmi_pipeline:Optional[tspa\n                 ce.dataflow.pipeline.queue.Pipeline[str]]=None,\n                 exit_event:Optional[threading.Event]=None)\n\nproduce data from kvaser and put into the pipeline\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraw_pipeline\nPipelineDQ\n\nPipelineDQ[dict[str, str]],\n\n\nhmi_pipeline\nOptional\nNone\nHMI pipeline\n\n\nexit_event\nOptional\nNone\ninput event exit\n\n\n\n\nsource\n\n\nKvaser.filter\n\n Kvaser.filter (in_pipeline:tspace.dataflow.pipeline.deque.PipelineDQ[typi\n                ng.Union[dict[str,str],dict[str,dict[str,list[typing.Union\n                [str,list[list[str]]]]]]]], out_pipeline:tspace.dataflow.p\n                ipeline.queue.Pipeline[pandas.core.frame.DataFrame],\n                start_event:Optional[threading.Event],\n                stop_event:Optional[threading.Event],\n                interrupt_event:Optional[threading.Event],\n                flash_event:Optional[threading.Event],\n                exit_event:Optional[threading.Event])\n\nfilter data from kvaser input pipeline and put into the output pipeline\n\n\n\n\nType\nDetails\n\n\n\n\nin_pipeline\nPipelineDQ\ninput PipelineDQ[dict[str, str]],\n\n\nout_pipeline\nPipeline\noutput Pipeline[pd.DataFrame],\n\n\nstart_event\nOptional\ninput event start\n\n\nstop_event\nOptional\ninput event stop\n\n\ninterrupt_event\nOptional\ninput event interrupt\n\n\nflash_event\nOptional\n\n\n\nexit_event\nOptional\ninput event exit\n\n\nReturns\nNone",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Dataflow</b>",
      "Kvaser"
    ]
  },
  {
    "objectID": "01.data.external.pandas_utils.html",
    "href": "01.data.external.pandas_utils.html",
    "title": "Pandas utilities",
    "section": "",
    "text": "source\n\n\n\n assemble_state_ser (state_columns:pandas.core.frame.DataFrame,\n                     tz:zoneinfo.ZoneInfo)\n\n*assemble state df from state_columns dataframe order is vital for the model\ninputs:\nstate_columns: pd.DataFrame\n“timestep, velocity, thrust, brake” contiguous storage in each measurement due to sort_index, output: [col0: brake, col1: thrust, col2: timestep, col3: velocity]\nreturn:\nstate: pd.Series\ntable_row_start: int*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nstate_columns\nDataFrame\nstate_columns: Dataframe with columns [‘timestep’, ‘velocity’, ‘thrust’, ‘brake’]\n\n\ntz\nZoneInfo\ntimezone for the timestamp\n\n\nReturns\nTuple\n\n\n\n\n\nassemble_state_ser(state, tz)[0]  # just showd the Dataframe, ignore row_start (its's 0)\n\n\nassert assemble_state_ser(state, tz)[1] == 0  # row_start should be 0\n\n\nassert isinstance(assemble_state_ser(state, tz)[0], pd.Series) == True\n\n\nfrom fastcore.test import *\n\n\ntest_eq(isinstance(assemble_state_ser(state, tz)[0], pd.Series), True)  # use fastcore testing utils\n\n\nsource\n\n\n\n\n assemble_reward_ser (power_columns:pandas.core.frame.DataFrame,\n                      obs_sampling_rate:int, ts)\n\nassemble reward df from motion_power df order is vital for the model: contiguous storage in each row, due to sort_index, output: power_columns: [‘current’, ‘voltage’] [timestep, work]\n\nsource\n\n\n\n\n assemble_flash_table (torque_map_line:numpy.ndarray, table_start:int,\n                       torque_table_row_num_flash:int,\n                       torque_table_col_num:int, speed_scale:tuple,\n                       pedal_scale:tuple)\n\ngenerate flash table df from torque_map_line order is vital for the model: contiguous storage in each row, due to sort_index, output: “r0, r1, r2, r3, …, speed, throttle(map),timestep”\n\nsource\n\n\n\n\n assemble_action_ser (torque_map_line:numpy.ndarray,\n                      torque_table_row_names:list[str], table_start:int, f\n                      lash_start_ts:pandas._libs.tslibs.timestamps.Timesta\n                      mp, flash_end_ts:pandas._libs.tslibs.timestamps.Time\n                      stamp, torque_table_row_num_flash:int,\n                      torque_table_col_num:int, speed_scale:tuple,\n                      pedal_scale:tuple, tz:zoneinfo.ZoneInfo)\n\ngenerate action df from torque_map_line order is vital for the model: contiguous storage in each row, due to sort_index, output: “r0, r1, r2, r3, …, speed, throttle(map),timestep”\n\ndf[\"action\"]\n\n\nc = df[\"action\", \"timestep\", 0].values\nc\n\n\ndf[\"action\", \"timestep\"].iloc[0].values\n\n\naction_ser = action['action'].iloc[0]\naction_ser.name = \"action\"\naction_ser\n\n\n# state = df['state'][\"timestep\"]\n# state[\"timestep\"].values\nactn = df[\"action\"].iloc[0]\nactn[\"r0\"].values\n\n\n## The construction of DF by raw values will lose timezone information\n# So you have to alway not directly build Dataframe from numpy array values\naction1 = pd.DataFrame(\n    [actn[\"r0\"].values, \n     actn[\"r1\"].values, \n     actn[\"r2\"].values, \n     actn[\"speed\"].values,\n     actn[\"throttle\"].values, \n     actn[\"timestep\"].values]\n).T\naction1.columns = [\"r0\", \"r1\", \"r2\", \"speed\", \"throttle\", \"timestep\"]\naction1\n\n\ntorque_table_row_names = [\"r0\", \"r1\", \"r2\"]\ntable_start = 4\ntorque_table_row_num_flash = 3\ntorque_table_col_num = 5\nspeed_scale = (0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120)\npedal_scale = (0, 0.25, 0.5, 0.75, 1.0)\ntz = ZoneInfo(\"Asia/Shanghai\")\n\n# state = df['state'].stack\nser_action = assemble_action_ser(\n    torque_table_line,\n    torque_table_row_names,\n    table_start,\n    flash_start_ts,\n    flash_end_ts,\n    torque_table_row_num_flash,\n    torque_table_col_num,\n    speed_scale,\n    pedal_scale,\n    tz)\n\n\nassert assemble_state_ser(state, tz)[1] == 0  # row_start should be 0\n\n\nassert isinstance(assemble_state_ser(state, tz)[0], pd.Series) == True\n\n\nfrom fastcore.test import *\n\n\ntest_eq(isinstance(assemble_state_ser(state, tz)[0], pd.Series), True)  # use fastcore testing utils\n\n\nsource\n\n\n\n\n nest (d:dict)\n\nConvert a flat dictionary with tuple key to a nested dictionary through to the leaves arrays will be converted to dictionaries with the index as the key no conversion of pd.Timestamp only for use in mongo records\n\nsource\n\n\n\n\n df_to_nested_dict (df_multi_indexed_col:pandas.core.frame.DataFrame)\n\nConvert a dataframe with multi-indexed columns to a nested dictionary\n\nsource\n\n\n\n\n eos_df_to_nested_dict (episode:pandas.core.frame.DataFrame)\n\nConvert an eos dataframe with multi-indexed columns to a nested dictionary Remove all the levels of the multi-indexed columns except for ‘timestamp’ Keep only the timestamp as the single key for the nested dictionary\n\nsource\n\n\n\n\n ep_nest (d:Dict)\n\nConvert a flat dictionary with tuple key to a nested dictionary with arrays at the leaves convert pd.Timestamp to millisecond long integer Timestamp with zoneinfo will be converted to UTC and then to millisecond long integer\n\nsource\n\n\n\n\n df_to_ep_nested_dict (df_multi_indexed_col:pandas.core.frame.DataFrame)\n\nConvert a dataframe with multi-indexed columns to a nested dictionary\n\nsource\n\n\n\n\n avro_ep_encoding (episode:pandas.core.frame.DataFrame)\n\n*avro encoding, parsing requires a schema defined in “data_io/pool/episode_avro_schema.py”\nConvert an eos dataframe with multi-indexed columns to a nested dictionary Remove all the levels of the multi-indexed columns except for ‘timestamp’ Keep only the timestamp as the single key for the nested dictionary ! Convert Timestamp to millisecond long integer!! for compliance to the avro storage format ! Timestamp with ZoneInfo will be converted to UTC and then to millisecond long integer as flat as possible PEP20: flat is better than nested!*\n\nsource\n\n\n\n\n avro_ep_decoding (episodes:list[typing.Dict],\n                   tz_info:Optional[zoneinfo.ZoneInfo])\n\n*avro decoding,\nConvert a list of nested dictionaries to DataFrame with multi-indexed columns and index ! Convert microsecond long integer to Timestamp! (avro storage format stores timestamp as long integer in keys but seem to have DateTime with timezone in the values.)\nApache Avro store datetime/timestamp as timezone unaware (default as UTC) Therefore, we need tz info either in the metadata or elsewhere to designate the timezone\nsort the column order*\n\nsource\n\n\n\n\n decode_mongo_records (df:pandas.core.frame.DataFrame,\n                       torque_table_row_names:list[str])\n\ndecoding the batch RECORD observations from mongodb nested dicts to pandas dataframe (EPISODE doesn’t need decoding, it is already a dataframe) TODO need to check whether sort_index is necessary\n\nsource\n\n\n\n\n decode_mongo_episodes (df:pandas.core.frame.DataFrame)\n\ndecoding the batch RECORD observations from mongodb nested dicts to pandas dataframe (EPISODE doesn’t need decoding, it is already a dataframe) TODO need to check whether sort_index is necessary\n\nsource\n\n\n\n\n encode_dataframe_from_parquet (df:pandas.core.frame.DataFrame)\n\ndecode the dataframe from parquet with flat column indices to MultiIndexed DataFrame\n\nsource\n\n\n\n\n decode_episode_batch_to_padded_arrays\n                                        (episodes:pandas.core.frame.DataFr\n                                        ame,\n                                        torque_table_row_names:list[str],\n                                        padding_value:float=-10000.0)\n\n*decode the dataframes to 3D numpy arrays [B, T, F] for states, actions, rewards, next_states episodes with variable lengths will turn into ragged arrays with the same raggedness, thus the same maximum length after padding the arrays will have the same shape and padding pattern.\nepisodes are not sorted and its internal index keeps the index order of the original episodes, not interleaved! idx_len_list: list of lengths of each episode in the batch, use explicit segmentation to avoid the bug, when the batch has duplicated episodes*\n\nsource\n\n\n\n\n encode_episode_dataframe_from_series\n                                       (observations:List[pandas.core.seri\n                                       es.Series],\n                                       torque_table_row_names:List[str],\n                                       episode_start_dt:datetime.datetime,\n                                       driver_str:str, truck_str:str)\n\nencode the list of observations as a dataframe with multi-indexed columns\n\nsource\n\n\n\n\n recover_episodestart_tzinfo_from_timestamp\n                                             (ts:pandas._libs.tslibs.times\n                                             tamps.Timestamp,\n                                             tzinfo:zoneinfo.ZoneInfo)\n\nrecover the timezone information from the parquet folder name string",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Data</b>",
      "<b>External</b>",
      "Pandas utilities"
    ]
  },
  {
    "objectID": "01.data.external.pandas_utils.html#dataframe-for-state-action-reward-next_state",
    "href": "01.data.external.pandas_utils.html#dataframe-for-state-action-reward-next_state",
    "title": "Pandas utilities",
    "section": "",
    "text": "source\n\n\n\n assemble_state_ser (state_columns:pandas.core.frame.DataFrame,\n                     tz:zoneinfo.ZoneInfo)\n\n*assemble state df from state_columns dataframe order is vital for the model\ninputs:\nstate_columns: pd.DataFrame\n“timestep, velocity, thrust, brake” contiguous storage in each measurement due to sort_index, output: [col0: brake, col1: thrust, col2: timestep, col3: velocity]\nreturn:\nstate: pd.Series\ntable_row_start: int*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nstate_columns\nDataFrame\nstate_columns: Dataframe with columns [‘timestep’, ‘velocity’, ‘thrust’, ‘brake’]\n\n\ntz\nZoneInfo\ntimezone for the timestamp\n\n\nReturns\nTuple\n\n\n\n\n\nassemble_state_ser(state, tz)[0]  # just showd the Dataframe, ignore row_start (its's 0)\n\n\nassert assemble_state_ser(state, tz)[1] == 0  # row_start should be 0\n\n\nassert isinstance(assemble_state_ser(state, tz)[0], pd.Series) == True\n\n\nfrom fastcore.test import *\n\n\ntest_eq(isinstance(assemble_state_ser(state, tz)[0], pd.Series), True)  # use fastcore testing utils\n\n\nsource\n\n\n\n\n assemble_reward_ser (power_columns:pandas.core.frame.DataFrame,\n                      obs_sampling_rate:int, ts)\n\nassemble reward df from motion_power df order is vital for the model: contiguous storage in each row, due to sort_index, output: power_columns: [‘current’, ‘voltage’] [timestep, work]\n\nsource\n\n\n\n\n assemble_flash_table (torque_map_line:numpy.ndarray, table_start:int,\n                       torque_table_row_num_flash:int,\n                       torque_table_col_num:int, speed_scale:tuple,\n                       pedal_scale:tuple)\n\ngenerate flash table df from torque_map_line order is vital for the model: contiguous storage in each row, due to sort_index, output: “r0, r1, r2, r3, …, speed, throttle(map),timestep”\n\nsource\n\n\n\n\n assemble_action_ser (torque_map_line:numpy.ndarray,\n                      torque_table_row_names:list[str], table_start:int, f\n                      lash_start_ts:pandas._libs.tslibs.timestamps.Timesta\n                      mp, flash_end_ts:pandas._libs.tslibs.timestamps.Time\n                      stamp, torque_table_row_num_flash:int,\n                      torque_table_col_num:int, speed_scale:tuple,\n                      pedal_scale:tuple, tz:zoneinfo.ZoneInfo)\n\ngenerate action df from torque_map_line order is vital for the model: contiguous storage in each row, due to sort_index, output: “r0, r1, r2, r3, …, speed, throttle(map),timestep”\n\ndf[\"action\"]\n\n\nc = df[\"action\", \"timestep\", 0].values\nc\n\n\ndf[\"action\", \"timestep\"].iloc[0].values\n\n\naction_ser = action['action'].iloc[0]\naction_ser.name = \"action\"\naction_ser\n\n\n# state = df['state'][\"timestep\"]\n# state[\"timestep\"].values\nactn = df[\"action\"].iloc[0]\nactn[\"r0\"].values\n\n\n## The construction of DF by raw values will lose timezone information\n# So you have to alway not directly build Dataframe from numpy array values\naction1 = pd.DataFrame(\n    [actn[\"r0\"].values, \n     actn[\"r1\"].values, \n     actn[\"r2\"].values, \n     actn[\"speed\"].values,\n     actn[\"throttle\"].values, \n     actn[\"timestep\"].values]\n).T\naction1.columns = [\"r0\", \"r1\", \"r2\", \"speed\", \"throttle\", \"timestep\"]\naction1\n\n\ntorque_table_row_names = [\"r0\", \"r1\", \"r2\"]\ntable_start = 4\ntorque_table_row_num_flash = 3\ntorque_table_col_num = 5\nspeed_scale = (0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120)\npedal_scale = (0, 0.25, 0.5, 0.75, 1.0)\ntz = ZoneInfo(\"Asia/Shanghai\")\n\n# state = df['state'].stack\nser_action = assemble_action_ser(\n    torque_table_line,\n    torque_table_row_names,\n    table_start,\n    flash_start_ts,\n    flash_end_ts,\n    torque_table_row_num_flash,\n    torque_table_col_num,\n    speed_scale,\n    pedal_scale,\n    tz)\n\n\nassert assemble_state_ser(state, tz)[1] == 0  # row_start should be 0\n\n\nassert isinstance(assemble_state_ser(state, tz)[0], pd.Series) == True\n\n\nfrom fastcore.test import *\n\n\ntest_eq(isinstance(assemble_state_ser(state, tz)[0], pd.Series), True)  # use fastcore testing utils\n\n\nsource\n\n\n\n\n nest (d:dict)\n\nConvert a flat dictionary with tuple key to a nested dictionary through to the leaves arrays will be converted to dictionaries with the index as the key no conversion of pd.Timestamp only for use in mongo records\n\nsource\n\n\n\n\n df_to_nested_dict (df_multi_indexed_col:pandas.core.frame.DataFrame)\n\nConvert a dataframe with multi-indexed columns to a nested dictionary\n\nsource\n\n\n\n\n eos_df_to_nested_dict (episode:pandas.core.frame.DataFrame)\n\nConvert an eos dataframe with multi-indexed columns to a nested dictionary Remove all the levels of the multi-indexed columns except for ‘timestamp’ Keep only the timestamp as the single key for the nested dictionary\n\nsource\n\n\n\n\n ep_nest (d:Dict)\n\nConvert a flat dictionary with tuple key to a nested dictionary with arrays at the leaves convert pd.Timestamp to millisecond long integer Timestamp with zoneinfo will be converted to UTC and then to millisecond long integer\n\nsource\n\n\n\n\n df_to_ep_nested_dict (df_multi_indexed_col:pandas.core.frame.DataFrame)\n\nConvert a dataframe with multi-indexed columns to a nested dictionary\n\nsource\n\n\n\n\n avro_ep_encoding (episode:pandas.core.frame.DataFrame)\n\n*avro encoding, parsing requires a schema defined in “data_io/pool/episode_avro_schema.py”\nConvert an eos dataframe with multi-indexed columns to a nested dictionary Remove all the levels of the multi-indexed columns except for ‘timestamp’ Keep only the timestamp as the single key for the nested dictionary ! Convert Timestamp to millisecond long integer!! for compliance to the avro storage format ! Timestamp with ZoneInfo will be converted to UTC and then to millisecond long integer as flat as possible PEP20: flat is better than nested!*\n\nsource\n\n\n\n\n avro_ep_decoding (episodes:list[typing.Dict],\n                   tz_info:Optional[zoneinfo.ZoneInfo])\n\n*avro decoding,\nConvert a list of nested dictionaries to DataFrame with multi-indexed columns and index ! Convert microsecond long integer to Timestamp! (avro storage format stores timestamp as long integer in keys but seem to have DateTime with timezone in the values.)\nApache Avro store datetime/timestamp as timezone unaware (default as UTC) Therefore, we need tz info either in the metadata or elsewhere to designate the timezone\nsort the column order*\n\nsource\n\n\n\n\n decode_mongo_records (df:pandas.core.frame.DataFrame,\n                       torque_table_row_names:list[str])\n\ndecoding the batch RECORD observations from mongodb nested dicts to pandas dataframe (EPISODE doesn’t need decoding, it is already a dataframe) TODO need to check whether sort_index is necessary\n\nsource\n\n\n\n\n decode_mongo_episodes (df:pandas.core.frame.DataFrame)\n\ndecoding the batch RECORD observations from mongodb nested dicts to pandas dataframe (EPISODE doesn’t need decoding, it is already a dataframe) TODO need to check whether sort_index is necessary\n\nsource\n\n\n\n\n encode_dataframe_from_parquet (df:pandas.core.frame.DataFrame)\n\ndecode the dataframe from parquet with flat column indices to MultiIndexed DataFrame\n\nsource\n\n\n\n\n decode_episode_batch_to_padded_arrays\n                                        (episodes:pandas.core.frame.DataFr\n                                        ame,\n                                        torque_table_row_names:list[str],\n                                        padding_value:float=-10000.0)\n\n*decode the dataframes to 3D numpy arrays [B, T, F] for states, actions, rewards, next_states episodes with variable lengths will turn into ragged arrays with the same raggedness, thus the same maximum length after padding the arrays will have the same shape and padding pattern.\nepisodes are not sorted and its internal index keeps the index order of the original episodes, not interleaved! idx_len_list: list of lengths of each episode in the batch, use explicit segmentation to avoid the bug, when the batch has duplicated episodes*\n\nsource\n\n\n\n\n encode_episode_dataframe_from_series\n                                       (observations:List[pandas.core.seri\n                                       es.Series],\n                                       torque_table_row_names:List[str],\n                                       episode_start_dt:datetime.datetime,\n                                       driver_str:str, truck_str:str)\n\nencode the list of observations as a dataframe with multi-indexed columns\n\nsource\n\n\n\n\n recover_episodestart_tzinfo_from_timestamp\n                                             (ts:pandas._libs.tslibs.times\n                                             tamps.Timestamp,\n                                             tzinfo:zoneinfo.ZoneInfo)\n\nrecover the timezone information from the parquet folder name string",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Data</b>",
      "<b>External</b>",
      "Pandas utilities"
    ]
  },
  {
    "objectID": "01.data.core.html",
    "href": "01.data.core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nMotionPower\n\n MotionPower (timestep:pandas._libs.tslibs.timestamps.Timestamp,\n              velocity:float, thrust:float, brake:float, current:float,\n              voltage:float)\n\n*Motion power tuple for raw data captured by Kvaser\nAttributes:\ntimestep: timestamp of the tuple\nvelocity: velocity in m/s,\nthrust: thrust in percentage of full acc pedal,\nbrake: brake in percentage of full brake pedal,\ncurrent: current in A,\nvoltage: voltage in V,\nreturn:\n    MotionPower tuple*\n\nsource\n\n\nECUMixin\n\n ECUMixin (ecu_observation_number:int=30,\n           ecu_observation_frequency:int=20, ecu_countdown:int=3)\n\n*ECU mixin class, for inheriting by Kvaser generated observation data\nAttributes:\necu_observation_number: number of observations per ECU\necu_observation_frequency: frequency of observations per ECU\necu_countdown: countdown time for ECU\nreturn:\n    ECUMixin*\n\nsource\n\n\nCloudMixin\n\n CloudMixin (cloud_signal_frequency:int=50, cloud_gear_frequency:int=2,\n             cloud_unit_duration:int=1, cloud_unit_number:int=4)\n\n*Cloud mixin class, for inheriting by tbox generated observation data\nAttributes:\ncloud_signal_frequency: frequency of observations of Tbox\ncloud_gear_frequency: frequency of gear signal of Tbox\ncloud_unit_duration: unit duration of Tbox in seconds\ncloud_unit_number: number of observation units of Tbox\nreturn:\n    CloudMixin*\n\nsource\n\n\nStateUnitCodes\n\n StateUnitCodes (velocity_unit_code:str='kph', thrust_unit_code:str='pct',\n                 brake_unit_code:str='pct')\n\n*Observation of the episode\nattribute:\nvelocity_unit_code: unit of velocity, default \"kph\"\nthrust_unit_code: unit of thrust, default \"pct\"\nbrake_unit_code: unit of brake, default \"pct\"*\n\nsource\n\n\nStateSpecs\n\n StateSpecs (state_unit_codes:__main__.StateUnitCodes=&lt;factory&gt;,\n             state_number:int=3, unit_number_per_state:int=200,\n             unit_duration:float=1.0, frequency:int=50)\n\n*Observation of the episode\nattributes:\n    state_unit_codes: StateUnitCodes\n    state_number: number of states, default 3, velocity, thrust, brake\n    unit_number_per_state: number of units, default 4\n    unit_duration: duration of each unit, default 1 second\n    frequency: frequency of each unit, default 50 Hz*\n\nsource\n\n\nStateSpecsCloud\n\n StateSpecsCloud (state_unit_codes:__main__.StateUnitCodes=&lt;factory&gt;,\n                  cloud_interface:__main__.CloudMixin=&lt;factory&gt;,\n                  state_number:int=3, unit_number_per_state:int=200,\n                  unit_duration:float=1.0, frequency:int=50)\n\n*StateSpecs for cloud interface\nattributes:\ncloud_interface: CloudMixin*\n\nsource\n\n\nStateSpecsECU\n\n StateSpecsECU (state_unit_codes:__main__.StateUnitCodes=&lt;factory&gt;,\n                ecu_interface:__main__.ECUMixin=&lt;factory&gt;,\n                state_number:int=3, unit_number_per_state:int=200,\n                unit_duration:float=1.0, frequency:int=50)\n\n*StateSpecs for Kvaser interface\nattributes:\necu_interface: ECUMixin*\n\nsource\n\n\nActionSpecs\n\n ActionSpecs (action_unit_code:str='nm', action_row_number:int=4,\n              action_column_number:int=17)\n\n*Action of the episode\nattributes:\naction_unit_code: unit of action, default \"nm\"\naction_row_number: number of rows, default 4\naction_column_number: number of columns, default 17*\n\nsource\n\n\nRewardSpecs\n\n RewardSpecs (reward_unit_code:str='wh', reward_number:int=1)\n\n*Reward of the episode\nAttributes:\nreward_unit_code: unit of reward, default \"wh\"\nreward_number: number of rewards, default 1, current reward, can be extended to multiple past rewards*\n\nsource\n\n\nObservationMeta\n\n ObservationMeta (state_specs:__main__.StateSpecs,\n                  action_specs:__main__.ActionSpecs,\n                  reward_specs:__main__.RewardSpecs,\n                  site:tspace.data.location.EosLocation)\n\n*selected metadata for db document matching pandas DataFrame\nAttributes:\nstate_specs: StateSpecs\naction_specs: ActionSpecs\nreward_specs: RewardSpecs\nsite: EosLocation  # \"at\"  # observation (testing) site*\n\nsource\n\n\nObservationMeta.get_number_of_states\n\n ObservationMeta.get_number_of_states ()\n\nget number of states from StateSpecs\n\nsource\n\n\nObservationMeta.get_number_of_actions\n\n ObservationMeta.get_number_of_actions ()\n\nget number of actions from ActionSpecs\n\nsource\n\n\nObservationMeta.get_number_of_states_actions\n\n ObservationMeta.get_number_of_states_actions ()\n\nget number of states and actions from Plot\n\nsource\n\n\nObservationMeta.have_same_meta\n\n ObservationMeta.have_same_meta (meta_to_compare:__main__.ObservationMeta)\n\nCompare two plots, return True if they are the same, while ignoring the ‘when’ field\n\nsource\n\n\nObservationMeta.get_torque_table_row_names\n\n ObservationMeta.get_torque_table_row_names ()\n\nget torque table row names from reward_specs defined as [r0, r1, r2, …]\n\nsource\n\n\nObservationMetaCloud\n\n ObservationMetaCloud (state_specs:__main__.StateSpecsCloud,\n                       action_specs:__main__.ActionSpecs,\n                       reward_specs:__main__.RewardSpecs,\n                       site:tspace.data.location.EosLocation)\n\nselected metadata for db document matching pandas DataFrame\n\nsource\n\n\nObservationMetaECU\n\n ObservationMetaECU (state_specs:__main__.StateSpecsECU,\n                     action_specs:__main__.ActionSpecs,\n                     reward_specs:__main__.RewardSpecs,\n                     site:tspace.data.location.EosLocation)\n\nselected metadata for db document matching pandas DataFrame\n\nsource\n\n\nDataFrameDoc\n*Record doc type of mongo pool for record\nattributes:\ntimestamp: timestamp of the record\nmeta: metadata of the record\nobservation: observation of the record*\n\nsource\n\n\nPoolQuery\n\n PoolQuery (vehicle:str, driver:str,\n            episodestart_start:datetime.datetime=datetime.datetime(2021,\n            1, 1, 0, 0, tzinfo=&lt;DstTzInfo 'Asia/Shanghai' CST+8:00:00\n            STD&gt;),\n            episodestart_end:datetime.datetime=datetime.datetime(2031, 12,\n            31, 0, 0, tzinfo=&lt;DstTzInfo 'Asia/Shanghai' CST+8:00:00 STD&gt;),\n            timestamp_start:Optional[datetime.datetime]=None,\n            timestamp_end:Optional[datetime.datetime]=None,\n            seq_len_from:Optional[int]=None,\n            seq_len_to:Optional[int]=None)\n\nQuery for Record\n\nsource\n\n\nget_filemeta_config\n\n get_filemeta_config (data_folder:str, config_file:Optional[str],\n                      meta:Union[__main__.ObservationMetaCloud,__main__.Ob\n                      servationMetaECU], coll_type:str)\n\nGet the filepool config from the specified path data_folder + ’’ + config_file and compare the meat data with the plot info Returns: ConfigParser: filepool config\n\nsource\n\n\nconfigparser_as_dict\n\n configparser_as_dict (config:configparser.ConfigParser)\n\n*Converts a ConfigParser object into a dictionary.\nThe resulting dictionary has sections as keys which point to a dict of the sections options as key =&gt; value pairs.*",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Data</b>",
      "core"
    ]
  },
  {
    "objectID": "07.agent.utils.hyperparams.html",
    "href": "07.agent.utils.hyperparams.html",
    "title": "Hyperparameters",
    "section": "",
    "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nsource\n\nHyperParamDPG\n\n HyperParamDPG (BatchSize:int=4, NStates:int=90, NActions:int=68,\n                ActionBias:float=0.0, NLayerActor:int=2,\n                NLayerCritic:int=2, Gamma:float=0.99,\n                TauActor:float=0.005, TauCritic:float=0.005,\n                ActorLR:float=0.001, CriticLR:float=0.002,\n                CkptInterval:int=5)\n\n*Generic Hyperparameters for the RL agent\nAttributes:\n- BatchSize: int = 4  # batch size for training\n- NStates: int = (\n    default_truck.observation_numel\n)  # number of states in the state space\n- NActions: int = (\n    default_truck.torque_flash_numel\n)  # number of actions in the action space\n- ActionBias: float = 0.0  # bias for action output\n- NLayerActor: int = 2  # number of layers for the actor network\n- NLayerCritic: int = 2  # number of layers for the critic network\n- Gamma: float = 0.99  # Gamma value for RL discount\n- TauActor: float = 0.005  # Tau value for Polyak averaging for the actor network\n- TauCritic: float = 0.005  # Tau value for Polyak averaging for the actor network\n- ActorLR: float = 0.001  # learning rate for the actor network\n- CriticLR: float = 0.002  # learning rate for the critic network\n- CkptInterval: int = 5  # checkpoint interval*\n\nsource\n\n\nHyperParamDDPG\n\n HyperParamDDPG (BatchSize:int=4, NStates:int=90, NActions:int=68,\n                 ActionBias:float=0.0, NLayerActor:int=2,\n                 NLayerCritic:int=2, Gamma:float=0.99,\n                 TauActor:float=0.005, TauCritic:float=0.005,\n                 ActorLR:float=0.001, CriticLR:float=0.002,\n                 CkptInterval:int=5,\n                 CriticStateInputDenseDimension1:int=16,\n                 CriticStateInputDenseDimension2:int=32,\n                 CriticActionInputDenseDimension:int=32,\n                 CriticOutputDenseDimension1:int=256,\n                 CriticOutputDenseDimension2:int=256,\n                 ActorInputDenseDimension1:int=256,\n                 ActorInputDenseDimension2:int=256)\n\n*Hyperparameters for the DDPG agent\nAttributes:\n- CriticStateInputDenseDimension1: int = (\n    16  # output dimension for the state input (first) Dense layer\n)\n- CriticStateInputDenseDimension2: int = (\n    32  # output dimension for the state input second Dense layer\n)\n- CriticActionInputDenseDimension: int = (\n    32  # output dimension for the action input Dense layer\n)\n- CriticOutputDenseDimension1: int = (\n    256  # output dimension for the first critic output Dense layer\n)\n- CriticOutputDenseDimension2: int = (\n    256  # output dimension for the second critic output Dense layer\n)\n- ActorInputDenseDimension1: int = (\n    256  # output dimension for the first actor input Dense layer\n)\n- ActorInputDenseDimension2: int = (\n    256  # output dimension for the second actor input Dense layer\n)*\n\nsource\n\n\nHyperParamIDQL\n\n HyperParamIDQL (BatchSize:int=4, NStates:int=90, NActions:int=68,\n                 ActionBias:float=0.0, NLayerActor:int=2,\n                 NLayerCritic:int=2, Gamma:float=0.99,\n                 TauActor:float=0.005, TauCritic:float=0.005,\n                 ActorLR:float=0.001, CriticLR:float=0.002,\n                 CkptInterval:int=5,\n                 CriticStateInputDenseDimension1:int=16,\n                 CriticStateInputDenseDimension2:int=32,\n                 CriticActionInputDenseDimension:int=32,\n                 CriticOutputDenseDimension1:int=256,\n                 CriticOutputDenseDimension2:int=256,\n                 ActorInputDenseDimension1:int=256,\n                 ActorInputDenseDimension2:int=256)\n\n*Hyperparameters for the DDPG agent\nAttributes:\n- CriticStateInputDenseDimension1: int = (\n    16  # output dimension for the state input (first) Dense layer\n)\n- CriticStateInputDenseDimension2: int = (\n    32  # output dimension for the state input second Dense layer\n)\n- CriticActionInputDenseDimension: int = (\n    32  # output dimension for the action input Dense layer\n)\n- CriticOutputDenseDimension1: int = (\n    256  # output dimension for the first critic output Dense layer\n)\n- CriticOutputDenseDimension2: int = (\n    256  # output dimension for the second critic output Dense layer\n)\n- ActorInputDenseDimension1: int = (\n    256  # output dimension for the first actor input Dense layer\n)\n- ActorInputDenseDimension2: int = (\n    256  # output dimension for the second actor input Dense layer\n)*\n\nsource\n\n\nHyperParamRDPG\n\n HyperParamRDPG (BatchSize:int=4, NStates:int=90, NActions:int=68,\n                 ActionBias:float=0.0, NLayerActor:int=2,\n                 NLayerCritic:int=2, Gamma:float=0.99,\n                 TauActor:float=0.005, TauCritic:float=0.005,\n                 ActorLR:float=0.001, CriticLR:float=0.002,\n                 CkptInterval:int=5, HiddenDimension:int=256,\n                 PaddingValue:float=-10000, tbptt_k1:int=200,\n                 tbptt_k2:int=200)\n\n*Hyperparameters for the RDPG agent\nAttributes:\n- HiddenDimension: int = 256  # hidden unit number for the action input layer\n- PaddingValue: float = (\n    -10000\n)  # padding value for the input, impossible value for observation, action or reward\n- tbptt_k1: int = 200  # truncated backpropagation through time: forward steps,\n- tbptt_k2: int = 200  # truncated backpropagation through time: backward steps*\n\nsource\n\n\nHyperParamIDQL\n\n HyperParamIDQL (BatchSize:int=4, NStates:int=90, NActions:int=68,\n                 ActionBias:float=0.0, NLayerActor:int=2,\n                 NLayerCritic:int=2, Gamma:float=0.99,\n                 TauActor:float=0.005, TauCritic:float=0.005,\n                 ActorLR:float=0.001, CriticLR:float=0.002,\n                 CkptInterval:int=5, HiddenDimension:int=256,\n                 PaddingValue:float=-10000, tbptt_k1:int=200,\n                 tbptt_k2:int=200)\n\n*Hyperparameters for the IDQL agent\nAttributes:\n- HiddenDimension: int = 256  # hidden unit number for the action input layer\n- PaddingValue: float = (\n    -10000\n)  # padding value for the input, impossible value for observation, action or reward\n- tbptt_k1: int = 200  # truncated backpropagation through time: forward steps,\n- tbptt_k2: int = 200  # truncated backpropagation through time: backward steps*",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Agent</b>",
      "<b>utils</b>",
      "Hyperparameters"
    ]
  },
  {
    "objectID": "06.dataflow.filter.filter.html",
    "href": "06.dataflow.filter.filter.html",
    "title": "Filter",
    "section": "",
    "text": "source\n\nFilter\n\n Filter ()\n\nFilter consume data into the pipeline. It provides a shared interface for signal processing units filter().\n\nsource\n\n\nFilter.filter\n\n Filter.filter (in_pipeline:Union[tspace.dataflow.pipeline.deque.PipelineD\n                Q[~T_RAW],tspace.dataflow.pipeline.queue.Pipeline[~T]],\n                out_pipeline:tspace.dataflow.pipeline.queue.Pipeline[~T],\n                start_event:Optional[threading.Event],\n                stop_event:Optional[threading.Event],\n                interrupt_event:Optional[threading.Event],\n                flash_event:Optional[threading.Event],\n                exit_event:Optional[threading.Event])\n\nfilter data from in_pipeline to out_pipeline\n\n\n\n\nType\nDetails\n\n\n\n\nin_pipeline\nUnion\n\n\n\nout_pipeline\nPipeline\noutput pipeline\n\n\nstart_event\nOptional\ninput event start\n\n\nstop_event\nOptional\ninput event stop\n\n\ninterrupt_event\nOptional\ninput event interrupt\n\n\nflash_event\nOptional\ninput event flash\n\n\nexit_event\nOptional\ninput event exit\n\n\nReturns\nNone",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Dataflow</b>",
      "<b>Filter</b>",
      "Filter"
    ]
  },
  {
    "objectID": "07.agent.rdpg.rdpg.html",
    "href": "07.agent.rdpg.rdpg.html",
    "title": "RDPG",
    "section": "",
    "text": "source\n\nRDPG\n\n RDPG (actor_net:Optional[tspace.agent.rdpg.seq_actor.SeqActor]=None,\n       critic_net:Optional[tspace.agent.rdpg.seq_critic.SeqCritic]=None, t\n       arget_actor_net:Optional[tspace.agent.rdpg.seq_actor.SeqActor]=None\n       , target_critic_net:Optional[tspace.agent.rdpg.seq_critic.SeqCritic\n       ]=None, _ckpt_actor_dir:Optional[pathlib.Path]=None,\n       _ckpt_critic_dir:Optional[pathlib.Path]=None,\n       _truck:tspace.config.vehicles.Truck,\n       _driver:tspace.config.drivers.Driver, _resume:bool, _coll_type:str,\n       _hyper_param:Union[tspace.agent.utils.hyperparams.HyperParamDDPG,ts\n       pace.agent.utils.hyperparams.HyperParamRDPG,tspace.agent.utils.hype\n       rparams.HyperParamIDQL], _pool_key:str, _data_folder:str,\n       _infer_mode:bool, _buffer:Union[tspace.storage.buffer.mongo.MongoBu\n       ffer,tspace.storage.buffer.dask.DaskBuffer,NoneType]=None, _episode\n       _start_dt:Optional[pandas._libs.tslibs.timestamps.Timestamp]=None, \n       _observation_meta:Union[tspace.data.core.ObservationMetaCloud,tspac\n       e.data.core.ObservationMetaECU,NoneType]=None,\n       _torque_table_row_names:Optional[list[str]]=None,\n       _observations:Optional[list[pandas.core.series.Series]]=None,\n       _epi_no:Optional[int]=None, logger:Optional[logging.Logger]=None,\n       dict_logger:Optional[dict]=None)\n\n*RDPG agent for VEOS.\nAbstracts:\ndata interface:\n    - pool in mongodb\n    - buffer in memory (numpy array)\nmodel interface:\n    - actor network\n    - critic network\nAttributes:\nactor_net: actor network\ncritic_net: critic network\ntarget_actor_net: target actor network\ntarget_critic_net: target critic network\n_ckpt_actor_dir: checkpoint directory for actor\n_ckpt_critic_dir: checkpoint directory for critic*\n\nsource\n\n\nRDPG.__post_init__\n\n RDPG.__post_init__ ()\n\n*initialize the rdpg agent.\nargs: truck.ObservationNumber (int): dimension of the state space. padding_value (float): value to pad the state with, impossible value for observation, action or re*\n\nsource\n\n\nRDPG.__repr__\n\n RDPG.__repr__ ()\n\nReturn repr(self).\n\nsource\n\n\nRDPG.__str__\n\n RDPG.__str__ ()\n\nReturn str(self).\n\nsource\n\n\nRDPG.__hash__\n\n RDPG.__hash__ ()\n\nReturn hash(self).\n\nsource\n\n\nRDPG.touch_gpu\n\n RDPG.touch_gpu ()\n\ntouch the gpu to avoid the first time delay\n\nsource\n\n\nRDPG.init_checkpoint\n\n RDPG.init_checkpoint ()\n\ncreate or restore from checkpoint\n\nsource\n\n\nRDPG.actor_predict\n\n RDPG.actor_predict (state:pandas.core.series.Series)\n\n*get the action given a single observations by inference\nbatch size cannot be 1. For LSTM to be stateful, batch size must match the training scheme.*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nstate\nSeries\nstate sequence of the current episode\n\n\nReturns\nndarray\naction sequence of the current episode\n\n\n\n\nsource\n\n\nRDPG.actor_predict_step\n\n RDPG.actor_predict_step\n                          (states:tensorflow.python.framework.tensor.Tenso\n                          r, last_actions:tensorflow.python.framework.tens\n                          or.Tensor)\n\n*evaluate the actors given a single observations.\nbatch size is 1.*\n\n\n\n\nType\nDetails\n\n\n\n\nstates\nTensor\nstate, dimension: [B,T,D]\n\n\nlast_actions\nTensor\nlast action, dimension [B,T,D]\n\n\nReturns\nTensor\n\n\n\n\n\nsource\n\n\nRDPG.train\n\n RDPG.train ()\n\n*train the actor and critic moving network with truncated Backpropagation through time (TBPTT)\nwith k1 = k2 = self.hyperparam.tbptt_k1 (keras)\nReturn: tuple: (actor_loss, critic_loss)*\n\nsource\n\n\nRDPG.train_step\n\n RDPG.train_step (s_n_t, a_n_t, r_n_t, ns_n_t)\n\ntrain in one step the critic using bptt\n\nsource\n\n\nRDPG.end_episode\n\n RDPG.end_episode ()\n\nend the episode by depositing the episdoe and resetting the states of the actor and critic networks\n\nsource\n\n\nRDPG.get_losses\n\n RDPG.get_losses ()\n\nget the losses of the actor and critic networks\n\nsource\n\n\nRDPG.notrain\n\n RDPG.notrain ()\n\n*purely evaluate the actor and critic networks to return the losses without training.\nReturn: tuple: (actor_loss, critic_loss)*\n\nsource\n\n\nRDPG.soft_update_target\n\n RDPG.soft_update_target ()\n\n*update target networks with tiny tau value,\ntypical value 0.001. done after each batch, slowly update target by polyak averaging.*\n\nsource\n\n\nRDPG.save_ckpt\n\n RDPG.save_ckpt ()\n\nsave the checkpoints of the actor and critic networks",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Agent</b>",
      "<b>rdpg</b>",
      "RDPG"
    ]
  },
  {
    "objectID": "03.config.vcu.html",
    "href": "03.config.vcu.html",
    "title": "vcu",
    "section": "",
    "text": "For real vcu, values in the table will be the requrested torque Current throttlel (0,1) should be a coefficient of multplicative factor like between +/- 20% or empirically give safety bounds. action space will be then within the bounds TODO ask for safety bounds and real vcu to be integrated. TODO generate a mask according to WLTC to reduce parameter optimization space.\n\nsource\n\n\n\n generate_torque_table (pedal_scale:tuple, speed_scale:tuple)\n\n*Generate VCU calibration parameters for a given truck. simple piecewise linear surface, close to default calibration table Input is npd 17, nvl 21; output vcu_param_list as float32 pedal is x(column), velocity is y(row) )\nParameters:\npedal_scale: tuple\n    pedal range (0,1)\nspeed_scale: tuple\n    speed range (0,120)\nReturn: pandas dataframe*\n\n\n\n\nType\nDetails\n\n\n\n\npedal_scale\ntuple\n\n\n\nspeed_scale\ntuple\npedal range (0,1) # speed range (0,120)\n\n\n\n\nsource\n\n\n\n\n generate_vcu_calibration (npd:int, pedal_range:tuple, nvl:int,\n                           velocity_range:tuple, shortcut:int,\n                           data_root:pathlib.Path)\n\n*Generate VCU calibration parameters for a given truck.\npedal is x(column), velocity is y(row) input : npd 17, nvl 21; output vcu_param_list as float32\nParameters:\nnpd: int\n    number of pedal steps\npedal_range: tuple\n    pedal range (0,1)\nnvl: int\n    number of velocity steps\nvelocity_range: tuple\n    speed range (0,120)\nshortcut: int\n    1: use segment-wise linear eco calibration table\n    2: use init table\n    3: use latest pedal map that was used\ndata_root: str\n    path to data folder\nReturn: pandas dataframe*\n\n\n\n\nType\nDetails\n\n\n\n\nnpd\nint\nnumber of pedal steps\n\n\npedal_range\ntuple\npedal range (0,1)\n\n\nnvl\nint\nnumber of velocity steps\n\n\nvelocity_range\ntuple\nspeed range (0,120)\n\n\nshortcut\nint\n1: use default eco calibration table\n\n\ndata_root\nPath\npath to data folder\n\n\n\n\n# fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n# # Plot the surface.\n# surf = ax.plot_surface(pdv, vlv, v, cmap=cm.coolwarm,\n#                        linewidth=0, antialiased=False)\n# # Customize the z axis.\n# ax.set_zlim(-0.01, 1.01)\n# ax.zaxis.set_major_locator(LinearLocator(10))\n# # A StrMethodFormatter is used automatically\n# ax.zaxis.set_major_formatter('{x:.02f}')\n# \n# # Add a color bar which maps values to colors.\n# fig.colorbar(surf, shrink=0.5, aspect=5)\n# plt.show()\n# return np.float32(v)\n\n\nsource\n\n\n\n\n generate_lookup_table (pedal_range:tuple, velocity_range:tuple,\n                        calib_table:pandas.core.frame.DataFrame)\n\n*Generate VCU calibration parameters for a given truck.\npedal in x(col), velocity in y(row) input : npd 17, nvl 21; output vcu_param_list as float32\nParameters:\npedal_range: tuple\n    pedal range (0,1)\nvelocity_range: tuple\n    speed range (0,120)\ncalib_table: pandas dataframe\n    calibration table\nReturn:\nnumpy array*\n\n\n\n\nsource\n\n\n\n test_generate_vcu_calibration ()\n\n\nsource\n\n\n\n\n test_generate_lookup_table ()\n\n\nvcu_calib_table = test_generate_vcu_calibration()\nvcu_calib_table\n\n\n\n\n\n\n\n\n0.0000\n0.0625\n0.1250\n0.1875\n0.2500\n0.3125\n0.3750\n0.4375\n0.5000\n0.5625\n0.6250\n0.6875\n0.7500\n0.8125\n0.8750\n0.9375\n1.0000\n\n\n\n\n0.000000\n0.0\n0.062500\n0.125000\n0.187500\n0.250000\n0.312500\n0.375000\n0.437500\n0.500000\n0.562500\n0.625000\n0.687500\n0.750000\n0.812500\n0.875000\n0.937500\n1.000000\n\n\n7.000000\n0.0\n0.026102\n0.052204\n0.078307\n0.104409\n0.130511\n0.156613\n0.182715\n0.208818\n0.234920\n0.261022\n0.287124\n0.313227\n0.339329\n0.365431\n0.391533\n0.417635\n\n\n1.052632\n0.0\n0.040565\n0.081130\n0.121695\n0.162260\n0.202825\n0.243390\n0.283955\n0.324520\n0.365085\n0.405650\n0.446215\n0.486780\n0.527345\n0.567910\n0.608475\n0.649040\n\n\n2.105263\n0.0\n0.035416\n0.070833\n0.106249\n0.141666\n0.177082\n0.212498\n0.247915\n0.283331\n0.318748\n0.354164\n0.389580\n0.424997\n0.460413\n0.495830\n0.531246\n0.566662\n\n\n3.157895\n0.0\n0.032273\n0.064547\n0.096820\n0.129093\n0.161366\n0.193640\n0.225913\n0.258186\n0.290460\n0.322733\n0.355006\n0.387280\n0.419553\n0.451826\n0.484099\n0.516373\n\n\n4.210526\n0.0\n0.030027\n0.060054\n0.090080\n0.120107\n0.150134\n0.180161\n0.210187\n0.240214\n0.270241\n0.300268\n0.330294\n0.360321\n0.390348\n0.420375\n0.450402\n0.480428\n\n\n5.263158\n0.0\n0.028292\n0.056583\n0.084875\n0.113167\n0.141459\n0.169750\n0.198042\n0.226334\n0.254625\n0.282917\n0.311209\n0.339501\n0.367792\n0.396084\n0.424376\n0.452667\n\n\n6.315789\n0.0\n0.026887\n0.053774\n0.080661\n0.107549\n0.134436\n0.161323\n0.188210\n0.215097\n0.241984\n0.268871\n0.295758\n0.322646\n0.349533\n0.376420\n0.403307\n0.430194\n\n\n7.368421\n0.0\n0.025713\n0.051426\n0.077140\n0.102853\n0.128566\n0.154279\n0.179992\n0.205706\n0.231419\n0.257132\n0.282845\n0.308558\n0.334272\n0.359985\n0.385698\n0.411411\n\n\n8.421053\n0.0\n0.024709\n0.049418\n0.074127\n0.098836\n0.123545\n0.148254\n0.172963\n0.197672\n0.222381\n0.247090\n0.271799\n0.296509\n0.321218\n0.345927\n0.370636\n0.395345\n\n\n9.473684\n0.0\n0.023835\n0.047670\n0.071504\n0.095339\n0.119174\n0.143009\n0.166844\n0.190679\n0.214513\n0.238348\n0.262183\n0.286018\n0.309853\n0.333687\n0.357522\n0.381357\n\n\n10.526316\n0.0\n0.023063\n0.046126\n0.069189\n0.092252\n0.115315\n0.138378\n0.161441\n0.184504\n0.207567\n0.230630\n0.253693\n0.276756\n0.299819\n0.322883\n0.345946\n0.369009\n\n\n11.578947\n0.0\n0.022374\n0.044748\n0.067122\n0.089496\n0.111870\n0.134244\n0.156618\n0.178992\n0.201366\n0.223740\n0.246114\n0.268488\n0.290862\n0.313236\n0.335609\n0.357983\n\n\n12.631579\n0.0\n0.021753\n0.043506\n0.065259\n0.087012\n0.108765\n0.130518\n0.152271\n0.174024\n0.195777\n0.217530\n0.239283\n0.261036\n0.282789\n0.304542\n0.326294\n0.348047\n\n\n13.684211\n0.0\n0.021189\n0.042378\n0.063567\n0.084756\n0.105944\n0.127133\n0.148322\n0.169511\n0.190700\n0.211889\n0.233078\n0.254267\n0.275456\n0.296644\n0.317833\n0.339022\n\n\n14.736842\n0.0\n0.020673\n0.041346\n0.062019\n0.082692\n0.103365\n0.124038\n0.144712\n0.165385\n0.186058\n0.206731\n0.227404\n0.248077\n0.268750\n0.289423\n0.310096\n0.330769\n\n\n15.789474\n0.0\n0.020199\n0.040397\n0.060596\n0.080795\n0.100993\n0.121192\n0.141390\n0.161589\n0.181788\n0.201986\n0.222185\n0.242384\n0.262582\n0.282781\n0.302979\n0.323178\n\n\n16.842105\n0.0\n0.019760\n0.039520\n0.059280\n0.079040\n0.098800\n0.118560\n0.138320\n0.158080\n0.177840\n0.197600\n0.217360\n0.237120\n0.256880\n0.276640\n0.296400\n0.316160\n\n\n17.894737\n0.0\n0.019353\n0.038705\n0.058058\n0.077411\n0.096764\n0.116116\n0.135469\n0.154822\n0.174174\n0.193527\n0.212880\n0.232233\n0.251585\n0.270938\n0.290291\n0.309644\n\n\n18.947368\n0.0\n0.018973\n0.037946\n0.056919\n0.075892\n0.094865\n0.113838\n0.132811\n0.151784\n0.170757\n0.189730\n0.208703\n0.227676\n0.246649\n0.265622\n0.284595\n0.303568\n\n\n20.000000\n0.0\n0.018618\n0.037235\n0.055853\n0.074471\n0.093088\n0.111706\n0.130324\n0.148942\n0.167559\n0.186177\n0.204795\n0.223412\n0.242030\n0.260648\n0.279265\n0.297883\n\n\n\n\n\n\n\n\n# #| output: true\n# vcu_lookup_table = test_generate_lookup_table()\n# vcu_calib_table",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Config</b>",
      "vcu"
    ]
  },
  {
    "objectID": "03.config.vcu.html#testing",
    "href": "03.config.vcu.html#testing",
    "title": "vcu",
    "section": "",
    "text": "source\n\n\n\n test_generate_vcu_calibration ()\n\n\nsource\n\n\n\n\n test_generate_lookup_table ()\n\n\nvcu_calib_table = test_generate_vcu_calibration()\nvcu_calib_table\n\n\n\n\n\n\n\n\n0.0000\n0.0625\n0.1250\n0.1875\n0.2500\n0.3125\n0.3750\n0.4375\n0.5000\n0.5625\n0.6250\n0.6875\n0.7500\n0.8125\n0.8750\n0.9375\n1.0000\n\n\n\n\n0.000000\n0.0\n0.062500\n0.125000\n0.187500\n0.250000\n0.312500\n0.375000\n0.437500\n0.500000\n0.562500\n0.625000\n0.687500\n0.750000\n0.812500\n0.875000\n0.937500\n1.000000\n\n\n7.000000\n0.0\n0.026102\n0.052204\n0.078307\n0.104409\n0.130511\n0.156613\n0.182715\n0.208818\n0.234920\n0.261022\n0.287124\n0.313227\n0.339329\n0.365431\n0.391533\n0.417635\n\n\n1.052632\n0.0\n0.040565\n0.081130\n0.121695\n0.162260\n0.202825\n0.243390\n0.283955\n0.324520\n0.365085\n0.405650\n0.446215\n0.486780\n0.527345\n0.567910\n0.608475\n0.649040\n\n\n2.105263\n0.0\n0.035416\n0.070833\n0.106249\n0.141666\n0.177082\n0.212498\n0.247915\n0.283331\n0.318748\n0.354164\n0.389580\n0.424997\n0.460413\n0.495830\n0.531246\n0.566662\n\n\n3.157895\n0.0\n0.032273\n0.064547\n0.096820\n0.129093\n0.161366\n0.193640\n0.225913\n0.258186\n0.290460\n0.322733\n0.355006\n0.387280\n0.419553\n0.451826\n0.484099\n0.516373\n\n\n4.210526\n0.0\n0.030027\n0.060054\n0.090080\n0.120107\n0.150134\n0.180161\n0.210187\n0.240214\n0.270241\n0.300268\n0.330294\n0.360321\n0.390348\n0.420375\n0.450402\n0.480428\n\n\n5.263158\n0.0\n0.028292\n0.056583\n0.084875\n0.113167\n0.141459\n0.169750\n0.198042\n0.226334\n0.254625\n0.282917\n0.311209\n0.339501\n0.367792\n0.396084\n0.424376\n0.452667\n\n\n6.315789\n0.0\n0.026887\n0.053774\n0.080661\n0.107549\n0.134436\n0.161323\n0.188210\n0.215097\n0.241984\n0.268871\n0.295758\n0.322646\n0.349533\n0.376420\n0.403307\n0.430194\n\n\n7.368421\n0.0\n0.025713\n0.051426\n0.077140\n0.102853\n0.128566\n0.154279\n0.179992\n0.205706\n0.231419\n0.257132\n0.282845\n0.308558\n0.334272\n0.359985\n0.385698\n0.411411\n\n\n8.421053\n0.0\n0.024709\n0.049418\n0.074127\n0.098836\n0.123545\n0.148254\n0.172963\n0.197672\n0.222381\n0.247090\n0.271799\n0.296509\n0.321218\n0.345927\n0.370636\n0.395345\n\n\n9.473684\n0.0\n0.023835\n0.047670\n0.071504\n0.095339\n0.119174\n0.143009\n0.166844\n0.190679\n0.214513\n0.238348\n0.262183\n0.286018\n0.309853\n0.333687\n0.357522\n0.381357\n\n\n10.526316\n0.0\n0.023063\n0.046126\n0.069189\n0.092252\n0.115315\n0.138378\n0.161441\n0.184504\n0.207567\n0.230630\n0.253693\n0.276756\n0.299819\n0.322883\n0.345946\n0.369009\n\n\n11.578947\n0.0\n0.022374\n0.044748\n0.067122\n0.089496\n0.111870\n0.134244\n0.156618\n0.178992\n0.201366\n0.223740\n0.246114\n0.268488\n0.290862\n0.313236\n0.335609\n0.357983\n\n\n12.631579\n0.0\n0.021753\n0.043506\n0.065259\n0.087012\n0.108765\n0.130518\n0.152271\n0.174024\n0.195777\n0.217530\n0.239283\n0.261036\n0.282789\n0.304542\n0.326294\n0.348047\n\n\n13.684211\n0.0\n0.021189\n0.042378\n0.063567\n0.084756\n0.105944\n0.127133\n0.148322\n0.169511\n0.190700\n0.211889\n0.233078\n0.254267\n0.275456\n0.296644\n0.317833\n0.339022\n\n\n14.736842\n0.0\n0.020673\n0.041346\n0.062019\n0.082692\n0.103365\n0.124038\n0.144712\n0.165385\n0.186058\n0.206731\n0.227404\n0.248077\n0.268750\n0.289423\n0.310096\n0.330769\n\n\n15.789474\n0.0\n0.020199\n0.040397\n0.060596\n0.080795\n0.100993\n0.121192\n0.141390\n0.161589\n0.181788\n0.201986\n0.222185\n0.242384\n0.262582\n0.282781\n0.302979\n0.323178\n\n\n16.842105\n0.0\n0.019760\n0.039520\n0.059280\n0.079040\n0.098800\n0.118560\n0.138320\n0.158080\n0.177840\n0.197600\n0.217360\n0.237120\n0.256880\n0.276640\n0.296400\n0.316160\n\n\n17.894737\n0.0\n0.019353\n0.038705\n0.058058\n0.077411\n0.096764\n0.116116\n0.135469\n0.154822\n0.174174\n0.193527\n0.212880\n0.232233\n0.251585\n0.270938\n0.290291\n0.309644\n\n\n18.947368\n0.0\n0.018973\n0.037946\n0.056919\n0.075892\n0.094865\n0.113838\n0.132811\n0.151784\n0.170757\n0.189730\n0.208703\n0.227676\n0.246649\n0.265622\n0.284595\n0.303568\n\n\n20.000000\n0.0\n0.018618\n0.037235\n0.055853\n0.074471\n0.093088\n0.111706\n0.130324\n0.148942\n0.167559\n0.186177\n0.204795\n0.223412\n0.242030\n0.260648\n0.279265\n0.297883\n\n\n\n\n\n\n\n\n# #| output: true\n# vcu_lookup_table = test_generate_lookup_table()\n# vcu_calib_table",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Config</b>",
      "vcu"
    ]
  },
  {
    "objectID": "04.conn.tcp.html",
    "href": "04.conn.tcp.html",
    "title": "TCP",
    "section": "",
    "text": "source\n\ntcp_context\n\n tcp_context (vehicle:tspace.config.vehicles.Truck, host:str, port:str,\n              proxy:Optional[str]=None)\n\ntcp context manager.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvehicle\nTruck\n\nTruck object\n\n\nhost\nstr\n\nhost for Remote Can Server\n\n\nport\nstr\n\nport for Remote Can Server\n\n\nproxy\nOptional\nNone\nproxy for Remote Can Server\n\n\nReturns\nGenerator",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Connection</b>",
      "TCP"
    ]
  },
  {
    "objectID": "07.agent.rdpg.critic.html",
    "href": "07.agent.rdpg.critic.html",
    "title": "SeqCritic",
    "section": "",
    "text": "source\n\nSeqCritic\n\n SeqCritic (state_dim:int=0, action_dim:int=0, hidden_dim:int=0,\n            n_layers:int=0, batch_size:int=0, padding_value:float=0.0,\n            tau:float=0.0, lr:float=0.0, ckpt_dir:pathlib.Path=Path('.'),\n            ckpt_interval:int=0, logger:Optional[logging.Logger]=None,\n            dict_logger:Optional[dict]=None)\n\n*Sequential Critic network for the RDPG algorithm.\nAttributes:\nstate_dim (int): Dimension of the state space.\naction_dim (int): Dimension of the action space.\nhidden_dim (int): Dimension of the hidden layer.\nn_layers (int): Number of layers in the network.\nbatch_size (int): Batch size for the network.\npadding_value (float): Value to pad the input with.\ntau (float): Soft update parameter.\nlr (float): Learning rate for the network.\nckpt_dir (str): Directory to restore the checkpoint from.\nckpt_interval (int): Interval to save the checkpoint.\nlogger (logging.Logger): Logger for the class.\ndict_logger (dict): Dictionary to log the class.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstate_dim\nint\n0\ndimension of the state space, 600 for cloud, 90 for kvaser\n\n\naction_dim\nint\n0\ndimension of the action space, 68 for both cloud and kvaser\n\n\nhidden_dim\nint\n0\ndimension of the hidden layer\n\n\nn_layers\nint\n0\nlayer number of the lstm\n\n\nbatch_size\nint\n0\nbatch size for the network\n\n\npadding_value\nfloat\n0.0\nvalue to pad the input with\n\n\ntau\nfloat\n0.0\nsoft update parameter \\(\\tau\\)\n\n\nlr\nfloat\n0.0\nlearning rate for the network\n\n\nckpt_dir\nPath\n.\ndirectory to restore the checkpoint from\n\n\nckpt_interval\nint\n0\ninterval to save the checkpoint\n\n\nlogger\nOptional\nNone\nlogger for the class\n\n\ndict_logger\nOptional\nNone\nformat specs to log the class\n\n\n\n\nsource\n\n\nSeqCritic.__init__\n\n SeqCritic.__init__ (state_dim:int=0, action_dim:int=0, hidden_dim:int=0,\n                     n_layers:int=0, batch_size:int=0,\n                     padding_value:float=0.0, tau:float=0.0, lr:float=0.0,\n                     ckpt_dir:pathlib.Path=Path('.'), ckpt_interval:int=0,\n                     logger:Optional[logging.Logger]=None,\n                     dict_logger:Optional[dict]=None)\n\n*Initialize the critic network.\nrestore checkpoint from the provided directory if it exists, initialize otherwise. Args: state_dim (int): Dimension of the state space. action_dim (int): Dimension of the action space. hidden_dim (int): Dimension of the hidden layer. lr (float): Learning rate for the network. ckpt_dir (str): Directory to restore the checkpoint from.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstate_dim\nint\n0\ndimension of the state space, 600 for cloud, 90 for kvaser\n\n\naction_dim\nint\n0\ndimension of the action space, 68 for both cloud and kvaser\n\n\nhidden_dim\nint\n0\ndimension of the hidden layer\n\n\nn_layers\nint\n0\nlayer number of the lstm\n\n\nbatch_size\nint\n0\nbatch size for the network\n\n\npadding_value\nfloat\n0.0\nvalue to pad the input with\n\n\ntau\nfloat\n0.0\nsoft update parameter \\(\\tau\\)\n\n\nlr\nfloat\n0.0\nlearning rate for the network\n\n\nckpt_dir\nPath\n.\ndirectory to restore the checkpoint from\n\n\nckpt_interval\nint\n0\ninterval to save the checkpoint\n\n\nlogger\nOptional\nNone\nlogger for the class\n\n\ndict_logger\nOptional\nNone\nformat specs to log the class\n\n\n\n\nsource\n\n\nSeqCritic.clone_weights\n\n SeqCritic.clone_weights (moving_net)\n\nClone weights from a model to another model.\n\nsource\n\n\nSeqCritic.soft_update\n\n SeqCritic.soft_update (moving_net)\n\nUpdate the target critic weights.\n\nsource\n\n\nSeqCritic.save_ckpt\n\n SeqCritic.save_ckpt ()\n\nSave the checkpoint.\n\nsource\n\n\nSeqCritic.evaluate_q\n\n SeqCritic.evaluate_q (states, last_actions, actions)\n\n*Evaluate the action value given the state and action\nArgs:\nstates (np.array): State in a minibatch\nlast_actions (np.array): Action in a minibatch\nactions (np.array): Action in a minibatch\nReturns:\nnp.array: Q-value*",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Agent</b>",
      "<b>rdpg</b>",
      "SeqCritic"
    ]
  },
  {
    "objectID": "05.storage.buffer.buffer.html",
    "href": "05.storage.buffer.buffer.html",
    "title": "Buffer",
    "section": "",
    "text": "source\n\nBuffer\n\n Buffer (pool:Optional[tspace.storage.pool.pool.Pool], batch_size:int)\n\n*The abstract class for providing an buffer interface for data processing.\nBuffer is the internal dynamic memory object for pooling the experience tuples. It provides the following interface for inheriting classes (MongoBuffer, DaskBuffer, etc.):\nMethods: - load() - save() - store() - sample()\nAttributes: - pool: the pool object for storing the data - batch_size: the batch size for sampling - _type_T: the type of the data item (e.g. Record, Episode, etc.)*\n\nsource\n\n\nBuffer.store\n\n Buffer.store (episode:~ItemT)\n\nDeposit an item (record/episode) into the pool\n\nsource\n\n\nBuffer.find\n\n Buffer.find (query:tspace.data.core.PoolQuery)\n\nfind an itme by id or name.\n\nsource\n\n\nBuffer.sample\n\n Buffer.sample ()\n\nsample data pool to get (state, action, reward, nstate) as a tuple of 4 DataFrames\n\nsource\n\n\nBuffer.load\n\n Buffer.load ()\n\nload buffer from pool\n\nsource\n\n\nBuffer.close\n\n Buffer.close ()\n\nclose the pool, for destructor\n\nsource\n\n\nBuffer.__init_subclass__\n\n Buffer.__init_subclass__ ()\n\nGet the concrete type of the data item (e.g. Record, Episode, etc.)\n\nsource\n\n\nBuffer.__post_init__\n\n Buffer.__post_init__ ()\n\nUser weakref finalizer to make sure close is called when the object is destroyed",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Storage</b>",
      "<b>Buffer</b>",
      "Buffer"
    ]
  },
  {
    "objectID": "06.dataflow.cloud.html",
    "href": "06.dataflow.cloud.html",
    "title": "Cloud",
    "section": "",
    "text": "source\n\nCloud\n\n Cloud (truck:tspace.config.vehicles.TruckInCloud,\n        can_server:tspace.config.messengers.CANMessenger,\n        epi_countdown_time:float=3.0,\n        trip_server:Optional[tspace.config.messengers.TripMessenger]=None,\n        ui:str='UDP', remotecan:Optional[tspace.conn.remote_can_client.Rem\n        oteCanClient]=None, remoteClient_lock:Optional[&lt;built-\n        infunctionallocate_lock&gt;]=None,\n        driver:tspace.config.drivers.Driver, resume:bool=False,\n        data_dir:Optional[pathlib.Path]=None, flash_count:int=0,\n        episode_count:int=0, vcu_calib_table_row_start:int=0,\n        torque_table_default:Optional[pandas.core.frame.DataFrame]=None,\n        torque_table_live:Optional[pandas.core.frame.DataFrame]=None,\n        lock_watchdog:&lt;built-infunctionallocate_lock&gt;=&lt;unlocked\n        _thread.lock object at 0x7f9a997f6f80&gt;,\n        capture_failure_count:int=0, flash_failure_count:int=0,\n        logger:Optional[logging.Logger]=None,\n        dict_logger:Optional[dict]=None)\n\n*Kvaser is local vehicle interface with Producer(get vehicle status) and Consumer(flasher)\nAttributes:\ntruck: TruckInCloud\n    truck type is TruckInCloud\ncan_server: CANMessenger\n    can_server type is CANMessenger\ntrip_server: Optional[TripMessenger] = None\n    trip_server type is TripMessenger\nui: str = \"UDP\"\n    ui must be cloud, local or mobile, not {self.ui}\nweb_srv = (\"rocket_intra\",)\n    web_srv is a tuple of str\nepi_countdown_time: float = 3.0\n    epi_countdown_time is a float\nremotecan: Optional[RemoteCanClient] = None\n    RemoteCanClient type is RemoteCanClient\nrmq_consumer: Optional[ClearablePullConsumer] = None\n    ClearablePullConsumer type is ClearablePullConsumer\nrmq_message_ready: Optional[Message] = None\n    Message type is Message\nrmq_producer: Optional[Producer] = None\n    Producer type is Producer\nremoteClient_lock: Optional[Lock] = None\n    Lock type is Lock*\n\nsource\n\n\nCloud.init_cloud\n\n Cloud.init_cloud ()\n\ninitialize cloud interface, set proxy and remote can client and the lock\n\nsource\n\n\nCloud.init_internal_pipelines\n\n Cloud.init_internal_pipelines ()\n\ninitialize internal pipeline static type for cloud interface\n\nsource\n\n\nCloud.flash_vehicle\n\n Cloud.flash_vehicle (torque_table:pandas.core.frame.DataFrame)\n\nflash torque table to the VCU\n\nsource\n\n\nCloud.hmi_select\n\n Cloud.hmi_select ()\n\n*select HMI interface according to ui type.\nProduce data into the pipeline main entry to the capture thread sub-thread method Callable input parameters example: hmi_pipeline: Pipeline[str],\nReturn: Callback for the HMI thread with type Callable[[Pipeline[str], Optional[Event]], None]*\n\nsource\n\n\nCloud.produce\n\n Cloud.produce (raw_pipeline:tspace.dataflow.pipeline.deque.PipelineDQ[typ\n                ing.Union[dict[str,str],dict[str,dict[str,list[typing.Unio\n                n[str,list[list[str]]]]]]]], hmi_pipeline:Optional[tspace.\n                dataflow.pipeline.queue.Pipeline[str]]=None,\n                exit_event:Optional[threading.Event]=None)\n\nCreate secondary threading pool for HMI thread and data capture thread\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraw_pipeline\nPipelineDQ\n\n\n\n\nhmi_pipeline\nOptional\nNone\nPipelineDQ[dict[str, dict[str, list[Union[str, list[str]]]]]],\n\n\nexit_event\nOptional\nNone\n\n\n\n\n\nsource\n\n\nCloud.data_capture_from_remotecan\n\n Cloud.data_capture_from_remotecan (raw_pipeline:tspace.dataflow.pipeline.\n                                    deque.PipelineDQ[typing.Union[dict[str\n                                    ,str],dict[str,dict[str,list[typing.Un\n                                    ion[str,list[list[str]]]]]]]], exit_ev\n                                    ent:Optional[threading.Event]=None)\n\nCallback for the data capture thread\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraw_pipeline\nPipelineDQ\n\n\n\n\nexit_event\nOptional\nNone\nPipelineDQ[dict[str, dict[str, list[Union[str, list[str]]]]]],\n\n\n\n\nsource\n\n\nCloud.cloud_ping\n\n Cloud.cloud_ping ()\n\nutility function for ping test\n\nsource\n\n\nCloud.cloud_telnet_test\n\n Cloud.cloud_telnet_test ()\n\nUtility function for telnet test\n\nsource\n\n\nCloud.hmi_capture_from_udp\n\n Cloud.hmi_capture_from_udp\n                             (hmi_pipeline:tspace.dataflow.pipeline.queue.\n                             Pipeline[str],\n                             exit_event:Optional[threading.Event]=None)\n\nCallback function for getting HMI message from local UDP\n\nsource\n\n\nCloud.hmi_capture_from_rmq\n\n Cloud.hmi_capture_from_rmq\n                             (hmi_pipeline:tspace.dataflow.pipeline.queue.\n                             Pipeline[str],\n                             exit_event:Optional[threading.Event]=None)\n\nGet the hmi message from RocketMQ\n\nsource\n\n\nCloud.hmi_capture_from_dummy\n\n Cloud.hmi_capture_from_dummy\n                               (hmi_pipeline:tspace.dataflow.pipeline.queu\n                               e.Pipeline[str],\n                               exit_event:threading.Event)\n\n*Get the hmi status from dummy state management and remote can module\nThe only way to change the state with dummy mode is through Graceful Killer (Ctrl +C), which is triggered by GracefulKiller in the Cruncher thread and received in hmi_control*\n\nsource\n\n\nCloud.filter\n\n Cloud.filter (in_pipeline:tspace.dataflow.pipeline.deque.PipelineDQ[typin\n               g.Union[dict[str,str],dict[str,dict[str,list[typing.Union[s\n               tr,list[list[str]]]]]]]], out_pipeline:tspace.dataflow.pipe\n               line.queue.Pipeline[pandas.core.frame.DataFrame],\n               start_event:Optional[threading.Event],\n               stop_event:Optional[threading.Event],\n               interrupt_event:Optional[threading.Event],\n               flash_event:Optional[threading.Event],\n               exit_event:Optional[threading.Event])\n\nCallback function for the data filter thread, encapsulating the data into pandas DataFrame\n\n\n\n\nType\nDetails\n\n\n\n\nin_pipeline\nPipelineDQ\ninput pipelineDQ[raw data],\n\n\nout_pipeline\nPipeline\noutput pipeline[DataFrame]\n\n\nstart_event\nOptional\ninput event start\n\n\nstop_event\nOptional\nnot used for cloud\n\n\ninterrupt_event\nOptional\nnot used for cloud\n\n\nflash_event\nOptional\n\n\n\nexit_event\nOptional\ninput event exit\n\n\nReturns\nNone",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Dataflow</b>",
      "Cloud"
    ]
  },
  {
    "objectID": "01.data.time.html",
    "href": "01.data.time.html",
    "title": "time",
    "section": "",
    "text": "veos_lifetime_start_date\n\n\nveos_lifetime_end_date",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Data</b>",
      "time"
    ]
  },
  {
    "objectID": "01.data.time.html#project-start-and-end-info",
    "href": "01.data.time.html#project-start-and-end-info",
    "title": "time",
    "section": "",
    "text": "veos_lifetime_start_date\n\n\nveos_lifetime_end_date",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Data</b>",
      "time"
    ]
  },
  {
    "objectID": "01.data.time.html#timezone",
    "href": "01.data.time.html#timezone",
    "title": "time",
    "section": "timezone",
    "text": "timezone\n\npprint(timezones)",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Data</b>",
      "time"
    ]
  },
  {
    "objectID": "06.dataflow.pipeline.deque.html",
    "href": "06.dataflow.pipeline.deque.html",
    "title": "Deque",
    "section": "",
    "text": "source\n\nPipelineDQ\nPipeline with Deque for double-ended processing unit in dataflow\n\nsource\n\n\nPipelineDQ.get_data\n\n PipelineDQ.get_data ()\n\n*Get data from the pipeline\nreturn: data: data from the pipeline*\n\nsource\n\n\nPipelineDQ.put_data\n\n PipelineDQ.put_data (value:~T)\n\n*Put data into the pipeline\narg:\nvalue: data to be put into the pipeline\nreturn:\nNone*\n\nsource\n\n\nPipelineDQ.clear\n\n PipelineDQ.clear ()\n\n*clear the pipeline (Deque)\ncollection.deque has a clear method. Just call it.*",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Dataflow</b>",
      "<b>Pipeline</b>",
      "Deque"
    ]
  },
  {
    "objectID": "06.dataflow.pipeline.queue.html",
    "href": "06.dataflow.pipeline.queue.html",
    "title": "Queue",
    "section": "",
    "text": "source\n\nPipeline\n\n Pipeline (maxsize=0)\n\nPipeline class from Queue for single-ended unit in dataflow\n\nsource\n\n\nPipeline.get_data\n\n Pipeline.get_data ()\n\n*Get data from the pipeline\nreturn: data: data from the pipeline*\n\nsource\n\n\nPipeline.put_data\n\n Pipeline.put_data (value:~T)\n\n*Put data into the pipeline\narg:\nvalue: data to be put into the pipeline\nreturn:\nNone*\n\nsource\n\n\nPipeline.clear\n\n Pipeline.clear ()\n\n*clear the pipeline (Queue) as the standard Queue\nsince python queue.Queue does not have clear() method*",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Dataflow</b>",
      "<b>Pipeline</b>",
      "Queue"
    ]
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "utils",
    "section": "",
    "text": "source\n\n\n\n generate_state (tz:zoneinfo.ZoneInfo)\n\ngenerate a pandas DataFrame for testing purpose\n\nts = pd.to_datetime(datetime.now())\nts_ind1 = ts + pd.to_timedelta(np.arange(0, 4 * 20, 20), \"ms\").astype(\"timedelta64[us]\")\nts_ind = ts_ind1.astype('datetime64[us]')\nts_ind.unit",
    "crumbs": [
      "utils"
    ]
  },
  {
    "objectID": "utils.html#generate-state",
    "href": "utils.html#generate-state",
    "title": "utils",
    "section": "",
    "text": "source\n\n\n\n generate_state (tz:zoneinfo.ZoneInfo)\n\ngenerate a pandas DataFrame for testing purpose\n\nts = pd.to_datetime(datetime.now())\nts_ind1 = ts + pd.to_timedelta(np.arange(0, 4 * 20, 20), \"ms\").astype(\"timedelta64[us]\")\nts_ind = ts_ind1.astype('datetime64[us]')\nts_ind.unit",
    "crumbs": [
      "utils"
    ]
  },
  {
    "objectID": "utils.html#generate-action",
    "href": "utils.html#generate-action",
    "title": "utils",
    "section": "Generate action",
    "text": "Generate action\n\nsource\n\ngenerate_action\n\n generate_action (tz:zoneinfo.ZoneInfo)\n\ngenerate a pandas DataFrame for testing purpose\n\naction['timestep'].dtype",
    "crumbs": [
      "utils"
    ]
  },
  {
    "objectID": "utils.html#generate-reward",
    "href": "utils.html#generate-reward",
    "title": "utils",
    "section": "Generate reward",
    "text": "Generate reward\n\nsource\n\ngenerate_reward\n\n generate_reward (tz:zoneinfo.ZoneInfo)\n\ngenerate a pandas DataFrame for testing purpose",
    "crumbs": [
      "utils"
    ]
  },
  {
    "objectID": "utils.html#generate-nstate",
    "href": "utils.html#generate-nstate",
    "title": "utils",
    "section": "Generate nstate",
    "text": "Generate nstate\n\nsource\n\ngenerate_nstate\n\n generate_nstate (tz:zoneinfo.ZoneInfo)\n\ngenerate a pandas DataFrame for testing purpose",
    "crumbs": [
      "utils"
    ]
  },
  {
    "objectID": "utils.html#generate-observation",
    "href": "utils.html#generate-observation",
    "title": "utils",
    "section": "Generate observation",
    "text": "Generate observation\n\nsource\n\ngenerate_observation\n\n generate_observation (tz:zoneinfo.ZoneInfo)\n\ngenerate a list of pandas Series for testing purpose",
    "crumbs": [
      "utils"
    ]
  },
  {
    "objectID": "utils.html#generate-multiindex-dataframe",
    "href": "utils.html#generate-multiindex-dataframe",
    "title": "utils",
    "section": "Generate MultiIndex DataFrame",
    "text": "Generate MultiIndex DataFrame\n\nsource\n\ngenerate_df_multiindex\n\n generate_df_multiindex (tz:zoneinfo.ZoneInfo)",
    "crumbs": [
      "utils"
    ]
  },
  {
    "objectID": "utils.html#prepend-two-levels-of-index-vehicle-and-driver-to-the-dataframe-object",
    "href": "utils.html#prepend-two-levels-of-index-vehicle-and-driver-to-the-dataframe-object",
    "title": "utils",
    "section": "prepend two levels of index “vehicle” and “driver” to the DataFrame object",
    "text": "prepend two levels of index “vehicle” and “driver” to the DataFrame object\n\nsource\n\ngenerate_eos_df\n\n generate_eos_df (tz:zoneinfo.ZoneInfo)\n\ngenerate a pandas DataFrame for testing purpose\n\ngenerate_eos_df(tz)\n\n\nassert isinstance(generate_eos_df(tz).index, pd.MultiIndex), f\"dfs_episode.index is not a MultiIndex\"\n\n\nfrom fastcore.test import *\n\n\ntest_eq(isinstance(generate_eos_df(tz).index, pd.MultiIndex), True)",
    "crumbs": [
      "utils"
    ]
  },
  {
    "objectID": "utils.html#generate-recipe-for-local-pool-storage",
    "href": "utils.html#generate-recipe-for-local-pool-storage",
    "title": "utils",
    "section": "Generate Recipe for local pool storage",
    "text": "Generate Recipe for local pool storage\n\ntruck = trucks_by_id[\"VB7\"]\nmeta = ObservationMetaECU(\n    state_specs=StateSpecsECU(),\n    action_specs=ActionSpecs(\n        action_unit_code=\"nm\",\n        action_row_number=truck.torque_table_row_num_flash,\n        action_column_number=truck.torque_table_col_num,\n    ),\n    reward_specs=RewardSpecs(reward_unit_code=\"wh\", reward_number=1),\n    site=locations_by_abbr[truck.site.abbr],\n)\n\n\nmeta\n\n\ndata_folder = \"data\"\ncoll_type = \"RECORD\"\ntruck = trucks_by_id[\"default\"]\ntruck.site = locations_by_abbr[truck.site.abbr]\n\n\nnumber_states, number_actions = meta.get_number_of_states_actions()\nrecipe_default: ConfigParser = ConfigParser()\nrecipe_default.read_dict(\n    {\n        \"DEFAULT\": {  # should go into parquet tabel meta info\n            \"data_folder\": data_folder,  # '.',\n            \"recipe_file_name\": \"recipe.ini\",  # 'recipe.ini',\n            \"coll_type\": coll_type,\n        },\n        \"array_specs\": {  # should go into parquet columns meta info\n            \"states\": str(number_states),  # 50*4*3\n            \"actions\": str(number_actions),  # 17*4\n            \"rewards\": \"1\",\n            \"next_states\": str(number_states),  # 50*4*3\n        },\n    }\n)\n\n\nrecipe_generated = get_filemeta_config(data_folder=\"data\",config_file=\"recipe.ini\",meta=meta, coll_type=\"RECORD\")\n\n\nconfigparser_as_dict(recipe_generated)\n\n\ntest_eq(configparser_as_dict(recipe_default), configparser_as_dict(recipe_generated))",
    "crumbs": [
      "utils"
    ]
  },
  {
    "objectID": "02.system.log.html",
    "href": "02.system.log.html",
    "title": "log",
    "section": "",
    "text": "source\n\nset_root_logger\n\n set_root_logger (name:str, data_root:pathlib.Path, agent:str,\n                  tz:zoneinfo.ZoneInfo, truck:str, driver:str)\n\nSet the root logger for the system\n\n\n\n\nType\nDetails\n\n\n\n\nname\nstr\nname of the root logger\n\n\ndata_root\nPath\nroot path of the data\n\n\nagent\nstr\nagent name, ie. “DDPG”/“RDPG”\n\n\ntz\nZoneInfo\ntime zone of the logging\n\n\ntruck\nstr\ntruck name, ie. “VB7”\n\n\ndriver\nstr\ndriver name, ie. “wang-cheng”\n\n\nReturns\nTuple\nreturn the logger and the dict_logger",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">System</b>",
      "log"
    ]
  },
  {
    "objectID": "05.storage.pool.mongo.html",
    "href": "05.storage.pool.mongo.html",
    "title": "Mongo",
    "section": "",
    "text": "client_default: MongoClient = MongoClient(‘mongodb://localhost:27017/’) db_default = client_default[‘eos’] db_config_default = db_config_servers_by_name[“default”]\n\nsource\n\nMongoPool\n\n MongoPool (_cnt:int=0, meta:tspace.data.core.ObservationMeta,\n            db_config:tspace.config.db.DBConfig, doc_query:dict=&lt;factory&gt;,\n            codec_option:bson.codec_options.CodecOptions,\n            query:Optional[tspace.data.core.PoolQuery]=None,\n            coll_name:Optional[str]=None,\n            collection:Optional[pymongo.collection.Collection]=None,\n            client:Optional[pymongo.mongo_client.MongoClient]=None,\n            logger:Optional[logging.Logger]=None,\n            dict_logger:Optional[dict]=None)\n\n*A MongoDB store for storing data in a time series collection.\nfeatures:\ntimeseries interfacing pandas.DataFrame\nmultikey index for fast query\ntyped hints\nThe key leads to a config with db_name and collection name with a switch for record or episode.\nAttributes:\n- meta: ObservationMeta, meta information of the observation\n- db_config: DBConfig, database configuration\n- doc_query: dict, query dict for mongodb\n- codec_option: CodecOptions, codec options for mongodb\n- query: PoolQuery, query for mongodb\n- coll_name: str, collection name\n- collection: Collection, collection for mongodb\n- client: MongoClient, client for mongodb\n- logger: logging.Logger, logger for mongodb\n- dict_logger: dict, dict for logging*\n\nsource\n\n\nMongoPool.__post_init__\n\n MongoPool.__post_init__ ()\n\nUser weakref finalizer to make sure close is called when the object is destroyed\n\nsource\n\n\nMongoPool.load\n\n MongoPool.load ()\n\n*Initialize the pool interface\nThis function should: - connect to db - init*\n\nsource\n\n\nMongoPool.find_item\n\n MongoPool.find_item (doc_id:int)\n\nFind a record by id.\n\nsource\n\n\nMongoPool.close\n\n MongoPool.close ()\n\nclose the pool, for destructor\n\nsource\n\n\nMongoPool.parse_query\n\n MongoPool.parse_query (query:tspace.data.core.PoolQuery)\n\nOne-Trick Pony: check query is valid or not return dict as mongo document query filter if query is None, return empty dict\n\nsource\n\n\nMongoPool.store_record\n\n MongoPool.store_record (episode:pandas.core.frame.DataFrame)\n\nDeposit the records of an episode into the db.\n\nsource\n\n\nMongoPool.store_episode\n\n MongoPool.store_episode (episode:pandas.core.frame.DataFrame)\n\nDeposit a DataFrame of an episode into the db.\n\nsource\n\n\nMongoPool.store\n\n MongoPool.store (episode:pandas.core.frame.DataFrame)\n\n*Deposit the records of an episode into the db.\nBased on the type of the db collection, store the episode as a set of records or as a complete episode.*\n\nsource\n\n\nMongoPool.find\n\n MongoPool.find (query:tspace.data.core.PoolQuery)\n\nFind records by a query object.\n\nsource\n\n\nMongoPool.delete\n\n MongoPool.delete (item_id)\n\nDelete a record by item id.\n\nsource\n\n\nMongoPool._count\n\n MongoPool._count (query:Optional[tspace.data.core.PoolQuery]=None)\n\n*Count the number of records in the db.\nfor episode/record document*\n\nsource\n\n\nMongoPool.sample\n\n MongoPool.sample (size:int=4,\n                   query:Optional[tspace.data.core.PoolQuery]=None)\n\n*Sample a batch of records from the db.\ndb.hyperparameters_collection.aggregate([ {“\\(match\": {\"start_time\": {\"\\)exists”: False}}}, {“$sample”: {“size”: 1}}])\nif PoolQuery doesn’t contain ‘timestamp_start’ and ‘timestamp_end’, the full episode is retrieved.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsize\nint\n4\nbatch size, default 4\n\n\nquery\nOptional[PoolQuery]\nNone\n\n\n\nReturns\nOptional[pd.DataFrame]\n\nsamples as one multi-indexed Pandas DataFrame",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Storage</b>",
      "<b>Pool</b>",
      "Mongo"
    ]
  },
  {
    "objectID": "04.conn.udp.html",
    "href": "04.conn.udp.html",
    "title": "UDP",
    "section": "",
    "text": "source\n\nudp_context\n\n udp_context (host:str, port:str, timeout:Optional[float]=5.0)\n\nUDP context manager\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nhost\nstr\n\nhost for udp socket\n\n\nport\nstr\n\nport for udp socket\n\n\ntimeout\nOptional\n5.0\ntimeout for udp socket\n\n\nReturns\nGenerator\n\nGenerator for udp socket",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Connection</b>",
      "UDP"
    ]
  },
  {
    "objectID": "04.conn.remote_can_client.html",
    "href": "04.conn.remote_can_client.html",
    "title": "RemoteCanClient",
    "section": "",
    "text": "source\n\nTimeoutHTTPAdapter\n\n TimeoutHTTPAdapter (*args, **kwargs)\n\n*TimeoutHTTPAdapter is used to set a default timeout for all requests\nReference: https://findwork.dev/blog/advanced-usage-python-requests-timeouts-retries-hooks/#retry-on-failure*\n\nsource\n\n\nRemoteCanClient\n\n RemoteCanClient (host:str, port:str, truck:tspace.config.vehicles.Truck,\n                  proxies:Optional[Dict]=None, max_connections:int=1,\n                  logger:Optional[logging.Logger]=None,\n                  dict_logger:Optional[Dict]=None)\n\n*RemoteCanClient is used to send torque map to the remote can server and get observation signals from the server.\nAttributes:\nurl: str\n    The url of the remote can server, generated from host and port.\nhost: str\n    The host of the remote can server.\nport: str\n    The port of the remote can server.\ntruck: Truck\n    The truck object.\nproxies: Optional[Dict]\n    The proxies used to connect to the remote can server.\nmax_connections: int\n    The maximum number of connections to the remote can server.\nlogger: Optional[logging.Logger]\n    The logger used to log the information.\ndict_logger: Optional[Dict]\n    The dictionary logger used to log the information.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nhost\nstr\n\nhost name for remote can server\n\n\nport\nstr\n\nport number for remote can server\n\n\ntruck\nTruck\n\ntruck object which RemoteCanClient is attached to\n\n\nproxies\nOptional\nNone\noptional proxies for remote can server\n\n\nmax_connections\nint\n1\nmaximum number of connections to remote can server\n\n\nlogger\nOptional\nNone\nlogger for RemoteCanClient\n\n\ndict_logger\nOptional\nNone\ndictionary logger for RemoteCanClient\n\n\n\n\nsource\n\n\nRemoteCanClient.send_torque_map\n\n RemoteCanClient.send_torque_map (pedal_map:pandas.core.frame.DataFrame,\n                                  swap:bool=True, timeout=32)\n\n*Send torque map to the remote can server.\nArgs: pedal_map: pd.DataFrame The torque map to be sent to the remote can server. swap: bool Whether to swap the pedal map. timeout: int The timeout for the request.\nReturns: json_ret: Dict The response from the remote can server if successful.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npedal_map\nDataFrame\n\nThe torque map to be sent to the remote can server.\n\n\nswap\nbool\nTrue\nWhether to swap the pedal map.\n\n\ntimeout\nint\n32\nThe timeout for the request.\n\n\nReturns\nDict\n\nThe response from the remote can server if successful\n\n\n\n\nsource\n\n\nRemoteCanClient.get_signals\n\n RemoteCanClient.get_signals (duration, timeout=32)\n\n*Get observation signals from the remote can server.\nArgs: duration: int The duration of the signals to be retrieved. timeout: int The timeout for the request.\nReturn: json_ret: Dict The response from the remote can server (a dictionary containing the observation) if successful.*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nduration\n\n\nThe duration of the signals to be retrieved.\n\n\ntimeout\nint\n32\nThe timeout for the request.\n\n\nReturns\nDict",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Connection</b>",
      "RemoteCanClient"
    ]
  },
  {
    "objectID": "03.config.messenger.html",
    "href": "03.config.messenger.html",
    "title": "messengers",
    "section": "",
    "text": "CANMessenger\n\n CANMessenger (server_name, host, port, protocol)\n\n\n\n\nTripMessenger\n\n TripMessenger (server_name, host, port, protocol)\n\n\ncan_server_list\n\n\ncan_servers_by_name\n\n\ncan_servers_by_host\n\n\ntrip_server_list\n\n\ntrip_servers_by_name\n\n\ntrip_servers_by_host",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Config</b>",
      "messengers"
    ]
  },
  {
    "objectID": "02.system.gracefulkiller.html",
    "href": "02.system.gracefulkiller.html",
    "title": "graceful killer",
    "section": "",
    "text": "source\n\nGracefulKiller\n\n GracefulKiller (exit_evt:threading.Event)\n\n*GracefulKiller is a class that can be used to handle the exit signals\nIt forwards the three exit signals to threads that are waiting for the exit event to be set.\nAttribute:\nexit: Event\n    the event that will be set when the exit signal is received*",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">System</b>",
      "graceful killer"
    ]
  },
  {
    "objectID": "05.storage.buffer.dask.html",
    "href": "05.storage.buffer.dask.html",
    "title": "Dask",
    "section": "",
    "text": "source\n\nDaskBuffer\n\n DaskBuffer (pool:Union[tspace.storage.pool.parquet.ParquetPool,tspace.sto\n             rage.pool.avro.avro.AvroPool,NoneType]=None, batch_size:int,\n             recipe:configparser.ConfigParser,\n             driver:tspace.config.drivers.Driver,\n             truck:tspace.config.vehicles.Truck,\n             meta:tspace.data.core.ObservationMeta,\n             torque_table_row_names:list[str],\n             query:Optional[tspace.data.core.PoolQuery]=None,\n             logger:Optional[logging.Logger]=None,\n             dict_logger:Optional[dict]=None)\n\n*A Buffer connected with a data array file pool\nArgs: recipe: ConfigParser containing a folder for the data files and the ObservationMeta batch_size: the batch size for sampling driver: the driver truck: the subject of the experiment meta: the metadata of the overservation ObservationMeta torque_table_row_names: the names of the torque table rows, e.g. [‘r0, r1, r2, r3, r4, r5, r6, r7, r8, r9’] pool: the pool to sample from, default is ParquetPool query: the query to sample from the pool, default is PoolQuery logger: the logger dict_logger: the dictionary logger*\n\nsource\n\n\nDaskBuffer.__post_init__\n\n DaskBuffer.__post_init__ ()\n\nset logger and load the pool to the buffer\n\nsource\n\n\nDaskBuffer.load\n\n DaskBuffer.load ()\n\nload the pool to the buffer\n\nsource\n\n\nDaskBuffer.sample\n\n DaskBuffer.sample ()\n\nSampling from the MongoDB pool\n\nsource\n\n\nDaskBuffer.close\n\n DaskBuffer.close ()\n\nclose the pool, for destructor\n\nsource\n\n\nDaskBuffer.decode_batch_records\n\n DaskBuffer.decode_batch_records (batch:pandas.core.frame.DataFrame)\n\n*Decode the batch records from dask DataFrame to numpy arrays\nsample from parquet pool through dask give dask DataFrame, no heavy decoding required just slicing and converting to numpy array\nArg:\nbatch: the batch of records from dask DataFrame\nReturn:\n    states: the states of the batch\n    actions: the actions of the batch\n    rewards: the rewards of the batch\n    nstates: the next states of the batch*",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Storage</b>",
      "<b>Buffer</b>",
      "Dask"
    ]
  },
  {
    "objectID": "07.agent.ddpg.html",
    "href": "07.agent.ddpg.html",
    "title": "DDPG",
    "section": "",
    "text": "source\n\nDDPG\n\n DDPG (_buffer:Union[tspace.storage.buffer.mongo.MongoBuffer,tspace.storag\n       e.buffer.dask.DaskBuffer,NoneType]=None,\n       _actor_model:Optional[keras.src.models.model.Model]=None,\n       _critic_model:Optional[keras.src.models.model.Model]=None,\n       _target_actor_model:Optional[keras.src.models.model.Model]=None,\n       _target_critic_model:Optional[keras.src.models.model.Model]=None, m\n       anager_critic:Optional[tensorflow.python.checkpoint.checkpoint_mana\n       gement.CheckpointManager]=None, ckpt_critic:Optional[tensorflow.pyt\n       hon.checkpoint.checkpoint.Checkpoint]=None, manager_actor:Optional[\n       tensorflow.python.checkpoint.checkpoint_management.CheckpointManage\n       r]=None, ckpt_actor:Optional[tensorflow.python.checkpoint.checkpoin\n       t.Checkpoint]=None,\n       actor_saved_model_path:Optional[pathlib.Path]=None,\n       critic_saved_model_path:Optional[pathlib.Path]=None,\n       _truck:tspace.config.vehicles.Truck,\n       _driver:tspace.config.drivers.Driver, _resume:bool, _coll_type:str,\n       _hyper_param:Union[tspace.agent.utils.hyperparams.HyperParamDDPG,ts\n       pace.agent.utils.hyperparams.HyperParamRDPG,tspace.agent.utils.hype\n       rparams.HyperParamIDQL], _pool_key:str, _data_folder:str,\n       _infer_mode:bool, _episode_start_dt:Optional[pandas._libs.tslibs.ti\n       mestamps.Timestamp]=None, _observation_meta:Union[tspace.data.core.\n       ObservationMetaCloud,tspace.data.core.ObservationMetaECU,NoneType]=\n       None, _torque_table_row_names:Optional[list[str]]=None,\n       _observations:Optional[list[pandas.core.series.Series]]=None,\n       _epi_no:Optional[int]=None, logger:Optional[logging.Logger]=None,\n       dict_logger:Optional[dict]=None)\n\n*DDPG agent.\ndata interface: - pool in mongodb - buffer in memory (numpy array) model interface: - actor network - critic network\nAttributes:\n_buffer: Optional[Union[MongoBuffer, DaskBuffer]] = None\n    buffer for storing data, default is None\n_actor_model: Optional[tf.keras.Model] = None\n    actor network, default is None\n_critic_model: Optional[tf.keras.Model] = None\n    critic network, default is None\n_target_actor_model: Optional[tf.keras.Model] = None\n    target actor network, default is None\n_target_critic_model: Optional[tf.keras.Model] = None\n    target critic network, default is None\nmanager_critic: Optional[tf.train.CheckpointManager] = None\n    manager for saving critic network, default is None\nckpt_critic: Optional[tf.train.Checkpoint] = None\n    checkpoint for saving critic network, default is None\nmanager_actor: Optional[tf.train.CheckpointManager] = None\n    manager for saving actor network, default is None\nckpt_actor: Optional[tf.train.Checkpoint] = None\n    checkpoint for saving actor network, default is None\nactor_saved_model_path: Optional[Path] = None\n    path for saving actor network as saved model, default is None\ncritic_saved_model_path: Optional[Path] = None\n    path for saving critic network as saved model, default is None*\n\nsource\n\n\nDDPG.init_checkpoint\n\n DDPG.init_checkpoint ()\n\nadd checkpoints manager\n\nsource\n\n\nDDPG.save_as_saved_model\n\n DDPG.save_as_saved_model ()\n\nsave the actor and critic networks as saved model\n\nsource\n\n\nDDPG.load_saved_model\n\n DDPG.load_saved_model ()\n\nload the actor and critic networks from saved model\n\nsource\n\n\nDDPG.convert_to_tflite\n\n DDPG.convert_to_tflite ()\n\nconvert the actor and critic networks to tflite format\n\nsource\n\n\nDDPG.model_summary_print\n\n DDPG.model_summary_print (mdl:keras.src.models.model.Model,\n                           file_path:pathlib.Path)\n\nprint the model summary to a file\n\nsource\n\n\nDDPG.tflite_analytics_print\n\n DDPG.tflite_analytics_print (tflite_file_path:pathlib.Path)\n\nprint the tflite model analytics to a file\n\nsource\n\n\nDDPG.save_ckpt\n\n DDPG.save_ckpt ()\n\nSave checkpoints for the actor and critic networks\n\nsource\n\n\nDDPG.update_target\n\n DDPG.update_target (target_weights, weights, tau)\n\nupdate the target networks\n\nsource\n\n\nDDPG.soft_update_target\n\n DDPG.soft_update_target ()\n\nupdate the target networks with Polyak averaging\n\nsource\n\n\nDDPG.get_actor\n\n DDPG.get_actor (num_states:int, num_actions:int,\n                 num_actor_inputs1:int=256, num_actor_inputs2:int=256,\n                 num_layers:int=2, action_bias:float=0)\n\nCreate actor network\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_states\nint\n\nnumber of states, 600\n\n\nnum_actions\nint\n\nnumber of actions, 68\n\n\nnum_actor_inputs1\nint\n256\nnumber of inputs to the first dense layer, 256\n\n\nnum_actor_inputs2\nint\n256\nnumber of inputs to the second and subsequent dense layers, 256\n\n\nnum_layers\nint\n2\nnumber of layers, 2\n\n\naction_bias\nfloat\n0\naction bias, 0\n\n\n\n\nsource\n\n\nDDPG.get_critic\n\n DDPG.get_critic (num_states:int, num_actions:int,\n                  num_state_input_dense1:int=16,\n                  num_state_input_dense2:int=32,\n                  num_action_input_dense:int=32,\n                  num_output_dense1:int=256, num_output_dense2:int=256,\n                  num_layers:int=2)\n\n\nsource\n\n\nDDPG.policy\n\n DDPG.policy (state:pandas.core.series.Series)\n\n*sample actions with additive ou noise\ninput: state is a pd.Series of length 3103/4503 (r*c), output numpy array\nAction outputs and noise object are all row vectors of length 2117 (rc), output numpy array*\n\nsource\n\n\nDDPG.actor_predict\n\n DDPG.actor_predict (state:pandas.core.series.Series)\n\nactor_predict returns an action sampled from our Actor network without noise. add optional t just to have uniform interface with rdpg\n\nsource\n\n\nDDPG.infer_single_sample\n\n DDPG.infer_single_sample\n                           (state_flat:tensorflow.python.framework.tensor.\n                           Tensor)\n\nGet a single sample from inferring\n\nsource\n\n\nDDPG.touch_gpu\n\n DDPG.touch_gpu ()\n\ntouch gpu to initialize the graph\n\nsource\n\n\nDDPG.sample_minibatch\n\n DDPG.sample_minibatch ()\n\nConvert batch type from DataFrames to flattened tensors.\n\nsource\n\n\nDDPG.train\n\n DDPG.train ()\n\nTrain the networks on the batch sampled from the pool.\n\nsource\n\n\nDDPG.update_with_batch\n\n DDPG.update_with_batch (state_batch, action_batch, reward_batch,\n                         next_state_batch, training=True)\n\n*Update the networks with the batch sampled from the pool.\nEager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows TensorFlow to build a static graph out of the logic and computations in our function. This provides a large speed-up for blocks of code that contain many small TensorFlow operations such as this one.*\n\nsource\n\n\nDDPG.get_losses\n\n DDPG.get_losses ()\n\nGet the losses of the networks on the batch sampled from the pool.",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Agent</b>",
      "DDPG"
    ]
  },
  {
    "objectID": "02.system.exception.html",
    "href": "02.system.exception.html",
    "title": "exception",
    "section": "",
    "text": "source\n\nReadOnlyError\nCommon base class for all non-exit exceptions.\n\nsource\n\n\nWriteOnlyError\nCommon base class for all non-exit exceptions.\n\nsource\n\n\nTruckIDError\nCommon base class for all non-exit exceptions.",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">System</b>",
      "exception"
    ]
  },
  {
    "objectID": "05.storage.pool.avro.avro.html",
    "href": "05.storage.pool.avro.avro.html",
    "title": "Avro",
    "section": "",
    "text": "source\n\nAvroPool\n\n AvroPool (_cnt:int=0, recipe:configparser.ConfigParser,\n           query:tspace.data.core.PoolQuery,\n           meta:tspace.data.core.ObservationMeta,\n           pl_path:Optional[pathlib.Path]=None,\n           logger:Optional[logging.Logger]=None,\n           dict_logger:Optional[dict]=None,\n           dbg:Optional[dask.bag.core.Bag]=None,\n           dbg_schema:Optional[dict]=None)\n\n*AvroPool is the avro storage for pooling the real-time data from the cloud.\nFeatures: - It’s supposed to support large local data pool with buffer capacity only bounded by local system storage.\n- It uses Dask Bag to store the data in memory and Dask DataFrame to process the data.\n\n- Meta information is stored in avro metadata in each of avro file. Sampling\n\n- Random episodes needs some care to reassure the randomness. It uses Dask Delayed to parallelize the data processing like sampling\nAttributes: - dbg: Dask Bag of episodes - dbg_schema: schema for avro file decoding*\n\nsource\n\n\nAvroPool.__post_init__\n\n AvroPool.__post_init__ ()\n\nSet up logger, post init of DaskPool and load the pool.\n\nsource\n\n\nAvroPool.find\n\n AvroPool.find (query:tspace.data.core.PoolQuery)\n\n*Find records by the PoolQuery object. The down-stream task can use pandas dataframe unique() for index to get unique episodes.\nArg: query: PoolQuery object\nReturn: A multi-indexed DataFrame with all episodes in the query range.*\n\nsource\n\n\nAvroPool.load\n\n AvroPool.load ()\n\nload EPISODE arrays from avro files in folder specified by the recipe\n\nsource\n\n\nAvroPool.close\n\n AvroPool.close ()\n\nclose the pool\n\nsource\n\n\nAvroPool.sample\n\n AvroPool.sample (size:int=4,\n                  query:Optional[tspace.data.core.PoolQuery]=None)\n\n*Sample a batch of episodes from Apache avro pool.\ndownstream can use pandas DataFrame unique() for index to extract single episodes. since return is a dataframe, downstream can use pandas dataframe unique() for index to get unique episodes. Therefore, decoding to DataFrame have to be done in avro pool\nArgs: size: number of episodes to sample query: PoolQuery object\nReturn: A DataFrame with all episodes*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsize\nint\n4\ndesired size of the samples\n\n\nquery\nOptional[PoolQuery]\nNone\n\n\n\nReturns\npd.DataFrame\n\nquery for sampling\n\n\n\n\nsource\n\n\nAvroPool.store\n\n AvroPool.store (episode:pandas.core.frame.DataFrame)\n\nDeposit an episode as a single item into avro.\n\nsource\n\n\nAvroPool.get_query\n\n AvroPool.get_query (query:Optional[tspace.data.core.PoolQuery]=None)\n\n*get query from dask dataframe\nArg: query: PoolQuery object\nreturn: a Dask Bag with all episodes in the query range*\n\nsource\n\n\nAvroPool.remove_episode\n\n AvroPool.remove_episode (query:tspace.data.core.PoolQuery)\n\n*remove episodes in the query from bag, but not from avro file!\nDelete all episodes in the query range. Modify the bag in place.\nArg: query: PoolQuery object\nReturn: None*",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Storage</b>",
      "<b>Pool</b>",
      "<b>Avro</b>",
      "Avro"
    ]
  },
  {
    "objectID": "01.data.location.html",
    "href": "01.data.location.html",
    "title": "location",
    "section": "",
    "text": "LocationCat",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Data</b>",
      "location"
    ]
  },
  {
    "objectID": "01.data.location.html#location-category",
    "href": "01.data.location.html#location-category",
    "title": "location",
    "section": "",
    "text": "LocationCat",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Data</b>",
      "location"
    ]
  },
  {
    "objectID": "01.data.location.html#location-class",
    "href": "01.data.location.html#location-class",
    "title": "location",
    "section": "location class",
    "text": "location class\n\nsource\n\nEosLocation\n\n EosLocation (abbr:str, name:str, cname:str, tz:zoneinfo.ZoneInfo)\n\n*location class for eos, abbr, name, cname, tz are required\nargs:\nabbr: abbreviation\nname: name\ncname: chinese name\ntz: timezone\nreturn: EosLocation*\n\n\nLocation serialization\n\nsource\n\n\nEosLocation.serialize_tz\n\n EosLocation.serialize_tz (tz:zoneinfo.ZoneInfo, _info)\n\n*serialize timezone to string\nargs:\ntz: timezone\n_info:  other info\nreturn:\n    str*\n\n\n\n\nType\nDetails\n\n\n\n\ntz\nZoneInfo\n\n\n\n_info\n\n# timezone # other info\n\n\n\n\npprint(locations)\n\n\npprint(locations_by_abbr)\n\n\npprint(locations_from_tz)",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Data</b>",
      "location"
    ]
  },
  {
    "objectID": "02.system.decorator.html",
    "href": "02.system.decorator.html",
    "title": "decorator",
    "section": "",
    "text": "source\n\nprepend_string_arg\n\n prepend_string_arg (str_arg='TQD_trqTrqSetNormal_MAP_v')\n\n*function decorator to prepend a string argument to a function\nParameters:\nstr_arg (str): string argument to prepend\nReturn:\n    function: decorated function*",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">System</b>",
      "decorator"
    ]
  },
  {
    "objectID": "06.dataflow.vehicle_interface.html",
    "href": "06.dataflow.vehicle_interface.html",
    "title": "VehicleInterface",
    "section": "",
    "text": "source\n\nVehicleInterface\n\n VehicleInterface (truck:tspace.config.vehicles.Truck,\n                   driver:tspace.config.drivers.Driver,\n                   can_server:tspace.config.messengers.CANMessenger,\n                   resume:bool=False,\n                   data_dir:Optional[pathlib.Path]=None,\n                   flash_count:int=0, episode_count:int=0,\n                   vcu_calib_table_row_start:int=0, torque_table_default:O\n                   ptional[pandas.core.frame.DataFrame]=None, torque_table\n                   _live:Optional[pandas.core.frame.DataFrame]=None,\n                   epi_countdown_time:float=3.0, lock_watchdog:&lt;built-\n                   infunctionallocate_lock&gt;=&lt;unlocked _thread.lock object\n                   at 0x7f845e29d600&gt;, capture_failure_count:int=0,\n                   flash_failure_count:int=0,\n                   logger:Optional[logging.Logger]=None,\n                   dict_logger:Optional[dict]=None)\n\n*VehicleInterface is an ABC. It’s a Producer(get vehicle status), a Consumer(flasher) and a Filter(generate observation data)\nArgs:\ntruck: [`Truck`](https://Binjian.github.io/tspace/03.config.vehicles.html#truck) object\ndriver: [`Driver`](https://Binjian.github.io/tspace/03.config.drivers.html#driver) object\ncan_server: `CANMessenger` object\nresume: resume from last table\ndata_dir: data directory\nflash_count: flash count\nepisode_count: episode count\nvcu_calib_table_row_start: vcu calibration table row start\ntorque_table_default: default torque table\ntorque_table_live: live torque table\nepi_countdown_time: episode countdown time\nlock_watchdog: lock for the following two watchdog variables\ncapture_failure_count: count of caputure failure\nflash_failure_count: count of flash failure\nlogger: logger\ndict_logger: dict logger*\n\nsource\n\n\nVehicleInterface.init_vehicle\n\n VehicleInterface.init_vehicle ()\n\ninitialize vehicle interface. Flashing the vehicle with initial/default table.\n\nsource\n\n\nVehicleInterface.flash_vehicle\n\n VehicleInterface.flash_vehicle (torque_table:pandas.core.frame.DataFrame)\n\nAbstract method to flash the vehicle. Implemented by the concrete class Kvaser and Cloud.\n\nsource\n\n\nVehicleInterface.hmi_control\n\n VehicleInterface.hmi_control\n                               (hmi_pipeline:tspace.dataflow.pipeline.queu\n                               e.Pipeline[str], observe_pipeline:tspace.da\n                               taflow.pipeline.queue.Pipeline[pandas.core.\n                               frame.DataFrame],\n                               start_event:threading.Event,\n                               stop_event:threading.Event,\n                               interrupt_event:threading.Event,\n                               countdown_event:threading.Event,\n                               exit_event:threading.Event,\n                               flash_event:threading.Event)\n\nHMI control logics by incoming events\n\n\n\n\nType\nDetails\n\n\n\n\nhmi_pipeline\nPipeline\ninput HMI pipeline\n\n\nobserve_pipeline\nPipeline\nobservation pipeline\n\n\nstart_event\nEvent\ninput event start\n\n\nstop_event\nEvent\ninput event stop\n\n\ninterrupt_event\nEvent\ninput event interrupt\n\n\ncountdown_event\nEvent\ninput event countdown\n\n\nexit_event\nEvent\ninput event exit\n\n\nflash_event\nEvent\ninput event flash\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nVehicleInterface.filter\n\n VehicleInterface.filter (in_pipeline:tspace.dataflow.pipeline.deque.Pipel\n                          ineDQ[typing.Union[dict[str,str],dict[str,dict[s\n                          tr,list[typing.Union[str,list[list[str]]]]]]]], \n                          out_pipeline:tspace.dataflow.pipeline.queue.Pipe\n                          line[pandas.core.frame.DataFrame],\n                          start_event:Optional[threading.Event],\n                          stop_event:Optional[threading.Event],\n                          interrupt_event:Optional[threading.Event],\n                          flash_event:Optional[threading.Event],\n                          exit_event:Optional[threading.Event])\n\n*Produce data into the pipeline\nmain entry to the capture thread sub-thread method*\n\n\n\n\nType\nDetails\n\n\n\n\nin_pipeline\nPipelineDQ\n\n\n\nout_pipeline\nPipeline\n\n\n\nstart_event\nOptional\n\n\n\nstop_event\nOptional\n\n\n\ninterrupt_event\nOptional\ninput event\n\n\nflash_event\nOptional\n\n\n\nexit_event\nOptional\n\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nVehicleInterface.init_internal_pipelines\n\n VehicleInterface.init_internal_pipelines ()\n\nAbstract method for initializing types of raw_pipeline and hmi_pipeline\n\nsource\n\n\nVehicleInterface.ignite\n\n VehicleInterface.ignite (observe_pipeline:tspace.dataflow.pipeline.queue.\n                          Pipeline[pandas.core.frame.DataFrame], flash_pip\n                          eline:tspace.dataflow.pipeline.queue.Pipeline[pa\n                          ndas.core.frame.DataFrame],\n                          start_event:threading.Event,\n                          stop_event:threading.Event,\n                          interrupt_event:threading.Event,\n                          flash_event:threading.Event,\n                          exit_event:threading.Event,\n                          watchdog_nap_time:float,\n                          watchdog_capture_error_upper_bound:int,\n                          watchdog_flash_error_upper_bound:int)\n\n*creating the ThreadPool for handing the hmi, data capturing and data processing\nmain entry to the vehicle thread. will spawn three further threads for - input processing, HMI control and output processing - data into the pipeline - handle the input pipeline - guide observation data into the output pipeline - start/stop/interrupt/countdown/exit event to control the state machine main entry to the capture thread*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nobserve_pipeline\nPipeline\nobservation pipeline\n\n\nflash_pipeline\nPipeline\nflash pipeline\n\n\nstart_event\nEvent\ninput event start\n\n\nstop_event\nEvent\ninput event stop\n\n\ninterrupt_event\nEvent\ninput event interrupt\n\n\nflash_event\nEvent\ninput event flash\n\n\nexit_event\nEvent\ninput event exit\n\n\nwatchdog_nap_time\nfloat\nwatch dog nap time in seconds\n\n\nwatchdog_capture_error_upper_bound\nint\ncapture error limit to exit for watch dog\n\n\nwatchdog_flash_error_upper_bound\nint\nflash error limit to exit for watch dog\n\n\n\n\nsource\n\n\nVehicleInterface.produce\n\n VehicleInterface.produce (raw_pipeline:tspace.dataflow.pipeline.deque.Pip\n                           elineDQ[typing.Union[dict[str,str],dict[str,dic\n                           t[str,list[typing.Union[str,list[list[str]]]]]]\n                           ]], hmi_pipeline:Optional[tspace.dataflow.pipel\n                           ine.queue.Pipeline[str]]=None,\n                           exit_event:Optional[threading.Event]=None)\n\n*Abstract method for producing data into the pipeline\nmain entry to the capture thread will spawn three further threads for input processing, HMI control and output processing*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraw_pipeline\nPipelineDQ\n\ninput pipeline for the raw data\n\n\nhmi_pipeline\nOptional\nNone\n\n\n\nexit_event\nOptional\nNone\ninput event exit\n\n\n\n\nsource\n\n\nVehicleInterface.watch_dog\n\n VehicleInterface.watch_dog (countdown_event:threading.Event,\n                             exit_event:threading.Event,\n                             watchdog_nap_time:float,\n                             watchdog_capture_error_upper_bound:int,\n                             watchdog_flash_error_upper_bound:int)\n\nwatch dog callback for the watch dog thread\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ncountdown_event\nEvent\nwatch dog need to send trigger signal to end count down thread\n\n\nexit_event\nEvent\ninput event\n\n\nwatchdog_nap_time\nfloat\nnap time for watch dog\n\n\nwatchdog_capture_error_upper_bound\nint\nupperbound for capture failure\n\n\nwatchdog_flash_error_upper_bound\nint\nupperbound for flash failure\n\n\n\n\nsource\n\n\nVehicleInterface.countdown\n\n VehicleInterface.countdown (observe_pipeline:tspace.dataflow.pipeline.que\n                             ue.Pipeline[pandas.core.frame.DataFrame],\n                             start_event:threading.Event,\n                             countdown_event:threading.Event,\n                             stop_event:threading.Event,\n                             exit_event:threading.Event)\n\ncountdown callback for the countdown thread\n\n\n\n\nType\nDetails\n\n\n\n\nobserve_pipeline\nPipeline\noutput pipeline\n\n\nstart_event\nEvent\noutput event\n\n\ncountdown_event\nEvent\ninput event\n\n\nstop_event\nEvent\noutput event\n\n\nexit_event\nEvent\ninput event\n\n\n\n\nsource\n\n\nVehicleInterface.consume\n\n VehicleInterface.consume (flash_pipeline:tspace.dataflow.pipeline.queue.P\n                           ipeline[pandas.core.frame.DataFrame],\n                           start_event:Optional[threading.Event]=None,\n                           stop_event:Optional[threading.Event]=None,\n                           interrupt_event:Optional[threading.Event]=None,\n                           exit_event:Optional[threading.Event]=None,\n                           flash_event:Optional[threading.Event]=None)\n\n*Consume data from the pipeline\nmain entry to the flash thread data in pipeline is a tuple of (torque_table, flash_start_row)*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nflash_pipeline\nPipeline\n\nflash pipeline\n\n\nstart_event\nOptional\nNone\ninput event start\n\n\nstop_event\nOptional\nNone\ninput event stop\n\n\ninterrupt_event\nOptional\nNone\ninput event interrupt\n\n\nexit_event\nOptional\nNone\ninput event exit\n\n\nflash_event\nOptional\nNone\ninput event flash",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Dataflow</b>",
      "VehicleInterface"
    ]
  },
  {
    "objectID": "04.conn.remote_can_exceptions.html",
    "href": "04.conn.remote_can_exceptions.html",
    "title": "Exceptions",
    "section": "",
    "text": "source\n\nRemoteCanException\n\n RemoteCanException (err_code:Optional[int]=1001,\n                     extra_msg:Optional[str]=None,\n                     codes:Optional[collections.UserDict]=None)\n\nBase class for all remote CAN exceptions.\n\n@dataclass(kw_only=True)\nclass RemoteCanException(Exception):\n    \"\"\"Base class for all remote CAN exceptions.\"\"\"\n\n    err_code: Optional[int] = 1001  # default exception is unknown connection error\n    extra_msg: Optional[str] = None\n    codes: Optional[UserDict] = None  # = field(default_factory=UserDict)\n\n    def __post_init__(self):\n        self.codes = UserDict(  # class attribute, if not given use the default\n            {\n                0: \"success\",\n                1: \"client_cannot_connect_to_server\",\n                2: \"ai_mode_shutdown\",\n                1000: \"network_connection_error\",\n                1001: \"network_unknown_error\",\n                1002: \"network_timeout\",\n                -1: \"tsp_internal_error\",\n                202: \"tsp_no_API_exist\",\n                206: \"tsp_parameter_wrong\",\n                301: \"tsp_out_of_time\",\n                302: \"tsp_command_execute_error\",\n                303: \"tsp_car_not_registered\",\n                304: \"tsp_car_offline\",\n                310: \"tsp_internal_exception_error\",\n                311: \"tsp_tbox_returned_error\",\n                3000: \"tsp_return_result_is_not_dictionary\",\n                3001: \"tsp_return_result_has_no_oss_link\",\n                2000: \"uds_version_failed\",\n                2001: \"ab_torque_switch_failed\",\n                2002: \"oss_data_not_enough\",\n                2003: \"torque_shape_error\",\n                2004: \"torque_range_error\",\n                2005: \"remote_can_unknown_format\",\n            }\n        )\n        # print(\n        #     f\"{{\\'header\\': \\'err_code\\': \\'{self.err_code}\\', \"\n        #     f\"\\'msg\\': \\'{self.codes[self.err_code]}\\', \"\n        #     f\"\\'extra_msg\\': \\'{self.extra_msg}\\'}}\"\n        # )",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Connection</b>",
      "Exceptions"
    ]
  },
  {
    "objectID": "03.config.utils.html",
    "href": "03.config.utils.html",
    "title": "utils",
    "section": "",
    "text": "source\n\nstr_to_truck\n\n str_to_truck (truck_str:str)\n\n*convert string to TruckInCloud or TruckInField object\nParameter:\ntruck_str: string of truch such as 'HMZABAAH7MF011058'  or \"VB7\",\nReturn:\n    truck: TruckInCloud or TruckInField*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ntruck_str\nstr\nstring of truch such as ‘HMZABAAH7MF011058’ or “VB7”,\n\n\nReturns\nUnion\nTruckInCloud or TruckInField object\n\n\n\n\nsource\n\n\nstr_to_driver\n\n str_to_driver (driver_str:str)\n\n*convert string to Driver object\nParameter:\ndriver_str: string of driver such as 'zheng-longfei'\nReturn:\n    driver: Driver object*\n\n\n\n\nType\nDetails\n\n\n\n\ndriver_str\nstr\nstring of driver such as ‘zheng-longfei’\n\n\nReturns\nDriver\nDriver object\n\n\n\n\nsource\n\n\nstr_to_can_server\n\n str_to_can_server (can_server_str:str)\n\n*convert string to CANMessenger object\nParameter:\ncan_server_str: string of can_server such as 'can_intra'\nReturn:\n    can_server: CANMessenger object*\n\n\n\n\nType\nDetails\n\n\n\n\ncan_server_str\nstr\nstring of can_server such as ‘can_intra’\n\n\nReturns\nCANMessenger\nCANMessenger object\n\n\n\n\nsource\n\n\nstr_to_trip_server\n\n str_to_trip_server (trip_server_str:str)\n\n*convert string to TripMessenger object\nParameter:\ntrip_server_str: string of trip_server such as 'rocket_intra'\nReturn:\n    trip_server: TripMessenger object*\n\n\n\n\nType\nDetails\n\n\n\n\ntrip_server_str\nstr\nstring of trip_server such as ‘rocket_intra’\n\n\nReturns\nTripMessenger\nTripMessenger object",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Config</b>",
      "utils"
    ]
  },
  {
    "objectID": "01.data.external.numpy_utils.html",
    "href": "01.data.external.numpy_utils.html",
    "title": "Numpy utilities",
    "section": "",
    "text": "source\n\nnan_helper_1d\n\n nan_helper_1d (y:numpy.ndarray)\n\n*Helper to handle indices and logical indices of NaNs.\nParameters:\n- y, 1d numpy array with possible NaNs\nReturns:\n- nans, logical indices of NaNs\n- index, a function, with signature indices= index(logical_indices),\n    to convert logical indices of NaNs to 'equivalent' indices\nExample: linear interpolation of NaNs\n&gt;&gt;&gt; nans, x= nan_helper_1d(y)\n&gt;&gt;&gt; y[nans]= np.interp(x(nans), x(~nans), y[~nans])\nsources:\nhttps://stackoverflow.com/questions/6518811/interpolate-nan-values-in-a-numpy-array*\n\n\n\n\nType\nDetails\n\n\n\n\ny\nndarray\nutils for handling indices and logical indices of NaNs\n\n\nReturns\nTuple\n\n\n\n\n\nsource\n\n\nnan_interp_1d\n\n nan_interp_1d (y:numpy.ndarray)\n\n*Linear interpolation of NaNs.\nParameter:\n- y, 1d numpy array with possible NaNs\nReturn:\n- y, 1d numpy array with NaNs replaced by linear interpolation\nExample: linear interpolation of NaNs\n&gt;&gt;&gt; y = nan_interp_1d(y)\nsources:\nhttps://stackoverflow.com/questions/6518811/interpolate-nan-values-in-a-numpy-array*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ny\nndarray\nlinear interpolation of NaNs\n\n\nReturns\nndarray\nnp array with NaNs replaced by linear interpolation\n\n\n\n\nsource\n\n\nragged_nparray_list_interp\n\n ragged_nparray_list_interp (ragged_list_list:List[List], ob_num:int)\n\n*Linear interpolation of NaNs.\nParameter:\n- y, list ragged array\nReturn:\n- y, 2d numpy array with missing elements replaced by linear interpolation\nExample: linear interpolation of NaNs\n&gt;&gt;&gt; y = nan_interp_1d(y)\nsources:\nhttps://stackoverflow.com/questions/6518811/interpolate-nan-values-in-a-numpy-array*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nragged_list_list\nList\nlist to be interpolated with NaNs\n\n\nob_num\nint\nnumber of observations\n\n\nReturns\nndarray\nnp array with NaNs replaced by linear interpolation\n\n\n\n\nsource\n\n\ntimestamps_from_can_strings\n\n timestamps_from_can_strings (can_timestamp_strings:List[str],\n                              signal_freq:float, unit_num:int,\n                              unit_duration:float)\n\n*Extract timestamps from CAN strings.\nParameters:\n- can_timestamp_strings, list of CAN strings\n- signal_freq, signal frequency\n- unit_num, number of CAN units\n- unit_duration, duration in seconds per CAN unit\nReturn:\n- timestamps, 1d numpy array of timestamps\nExample: extract timestamps from “CAN” strings\n&gt;&gt;&gt; timestamps_arr = timestamps_from_can_strings(can_timestamp_strings)*\n\n\n\n\nType\nDetails\n\n\n\n\ncan_timestamp_strings\nList\nlist of strings from CAN\n\n\nsignal_freq\nfloat\nsignal frequency\n\n\nunit_num\nint\nnumber of CAN units\n\n\nunit_duration\nfloat\nduration in seconds per CAN unit\n\n\nReturns\nndarray\nnp array of timestamps",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Data</b>",
      "<b>External</b>",
      "Numpy utilities"
    ]
  },
  {
    "objectID": "06.dataflow.filter.hetero.html",
    "href": "06.dataflow.filter.hetero.html",
    "title": "Heterofilter",
    "section": "",
    "text": "source\n\nHeteroFilter\n\n HeteroFilter ()\n\nHetereofilter is a filter that consumes data from a pipeline and produces data to another pipeline with a different type.\n\nsource\n\n\nHeteroFilter.filter\n\n HeteroFilter.filter\n                      (in_pipeline:tspace.dataflow.pipeline.deque.Pipeline\n                      DQ[~T_I], out_pipeline:tspace.dataflow.pipeline.queu\n                      e.Pipeline[~T_O],\n                      start_event:Optional[threading.Event],\n                      stop_event:Optional[threading.Event],\n                      interrupt_event:Optional[threading.Event],\n                      flash_event:Optional[threading.Event],\n                      exit_event:Optional[threading.Event])\n\nconsume data into the pipeline, and produce data to the output pipeline\n\n\n\n\nType\nDetails\n\n\n\n\nin_pipeline\nPipelineDQ\n\n\n\nout_pipeline\nPipeline\noutput pipeline\n\n\nstart_event\nOptional\ninput event start\n\n\nstop_event\nOptional\ninput event stop\n\n\ninterrupt_event\nOptional\ninput event interrupt\n\n\nflash_event\nOptional\ninput event flash\n\n\nexit_event\nOptional\ninput event exit\n\n\nReturns\nNone",
    "crumbs": [
      "<b style=\"color:DodgerBlue;\">Dataflow</b>",
      "<b>Filter</b>",
      "Heterofilter"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "tspace",
    "section": "",
    "text": "tspace is an data pipleline framework for deep reinforcement learning with IO interface, processing and configuration. The current code base depicts an automotive implementation. The goal of the system is to increase the energy efficiency (reward) of a BEV by imposing modification on parameters (action) of powertrain controller, the VCU, based on observations of the vehicle (state), i.e. speed, acceleration, electric engine current, voltage etc. The main features are:\n\nworks in both training and inferrence mode, supporting\n\ncoordinated ETL and ML pipelines,\nonline and offline training,\nlocal and distributed training;\n\nsupports multiple models:\n\nreinforcement learning models with DDPG and\nrecurrent models (RDPG) for time sequences with arbitrary length;\noffline reinforcement learning with “Implict Diffusion Q-Learning” (IDQ)\n\nthe data pipelines are compatible to both ETL and ML dataflow with\n\nsupport of multiple data sources (local CAN or remote cloud object storage),\nstateful time sequence processing with sequential model and\nsupport of both NoSQL database, local and cloud data storage.\n\n\n\nThe diagram shows the basic architecture of tspace.",
    "crumbs": [
      "tspace"
    ]
  },
  {
    "objectID": "index.html#ddpg",
    "href": "index.html#ddpg",
    "title": "tspace",
    "section": "DDPG",
    "text": "DDPG\n\nprovides methods to create, load or initialize the Deep Deterministic Policy Gradient Model, or restore checkpoints to it. It also exports the tflite model.\nIt provides the concrete methods for the abstract ones in the DPG interface.\nDDPG.infer_single_sample is the inference method with graph optimization via tf.function.\nDDPG.sample_minibatch provides a minibatch sampled from the buffer. It handles the bootstrap when the buffer is empty thus there is no samples in the Buffer when the first episode has not ended.\nDDPG.update_with_batch enforces the back propagation and applies the weight update to the actor and critic network during DDPG.train.",
    "crumbs": [
      "tspace"
    ]
  },
  {
    "objectID": "index.html#rdpg",
    "href": "index.html#rdpg",
    "title": "tspace",
    "section": "RDPG",
    "text": "RDPG\n\nprovides methods to create, load or initialize the Recurrent Deterministic Policy Gradient Model, or restore checkpoints to it.\nIt provides the concrete methods for the abstract ones in the DPG interface.\nRDPG.actor_predict_step is the inference method with graph optimization via tf.function.\nRDPG.train_step is the training method with graph optimization via tf.function. It also applies the weight update to the actor and critic network\nRDPG.train samples a ragged minibatch of episodes with different lengths from the buffer. It can handle training of time sequences with arbitrary length by truncated back propagation through time (TBPTT) with splitting the episodes and looping over the subsequences with Masking layers to update the weights by RDPG.train_step.",
    "crumbs": [
      "tspace"
    ]
  },
  {
    "objectID": "index.html#idql",
    "href": "index.html#idql",
    "title": "tspace",
    "section": "IDQL",
    "text": "IDQL\n\nprovides methods to create and initialize the Implicit Diffusion Q-learning Model.\nThe implementation of model is based on the repo jaxrl5 with Jax and Flax interface.\nIt provides the concrete methods for the abstract ones in the DPG interface.\nIDQL.actor_predict is the inference method.\nIDQL.train is the training method. Jaxrl5 takes care of the weight update to the actor and critic and the value network. It samples a minibatch of tuples (state, action, reward, next state) from the buffer.",
    "crumbs": [
      "tspace"
    ]
  },
  {
    "objectID": "index.html#seqactor",
    "href": "index.html#seqactor",
    "title": "tspace",
    "section": "SeqActor",
    "text": "SeqActor\nIt is the actor network with two recurrent LSTM layers, two dense layers and a Masking layer for handling ragged input sequence.\n\nSeqActor.predict outputs the action given the state for inference, thus the batch dimension has to be one.\nSeqActor.evaluate_actions outputs the action given a batch of states for training. It’s used in the training loop to get the prediction of the target actor network to calculate the critic loss.\nIt handles the ragged input sequences with Masking layer and the stateful recurrent layers for TBPTT\nFor inference, SeqCritic is not used and only SeqActor is required.",
    "crumbs": [
      "tspace"
    ]
  },
  {
    "objectID": "index.html#seqcritic",
    "href": "index.html#seqcritic",
    "title": "tspace",
    "section": "SeqCritic",
    "text": "SeqCritic\nIt is the critic network with two recurrent LSTM layers and two dense layer and a Masking layer for handling ragged input sequence.\n\nSeqCritic.evaluate_q gives the Q-value given a batch of the state and action. It’s used in the training loop RDPG.train_step to calculate the critic and actor loss.",
    "crumbs": [
      "tspace"
    ]
  },
  {
    "objectID": "index.html#buffer",
    "href": "index.html#buffer",
    "title": "tspace",
    "section": "Buffer",
    "text": "Buffer\nis an abstract class. It provides a view of data storage to the agent:\n\nAgent uses the abstract methods Buffer.load, Buffer.save and Buffer.close loads or saves data from or to the Pool, and closes the connection to the Pool.\nThe abstract Buffer.sample samples a minibatch from the Pool. It needs the child of Buffer to implement the concrete efficient sampling method, which depends on the underlying data storage system.\nThe concrete methode Buffer.store store the whole episode data into the Pool\nThe concrete methode Buffer.find simply calls Pool.find to find the data with the given query.\n\n\nMongoBuffer\nIt’s a concrete class for the underlying NoSQL database MongoDB.\n\nIt implements the abstract methods required by the Buffer interface.\nMongoBuffer.decode_batch_records prepare the sample batch data from MongoPool into a compliant format for agent training.\nIt can handle both DDPG record data type and RDPG episode data type.\n\n\n\nDaskBuffer\nIt’s a concrete class for the distributed data storage system Dask.\n\nIt implements the abstract methods required by the Buffer interface.\nDaskBuffer.decode_batch_records prepare the sample batch data from DaskPool into a compliant format for agent training.\nIt can handle both DDPG record data type and RDPG episode data type.",
    "crumbs": [
      "tspace"
    ]
  },
  {
    "objectID": "index.html#pool",
    "href": "index.html#pool",
    "title": "tspace",
    "section": "Pool",
    "text": "Pool\nis an abstract class. It’s the interface for the underlying data storage. For the moment, it’s implemented with MongoPool and DaskPool.\n\nIt defines the abstract methods Pool.load, Pool.close, Pool.store, Pool.delete, Pool.find, Pool.sample and Pool._count for the concrete classes to implement.\nIt defines PoolQuery as the query object for Pool.sample, Pool.find and Pool._count method.\nIt implements the iterable protocol with Pool.__iter__ and Pool.__getitem__ for the concrete classes to implement an efficient indexing method.\n\n\nMongoPool\nIt’s a concrete class for the underlying NoSQL database MongoDB with time series support. It handles both record data type and episode data type with MongoDB collection features.\n\nIt provides the interface to the MongoDB database with the pymongo library.\nIt implements the abstract methods required by the Pool interface.\nMongoPool.store_record stores the record data into the MongoDB database for DDPG agent.\nMongoPool.store_episode stores the episode data into the MongoDB database for RDPG agent.\n\n\n\nDaskPool\nIt’s an abstract class for the distributed data storage system Dask, since we have to use different backends: Parquet for record data type and avro for episode data type.\n\nIt supports both local file storage and remote object storage with the dask library.\nIt defines the generic data type for the abstract method required by the Pool interface. The generic data type can then be specialized by the concrete classes either as dask.DataFrame for record data type or dask.Bag for episode data type.\n\n\nParquetPool\nis a concrete class for the record data type with the Parquet file format as backend storage.\n\nIt implements the abstract methods required by the DaskPool interface and Pool subsequently.\nParquetPool.sample provides an efficient unified sampling interface via Dask.DataFrame to a Parquet storage either locally or remotely.\nParquetPool.get_query provides the query object through Dask indexing for the ParquetPool.sample method.\n\n\n\nAvroPool\nis a concrete class for the episode data type with the avro file format as backend storage.\n\nIt implements the abstract methods required by the DaskPool interface and Pool subsequently.\nAvroPool.sample provides an efficient unified sampling interface via Dask.Bag to a avro storage either locally or remotely.\nAvroPool.get_query provides the query object through Dask indexing for the AvroPool.sample method.",
    "crumbs": [
      "tspace"
    ]
  },
  {
    "objectID": "index.html#primary-threading-pool",
    "href": "index.html#primary-threading-pool",
    "title": "tspace",
    "section": "Primary threading pool",
    "text": "Primary threading pool\nis managed by Avatar with two primary threads in tspace.avatar.main:\n\nThe first primary thread is for data caputring\nThe second primary thread is for training and inference",
    "crumbs": [
      "tspace"
    ]
  },
  {
    "objectID": "index.html#data-capturing-thread",
    "href": "index.html#data-capturing-thread",
    "title": "tspace",
    "section": "Data capturing thread",
    "text": "Data capturing thread\ncalls VehicleInterface.ignite, which is shared by Kvaser and Cloud. It just starts a secondary threading pool containing six threads\n\nVehicleInterface.produce get the raw data either from the local UDP server as in Kvaser or the remote cloud object storage as in Cloud and forward it to the raw data pipeline. In case of Kvaser, it also gets the training HMI control messages from the same UDP server and put them in the HMI data pipeline.\nVehicleInterface.hmi_control manages the episodic state machine to control the training and inference process.\nVehicleInterface.countdown handles the episode end with a countdown timer to synchronize the data caputring is aligned with the episode end event.\nVehicleInterface.filter transforms the raw input json object into pandas.DataFrame and forward it to the input data pipeline of Cruncher.filter thread.\nVehicleInterface.consume is responsible for fetching the action object from the output data pipeline of Cruncher.filter thread and having it flashed on the vehicle ECU (VCU).\nVehicleInterface.watch_dog provides a watchdog to monitor the health of the data capturing process and the training process. It triggers the system stop if the observation or action quality is below a threshold.",
    "crumbs": [
      "tspace"
    ]
  },
  {
    "objectID": "index.html#model-training-and-inference-thread",
    "href": "index.html#model-training-and-inference-thread",
    "title": "tspace",
    "section": "Model training and inference thread",
    "text": "Model training and inference thread\ncall Cruncher.filter. Importantly, all processing in this thread is done synchronously in order to preserve the order of the time sequence, thus the causality of the oberservation and action.\n\nIt gets the data through the input pipeline and delegates the data to the agent for training or inference.\nAfter getting the prediction from the agent, it encodes the prediction result into an action object and forwards it through the output pipeline to VehicleInterface.consume to have it flashed on VCU.\nIt also controls the training loop, the inference loop and manage the training log and model checkpoint.\nThis thread is synchronized with the threads in the secondary threading pool with pre-defined threading.Event: start_event, stop_event, flash_event, interrupt_event and exit_event.",
    "crumbs": [
      "tspace"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "tspace",
    "section": "Install",
    "text": "Install\npip install tspace",
    "crumbs": [
      "tspace"
    ]
  }
]